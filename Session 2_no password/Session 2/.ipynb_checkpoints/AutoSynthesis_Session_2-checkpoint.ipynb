{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><strong>Autosynthesis Club</strong></h1>\n",
    "<h2 align=\"right\">Session 2 – Processing text data </h2>\n",
    "<h5 align=\"right\">14th Jan 2019 </h5>\n",
    "<h5 align=\"right\">Vincent </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1>Introduction</h1>\n",
    "<img src=\"Text mining.png\" width=\"400\" align=\"center\" title=\"Text mining\"/>\n",
    "\n",
    "<h2 align=\"left\"> (Data) mining</h2>\n",
    "<p align=\"left\"> - the practice of examining large pre-existing databases in order to generate new information </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">TEXT (data) to comparable forms </h2>\n",
    "<p align=\"left\"> One important question is how to use text data to generate information?<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">“Mathematics is the language in which God has written the universe”</h2>\n",
    "\n",
    "<img style=\"float:center\" src=\"Galilei.jpg\"/>\n",
    "\n",
    "<h4 align=\"center\"> -Galileo Galilei </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"left\"> What kind of ‘basic’ data can you generate from this abstract? (below) </h3>\n",
    "<img style=\"float:centre\" src=\"Picture3.png\" width=600 title=\"Abstract_trimmed\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1>Practical 1</h1>\n",
    "<h2> Convert your results into .csv file</h2>\n",
    "1. Open your Excel file and move to <strong>‘Results’</strong> tab <br>\n",
    "2. Save as <strong>CSV/ UTF-8 (comma delimited)</strong> and renamed as ‘AutoSession2’ <br>\n",
    "3. Drag them into the folder on Jupyer page <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Objective: Social media is an important pharma...\n",
       "1    Physicians intuitively apply pattern recogniti...\n",
       "2    BACKGROUND: The lipid scrambling activity of p...\n",
       "3    BACKGROUND: Primary care electronic medical re...\n",
       "4    The aim of this study was to evaluate the work...\n",
       "Name: Abstract, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('AutoSession2.csv'); #change it into your file name if it is in other names\n",
    "train['Abstract'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Basic statistics from texts</h2>\n",
    "\n",
    "- Number of words \n",
    "- Number of characters\n",
    "- Average word length\n",
    "- Number of stopwords (e.g. the, that, is…)\n",
    "- Number of special characters (e.g. %, -, (, )...)\n",
    "- Number of numerics\n",
    "- Number of uppercase words (e.g. RCT, ML, AUROC…)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: Social media is an important pharma...</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Physicians intuitively apply pattern recogniti...</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACKGROUND: The lipid scrambling activity of p...</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND: Primary care electronic medical re...</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The aim of this study was to evaluate the work...</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  word_count\n",
       "0  Objective: Social media is an important pharma...         246\n",
       "1  Physicians intuitively apply pattern recogniti...         247\n",
       "2  BACKGROUND: The lipid scrambling activity of p...         138\n",
       "3  BACKGROUND: Primary care electronic medical re...         272\n",
       "4  The aim of this study was to evaluate the work...         293"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['word_count'] = train['Abstract'].apply(lambda x: len(str(x).split(\" \"))) #number of words\n",
    "train[['Abstract','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of characters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: Social media is an important pharma...</td>\n",
       "      <td>1833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Physicians intuitively apply pattern recogniti...</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACKGROUND: The lipid scrambling activity of p...</td>\n",
       "      <td>1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND: Primary care electronic medical re...</td>\n",
       "      <td>1830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The aim of this study was to evaluate the work...</td>\n",
       "      <td>1909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  char_count\n",
       "0  Objective: Social media is an important pharma...        1833\n",
       "1  Physicians intuitively apply pattern recogniti...        1782\n",
       "2  BACKGROUND: The lipid scrambling activity of p...        1030\n",
       "3  BACKGROUND: Primary care electronic medical re...        1830\n",
       "4  The aim of this study was to evaluate the work...        1909"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['char_count'] = train['Abstract'].str.len() #number of characters\n",
    "train[['Abstract','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Average word length</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: Social media is an important pharma...</td>\n",
       "      <td>6.455285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Physicians intuitively apply pattern recogniti...</td>\n",
       "      <td>6.218623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACKGROUND: The lipid scrambling activity of p...</td>\n",
       "      <td>6.471014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND: Primary care electronic medical re...</td>\n",
       "      <td>5.731618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The aim of this study was to evaluate the work...</td>\n",
       "      <td>5.518771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  avg_word\n",
       "0  Objective: Social media is an important pharma...  6.455285\n",
       "1  Physicians intuitively apply pattern recogniti...  6.218623\n",
       "2  BACKGROUND: The lipid scrambling activity of p...  6.471014\n",
       "3  BACKGROUND: Primary care electronic medical re...  5.731618\n",
       "4  The aim of this study was to evaluate the work...  5.518771"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['Abstract'].apply(lambda x: avg_word(str(x))) #average word length\n",
    "train[['Abstract','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of upper case words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: Social media is an important pharma...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Physicians intuitively apply pattern recogniti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACKGROUND: The lipid scrambling activity of p...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND: Primary care electronic medical re...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The aim of this study was to evaluate the work...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  upper\n",
       "0  Objective: Social media is an important pharma...      9\n",
       "1  Physicians intuitively apply pattern recogniti...      0\n",
       "2  BACKGROUND: The lipid scrambling activity of p...      4\n",
       "3  BACKGROUND: Primary care electronic medical re...     16\n",
       "4  The aim of this study was to evaluate the work...     22"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['upper'] = train['Abstract'].apply(lambda x: len([x for x in str(x).split() if x.isupper()])) #number of uppercase words\n",
    "train[['Abstract','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of digits</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: Social media is an important pharma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Physicians intuitively apply pattern recogniti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACKGROUND: The lipid scrambling activity of p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND: Primary care electronic medical re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The aim of this study was to evaluate the work...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  numerics\n",
       "0  Objective: Social media is an important pharma...         0\n",
       "1  Physicians intuitively apply pattern recogniti...         1\n",
       "2  BACKGROUND: The lipid scrambling activity of p...         0\n",
       "3  BACKGROUND: Primary care electronic medical re...         1\n",
       "4  The aim of this study was to evaluate the work...         6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['numerics'] = train['Abstract'].apply(lambda x: len([x for x in str(x).split() if x.isdigit()])) #number of digits\n",
    "train[['Abstract','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of stopwords</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/linaro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')  #number of stopwords\n",
    "print (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: Social media is an important pharma...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Physicians intuitively apply pattern recogniti...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACKGROUND: The lipid scrambling activity of p...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND: Primary care electronic medical re...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The aim of this study was to evaluate the work...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  stopwords\n",
       "0  Objective: Social media is an important pharma...         63\n",
       "1  Physicians intuitively apply pattern recogniti...         77\n",
       "2  BACKGROUND: The lipid scrambling activity of p...         45\n",
       "3  BACKGROUND: Primary care electronic medical re...         99\n",
       "4  The aim of this study was to evaluate the work...         86"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['stopwords']=train['Abstract'].apply(lambda x: len([x for x in str(x).split() if x in stop])) #number of stopwords\n",
    "train[['Abstract','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of specific special characters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>special_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Objective: Social media is an important pharma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Physicians intuitively apply pattern recogniti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACKGROUND: The lipid scrambling activity of p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BACKGROUND: Primary care electronic medical re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The aim of this study was to evaluate the work...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  special_char\n",
       "0  Objective: Social media is an important pharma...             0\n",
       "1  Physicians intuitively apply pattern recogniti...             0\n",
       "2  BACKGROUND: The lipid scrambling activity of p...             0\n",
       "3  BACKGROUND: Primary care electronic medical re...             0\n",
       "4  The aim of this study was to evaluate the work...             3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['special_char'] = train['Abstract'].apply(lambda x: len([x for x in str(x).split() if x.count('%')])) #number of special character, i.e. -\n",
    "train[['Abstract','special_char']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Output: Basic statistics of abstracts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=train)\n",
    "df.to_excel('AutoSession2_BasicStatistics.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2>Noise</h2>\n",
    "Text data tend to contain some mistakes or unnecessary words. Removing them helps machine to read the data and save the amount of data to be processed.\n",
    "<h3 align=\"left\"> Can you still ‘know’ what this abstract about? </h3>\n",
    "<img style=\"float:centre\" src=\"Picture4.png\" width=600 title=\"Abstract_trimmed\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1 align=\"left\"> Practical 2</h1>\n",
    "<h2>Basic pre-processing (cleaning)</h2>\n",
    "\n",
    "<h3>Remove unnecessary words</h3>\n",
    "\n",
    " - Lower casing\n",
    " - Punctuation removal\n",
    " - Stopwords removal\n",
    " - Frequent words removal\n",
    " - Rare words removal\n",
    "\n",
    "<h3>Recover/tidy words to be read</h3>\n",
    "\n",
    " - Spelling correction\n",
    " - Tokenization\n",
    " - Stemming\n",
    " - Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:centre\" src=\"Picture5.png\" width=400 title=\"Wrong spelling1\"/>\n",
    "<br>\n",
    "<img style=\"float:centre\" src=\"Picture6.png\" width=400 title=\"Wrong spelling2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lower casing</h3>\n",
    "- change all characters into lower case to avoid 'new' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKGROUND: Primary care electronic medical record (EMR) data are being used for research, surveillance, and clinical monitoring. To broaden the reach and usability of EMR data, case definitions must be specified to identify and characterize important chronic conditions. The purpose of this study is to identify all case definitions for a set of chronic conditions that have been tested and validated in primary care EMR and EMR-linked data. This work will provide a reference list of case definitions, together with their performance metrics, and will identify gaps where new case definitions are needed. METHODS: We will consider a set of 40 chronic conditions, previously identified as potentially important for surveillance in a review of multimorbidity measures. We will perform a systematic search of the published literature to identify studies that describe case definitions for clinical conditions in EMR data and report the performance of these definitions. We will stratify our search by studies that use EMR data alone and those that use EMR-linked data. We will compare the performance of different definitions for the same conditions and explore the influence of data source, jurisdiction, and patient population. DISCUSSION: EMR data from primary care providers can be compiled and used for benefit by the healthcare system. Not only does this work have the potential to further develop disease surveillance and health knowledge, EMR surveillance systems can provide rapid feedback to participating physicians regarding their patients. Existing case definitions will serve as a starting point for the development and validation of new case definitions and will enable better surveillance, research, and practice feedback based on detailed clinical EMR data. SYSTEMATIC REVIEW REGISTRATION: PROSPERO CRD42016040020.\n",
      "--------------------------------------------------------------------------------\n",
      "background: primary care electronic medical record (emr) data are being used for research, surveillance, and clinical monitoring. to broaden the reach and usability of emr data, case definitions must be specified to identify and characterize important chronic conditions. the purpose of this study is to identify all case definitions for a set of chronic conditions that have been tested and validated in primary care emr and emr-linked data. this work will provide a reference list of case definitions, together with their performance metrics, and will identify gaps where new case definitions are needed. methods: we will consider a set of 40 chronic conditions, previously identified as potentially important for surveillance in a review of multimorbidity measures. we will perform a systematic search of the published literature to identify studies that describe case definitions for clinical conditions in emr data and report the performance of these definitions. we will stratify our search by studies that use emr data alone and those that use emr-linked data. we will compare the performance of different definitions for the same conditions and explore the influence of data source, jurisdiction, and patient population. discussion: emr data from primary care providers can be compiled and used for benefit by the healthcare system. not only does this work have the potential to further develop disease surveillance and health knowledge, emr surveillance systems can provide rapid feedback to participating physicians regarding their patients. existing case definitions will serve as a starting point for the development and validation of new case definitions and will enable better surveillance, research, and practice feedback based on detailed clinical emr data. systematic review registration: prospero crd42016040020.\n"
     ]
    }
   ],
   "source": [
    "train['Abstract_lower_casing'] = train['Abstract'].apply(lambda x: \" \".join(x.lower() for x in str(x).split())) #convert all characteristics into lower case\n",
    "print (train.loc[3,'Abstract'])\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (train.loc[3,'Abstract_lower_casing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Punctuation removal</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKGROUND: Primary care electronic medical record (EMR) data are being used for research, surveillance, and clinical monitoring. To broaden the reach and usability of EMR data, case definitions must be specified to identify and characterize important chronic conditions. The purpose of this study is to identify all case definitions for a set of chronic conditions that have been tested and validated in primary care EMR and EMR-linked data. This work will provide a reference list of case definitions, together with their performance metrics, and will identify gaps where new case definitions are needed. METHODS: We will consider a set of 40 chronic conditions, previously identified as potentially important for surveillance in a review of multimorbidity measures. We will perform a systematic search of the published literature to identify studies that describe case definitions for clinical conditions in EMR data and report the performance of these definitions. We will stratify our search by studies that use EMR data alone and those that use EMR-linked data. We will compare the performance of different definitions for the same conditions and explore the influence of data source, jurisdiction, and patient population. DISCUSSION: EMR data from primary care providers can be compiled and used for benefit by the healthcare system. Not only does this work have the potential to further develop disease surveillance and health knowledge, EMR surveillance systems can provide rapid feedback to participating physicians regarding their patients. Existing case definitions will serve as a starting point for the development and validation of new case definitions and will enable better surveillance, research, and practice feedback based on detailed clinical EMR data. SYSTEMATIC REVIEW REGISTRATION: PROSPERO CRD42016040020.\n",
      "--------------------------------------------------------------------------------\n",
      "BACKGROUND Primary care electronic medical record EMR data are being used for research surveillance and clinical monitoring To broaden the reach and usability of EMR data case definitions must be specified to identify and characterize important chronic conditions The purpose of this study is to identify all case definitions for a set of chronic conditions that have been tested and validated in primary care EMR and EMRlinked data This work will provide a reference list of case definitions together with their performance metrics and will identify gaps where new case definitions are needed METHODS We will consider a set of 40 chronic conditions previously identified as potentially important for surveillance in a review of multimorbidity measures We will perform a systematic search of the published literature to identify studies that describe case definitions for clinical conditions in EMR data and report the performance of these definitions We will stratify our search by studies that use EMR data alone and those that use EMRlinked data We will compare the performance of different definitions for the same conditions and explore the influence of data source jurisdiction and patient population DISCUSSION EMR data from primary care providers can be compiled and used for benefit by the healthcare system Not only does this work have the potential to further develop disease surveillance and health knowledge EMR surveillance systems can provide rapid feedback to participating physicians regarding their patients Existing case definitions will serve as a starting point for the development and validation of new case definitions and will enable better surveillance research and practice feedback based on detailed clinical EMR data SYSTEMATIC REVIEW REGISTRATION PROSPERO CRD42016040020\n"
     ]
    }
   ],
   "source": [
    "train['Abstract_punct_remove'] = train['Abstract'].str.replace('[^\\w\\s]','') #remove all special characteristics\n",
    "print (train.loc[3,'Abstract'])\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (train.loc[3,'Abstract_punct_remove'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stopwords removal</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aim of this study was to evaluate the workflow efficiency of a new automatic coronary-specific reconstruction technique (Smart Phase, GE Healthcare-SP) for selection of the best cardiac phase with least coronary motion when compared with expert manual selection (MS) of best phase in patients with high heart rate. A total of 46 patients with heart rates above 75 bpm who underwent single beat coronary computed tomography angiography (CCTA) were enrolled in this study. CCTA of all subjects were performed on a 256-detector row CT scanner (Revolution CT, GE Healthcare, Waukesha, Wisconsin, US). With the SP technique, the acquired phase range was automatically searched in 2% phase intervals during the reconstruction process to determine the optimal phase for coronary assessment, while for routine expert MS, reconstructions were performed at 5% intervals and a best phase was manually determined. The reconstruction and review times were recorded to measure the workflow efficiency for each method. Two reviewers subjectively assessed image quality for each coronary artery in the MS and SP reconstruction volumes using a 4-point grading scale. The average HR of the enrolled patients was 91.1+/-19.0bpm. A total of 204 vessels were assessed. The subjective image quality using SP was comparable to that of the MS, 1.45+/-0.85 vs 1.43+/-0.81 respectively (p = 0.88). The average time was 246 seconds for the manual best phase selection, and 98 seconds for the SP selection, resulting in average time saving of 148 seconds (60%) with use of the SP algorithm. The coronary specific automatic cardiac best phase selection technique (Smart Phase) improves clinical workflow in high heart rate patients and provides image quality comparable with manual cardiac best phase selection. Reconstruction of single-beat CCTA exams with SP can benefit the users with less experienced in CCTA image interpretation.\n",
      "--------------------------------------------------------------------------------\n",
      "The aim study evaluate workflow efficiency new automatic coronary-specific reconstruction technique (Smart Phase, GE Healthcare-SP) selection best cardiac phase least coronary motion compared expert manual selection (MS) best phase patients high heart rate. A total 46 patients heart rates 75 bpm underwent single beat coronary computed tomography angiography (CCTA) enrolled study. CCTA subjects performed 256-detector row CT scanner (Revolution CT, GE Healthcare, Waukesha, Wisconsin, US). With SP technique, acquired phase range automatically searched 2% phase intervals reconstruction process determine optimal phase coronary assessment, routine expert MS, reconstructions performed 5% intervals best phase manually determined. The reconstruction review times recorded measure workflow efficiency method. Two reviewers subjectively assessed image quality coronary artery MS SP reconstruction volumes using 4-point grading scale. The average HR enrolled patients 91.1+/-19.0bpm. A total 204 vessels assessed. The subjective image quality using SP comparable MS, 1.45+/-0.85 vs 1.43+/-0.81 respectively (p = 0.88). The average time 246 seconds manual best phase selection, 98 seconds SP selection, resulting average time saving 148 seconds (60%) use SP algorithm. The coronary specific automatic cardiac best phase selection technique (Smart Phase) improves clinical workflow high heart rate patients provides image quality comparable manual cardiac best phase selection. Reconstruction single-beat CCTA exams SP benefit users less experienced CCTA image interpretation.\n"
     ]
    }
   ],
   "source": [
    "train['Abstract_stop_remove'] = train['Abstract'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n",
    "print (train.loc[4,'Abstract'])\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (train.loc[4,'Abstract_stop_remove'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Frequent words removal</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the     491\n",
      "and     485\n",
      "of      481\n",
      "to      282\n",
      "in      240\n",
      "a       216\n",
      "for     160\n",
      "with    130\n",
      "The     101\n",
      "were     84\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "data        96\n",
      "patients    63\n",
      "review      60\n",
      "clinical    56\n",
      "results     52\n",
      "methods     52\n",
      "using       45\n",
      "health      41\n",
      "use         40\n",
      "research    40\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['Abstract'].dropna()).split()).value_counts()[:10]  #find first 10 most frequent occurred words\n",
    "\n",
    "train['Abstract_clean'] = train['Abstract'].apply(lambda x: \" \".join(x.lower() for x in str(x).split())) #convert all characteristics into lower case\n",
    "train['Abstract_clean'] = train['Abstract_clean'].str.replace('[^\\w\\s]','') #remove all special characteristics\n",
    "train['Abstract_clean'] = train['Abstract_clean'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n",
    "freq_clean = pd.Series(' '.join(train['Abstract_clean'].dropna()).split()).value_counts()[:10] #find first 10 most frequent occurred words\n",
    "\n",
    "print (freq)\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (freq_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKGROUND: Primary care electronic medical record (EMR) data are being used research, surveillance, clinical monitoring. To broaden reach usability EMR data, case definitions must be specified identify characterize important chronic conditions. purpose this study is identify all case definitions set chronic conditions that have been tested validated primary care EMR EMR-linked data. This work will provide reference list case definitions, together their performance metrics, will identify gaps where new case definitions are needed. METHODS: We will consider set 40 chronic conditions, previously identified as potentially important surveillance review multimorbidity measures. We will perform systematic search published literature identify studies that describe case definitions clinical conditions EMR data report performance these definitions. We will stratify our search by studies that use EMR data alone those that use EMR-linked data. We will compare performance different definitions same conditions explore influence data source, jurisdiction, patient population. DISCUSSION: EMR data from primary care providers can be compiled used benefit by healthcare system. Not only does this work have potential further develop disease surveillance health knowledge, EMR surveillance systems can provide rapid feedback participating physicians regarding their patients. Existing case definitions will serve as starting point development validation new case definitions will enable better surveillance, research, practice feedback based on detailed clinical EMR data. SYSTEMATIC REVIEW REGISTRATION: PROSPERO CRD42016040020.\n",
      "--------------------------------------------------------------------------------\n",
      "background primary care electronic medical record emr used surveillance monitoring broaden reach usability emr case definitions must specified identify characterize important chronic conditions purpose study identify case definitions set chronic conditions tested validated primary care emr emrlinked work provide reference list case definitions together performance metrics identify gaps new case definitions needed consider set 40 chronic conditions previously identified potentially important surveillance multimorbidity measures perform systematic search published literature identify studies describe case definitions conditions emr report performance definitions stratify search studies emr alone emrlinked compare performance different definitions conditions explore influence source jurisdiction patient population discussion emr primary care providers compiled used benefit healthcare system work potential develop disease surveillance knowledge emr surveillance systems provide rapid feedback participating physicians regarding existing case definitions serve starting point development validation new case definitions enable better surveillance practice feedback based detailed emr systematic registration prospero crd42016040020\n"
     ]
    }
   ],
   "source": [
    "freq = list(freq.index)  #remove first 10 most frequent occurred words\n",
    "train['Abstract_freq_remove'] = train['Abstract'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in freq))\n",
    "\n",
    "freq_clean = list(freq_clean.index)\n",
    "train['Abstract_clean_freq_remove'] = train['Abstract_clean'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in freq_clean))\n",
    "\n",
    "print (train.loc[3,'Abstract_freq_remove'])\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (train.loc[3,'Abstract_clean_freq_remove'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rare words removal</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premiere          1\n",
      "(CCTA)            1\n",
      "traditionally     1\n",
      "portray           1\n",
      "Efficiency        1\n",
      "Meta-analysis.    1\n",
      "make              1\n",
      "(0.86-0.89).      1\n",
      "stimulating       1\n",
      "EMBASE,           1\n",
      "dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "medically               1\n",
      "originates              1\n",
      "analyzing               1\n",
      "insufficiency           1\n",
      "parts                   1\n",
      "floor                   1\n",
      "ventilatorassociated    1\n",
      "panorama                1\n",
      "hospital                1\n",
      "effort                  1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq_les = pd.Series(' '.join(train['Abstract'].dropna()).split()).value_counts()[-10:] #find first 10 least frequent occurred words\n",
    "\n",
    "#train['Abstract_clean'] = train['Abstract'].apply(lambda x: \" \".join(x.lower() for x in str(x).split())) #convert all characteristics into lower case\n",
    "#train['Abstract_clean'] = train['Abstract_clean'].str.replace('[^\\w\\s]','') #remove all special characteristics\n",
    "#train['Abstract_clean'] = train['Abstract_clean'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n",
    "freq_clean_les = pd.Series(' '.join(train['Abstract_clean'].dropna()).split()).value_counts()[-10:] #find first 10 least frequent occurred words\n",
    "\n",
    "print (freq_les)\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (freq_clean_les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_les = list(freq_les.index) #remove least frequent words from original abstracts\n",
    "train['Abstract_freq_les_remove'] = train['Abstract'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in freq_les))\n",
    "\n",
    "freq_clean_les = list(freq_clean_les.index) #remove least frequent words from cleaned abstracts\n",
    "train['Abstract_clean_freq_les_remove'] = train['Abstract_clean'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in freq_clean_les))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spelling correction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite the constant negative press coffee\n"
     ]
    }
   ],
   "source": [
    "#pip install -U textblob #need to install textblob on terminal or anaconda promp\n",
    "#python -m textblob.download_corpora\n",
    "\n",
    "from textblob import TextBlob\n",
    "tweet = TextBlob(\"Despite the constant negative press covfefe\")\n",
    "print (tweet.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Translation (Gooogle translate API)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"朱利安是房間裡最高的人。\")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = TextBlob('Julian is the tallest person in the room.')\n",
    "translation.translate(to='zh-TW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming</h3>\n",
    "Removing suffices, i.e. “ing”, “ly”, “s”…etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background primary care electronic medical record emr data used research surveillance clinical monitoring broaden reach usability emr data case definitions must specified identify characterize important chronic conditions purpose study identify case definitions set chronic conditions tested validated primary care emr emrlinked data work provide reference list case definitions together performance metrics identify gaps new case definitions needed methods consider set 40 chronic conditions previously identified potentially important surveillance review multimorbidity measures perform systematic search published literature identify studies describe case definitions clinical conditions emr data report performance definitions stratify search studies use emr data alone use emrlinked data compare performance different definitions conditions explore influence data source jurisdiction patient population discussion emr data primary care providers compiled used benefit healthcare system work potential develop disease surveillance health knowledge emr surveillance systems provide rapid feedback participating physicians regarding patients existing case definitions serve starting point development validation new case definitions enable better surveillance research practice feedback based detailed clinical emr data systematic review registration prospero crd42016040020\n",
      "--------------------------------------------------------------------------------\n",
      "background primari care electron medic record emr data use research surveil clinic monitor broaden reach usabl emr data case definit must specifi identifi character import chronic condit purpos studi identifi case definit set chronic condit test valid primari care emr emrlink data work provid refer list case definit togeth perform metric identifi gap new case definit need method consid set 40 chronic condit previous identifi potenti import surveil review multimorbid measur perform systemat search publish literatur identifi studi describ case definit clinic condit emr data report perform definit stratifi search studi use emr data alon use emrlink data compar perform differ definit condit explor influenc data sourc jurisdict patient popul discuss emr data primari care provid compil use benefit healthcar system work potenti develop diseas surveil health knowledg emr surveil system provid rapid feedback particip physician regard patient exist case definit serv start point develop valid new case definit enabl better surveil research practic feedback base detail clinic emr data systemat review registr prospero crd42016040020\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer #Stemming\n",
    "st = PorterStemmer()\n",
    "train['Abstract_stem'] = train['Abstract_clean'].apply(lambda x: \" \".join([st.stem(word) for word in str(x).split()]))\n",
    "print (train.loc[3,'Abstract_clean'])\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (train.loc[3,'Abstract_stem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatization</h3>\n",
    "Convert the word into its root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/linaro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ace684262db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract_lemm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Abstract_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------------------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer #need to install textblob #lemmer\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "train['Abstract_lemm'] = train['Abstract_clean'].apply(lambda x: \" \".join([lm.lemmatize(word) for word in str(x).split()]))\n",
    "print (train.loc[3,'Abstract_clean'])\n",
    "print ('--------------------------------------------------------------------------------')\n",
    "print (train.loc[3,'Abstract_lemm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenization</h3>\n",
    "Dividing the text into a series of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Tokenization ('Bag of words')\n",
    "vectorizer = CountVectorizer()\n",
    "Tokenizer = vectorizer.fit_transform(train['Abstract_clean'])\n",
    "Words = vectorizer.get_feature_names()\n",
    "Words = np.asarray(Words)\n",
    "\n",
    "BoW =np.vstack((Words, Tokenizer.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Output: Basic text processing of abstracts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=train)\n",
    "df2 = pd.DataFrame(data=Words)\n",
    "writer = pd.ExcelWriter('AutoSession2_BasicProcessing.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name= 'BasicProcessing')\n",
    "df2.to_excel(writer, sheet_name= 'Tokenization')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['Abstract_clean'] = train['Abstract_clean'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in freq_clean_les))\n",
    "train['Abstract_clean'] = train['Abstract_clean'].apply(lambda x: \" \".join([lm.lemmatize(word) for word in str(x).split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1>Practical 3</h1>\n",
    "<h2>Advance Text Processing</h2>\n",
    "\n",
    "- N-grams\n",
    "- Term Frequency (TF)\n",
    "- Inverse Document Frequency (IDF)\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- Bag of Words\n",
    "- Sentiment Analysis\n",
    "- Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>N-grams</h3>\n",
    "Combination of N words used together (i.e. [‘Randomised’, \"controlled\", ‘trial’] as trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['objective', 'social']),\n",
       " WordList(['social', 'media']),\n",
       " WordList(['media', 'important']),\n",
       " WordList(['important', 'pharmacovigilance']),\n",
       " WordList(['pharmacovigilance', 'data']),\n",
       " WordList(['data', 'source']),\n",
       " WordList(['source', 'adverse']),\n",
       " WordList(['adverse', 'drug']),\n",
       " WordList(['drug', 'reaction']),\n",
       " WordList(['reaction', 'adr']),\n",
       " WordList(['adr', 'identification']),\n",
       " WordList(['identification', 'human']),\n",
       " WordList(['human', 'review']),\n",
       " WordList(['review', 'social']),\n",
       " WordList(['social', 'media']),\n",
       " WordList(['media', 'data']),\n",
       " WordList(['data', 'infeasible']),\n",
       " WordList(['infeasible', 'due']),\n",
       " WordList(['due', 'data']),\n",
       " WordList(['data', 'quantity']),\n",
       " WordList(['quantity', 'thus']),\n",
       " WordList(['thus', 'natural']),\n",
       " WordList(['natural', 'language']),\n",
       " WordList(['language', 'processing']),\n",
       " WordList(['processing', 'techniques']),\n",
       " WordList(['techniques', 'necessary']),\n",
       " WordList(['necessary', 'social']),\n",
       " WordList(['social', 'media']),\n",
       " WordList(['media', 'includes']),\n",
       " WordList(['includes', 'informal']),\n",
       " WordList(['informal', 'vocabulary']),\n",
       " WordList(['vocabulary', 'irregular']),\n",
       " WordList(['irregular', 'grammar']),\n",
       " WordList(['grammar', 'challenge']),\n",
       " WordList(['challenge', 'natural']),\n",
       " WordList(['natural', 'language']),\n",
       " WordList(['language', 'processing']),\n",
       " WordList(['processing', 'methods']),\n",
       " WordList(['methods', 'objective']),\n",
       " WordList(['objective', 'develop']),\n",
       " WordList(['develop', 'scalable']),\n",
       " WordList(['scalable', 'deeplearning']),\n",
       " WordList(['deeplearning', 'approach']),\n",
       " WordList(['approach', 'exceeds']),\n",
       " WordList(['exceeds', 'stateoftheart']),\n",
       " WordList(['stateoftheart', 'adr']),\n",
       " WordList(['adr', 'detection']),\n",
       " WordList(['detection', 'performance']),\n",
       " WordList(['performance', 'social']),\n",
       " WordList(['social', 'media']),\n",
       " WordList(['media', 'materials']),\n",
       " WordList(['materials', 'methods']),\n",
       " WordList(['methods', 'developed']),\n",
       " WordList(['developed', 'recurrent']),\n",
       " WordList(['recurrent', 'neural']),\n",
       " WordList(['neural', 'network']),\n",
       " WordList(['network', 'rnn']),\n",
       " WordList(['rnn', 'model']),\n",
       " WordList(['model', 'labels']),\n",
       " WordList(['labels', 'words']),\n",
       " WordList(['words', 'input']),\n",
       " WordList(['input', 'sequence']),\n",
       " WordList(['sequence', 'adr']),\n",
       " WordList(['adr', 'membership']),\n",
       " WordList(['membership', 'tags']),\n",
       " WordList(['tags', 'input']),\n",
       " WordList(['input', 'features']),\n",
       " WordList(['features', 'wordembedding']),\n",
       " WordList(['wordembedding', 'vectors']),\n",
       " WordList(['vectors', 'formed']),\n",
       " WordList(['formed', 'taskindependent']),\n",
       " WordList(['taskindependent', 'pretraining']),\n",
       " WordList(['pretraining', 'adr']),\n",
       " WordList(['adr', 'detection']),\n",
       " WordList(['detection', 'training']),\n",
       " WordList(['training', 'results']),\n",
       " WordList(['results', 'bestperforming']),\n",
       " WordList(['bestperforming', 'rnn']),\n",
       " WordList(['rnn', 'model']),\n",
       " WordList(['model', 'used']),\n",
       " WordList(['used', 'pretrained']),\n",
       " WordList(['pretrained', 'word']),\n",
       " WordList(['word', 'embeddings']),\n",
       " WordList(['embeddings', 'created']),\n",
       " WordList(['created', 'large']),\n",
       " WordList(['large', 'nondomainspecific']),\n",
       " WordList(['nondomainspecific', 'twitter']),\n",
       " WordList(['twitter', 'dataset']),\n",
       " WordList(['dataset', 'achieved']),\n",
       " WordList(['achieved', 'approximate']),\n",
       " WordList(['approximate', 'match']),\n",
       " WordList(['match', 'fmeasure']),\n",
       " WordList(['fmeasure', '0755']),\n",
       " WordList(['0755', 'adr']),\n",
       " WordList(['adr', 'identification']),\n",
       " WordList(['identification', 'dataset']),\n",
       " WordList(['dataset', 'compared']),\n",
       " WordList(['compared', '0631']),\n",
       " WordList(['0631', 'baseline']),\n",
       " WordList(['baseline', 'lexicon']),\n",
       " WordList(['lexicon', 'system']),\n",
       " WordList(['system', '065']),\n",
       " WordList(['065', 'stateoftheart']),\n",
       " WordList(['stateoftheart', 'conditional']),\n",
       " WordList(['conditional', 'random']),\n",
       " WordList(['random', 'field']),\n",
       " WordList(['field', 'model']),\n",
       " WordList(['model', 'feature']),\n",
       " WordList(['feature', 'analysis']),\n",
       " WordList(['analysis', 'indicated']),\n",
       " WordList(['indicated', 'semantic']),\n",
       " WordList(['semantic', 'information']),\n",
       " WordList(['information', 'pretrained']),\n",
       " WordList(['pretrained', 'word']),\n",
       " WordList(['word', 'embeddings']),\n",
       " WordList(['embeddings', 'boosted']),\n",
       " WordList(['boosted', 'sensitivity']),\n",
       " WordList(['sensitivity', 'combined']),\n",
       " WordList(['combined', 'contextual']),\n",
       " WordList(['contextual', 'awareness']),\n",
       " WordList(['awareness', 'captured']),\n",
       " WordList(['captured', 'rnn']),\n",
       " WordList(['rnn', 'precision']),\n",
       " WordList(['precision', 'discussion']),\n",
       " WordList(['discussion', 'model']),\n",
       " WordList(['model', 'required']),\n",
       " WordList(['required', 'taskspecific']),\n",
       " WordList(['taskspecific', 'feature']),\n",
       " WordList(['feature', 'engineering']),\n",
       " WordList(['engineering', 'suggesting']),\n",
       " WordList(['suggesting', 'generalizability']),\n",
       " WordList(['generalizability', 'additional']),\n",
       " WordList(['additional', 'sequencelabeling']),\n",
       " WordList(['sequencelabeling', 'tasks']),\n",
       " WordList(['tasks', 'learning']),\n",
       " WordList(['learning', 'curve']),\n",
       " WordList(['curve', 'analysis']),\n",
       " WordList(['analysis', 'showed']),\n",
       " WordList(['showed', 'model']),\n",
       " WordList(['model', 'reached']),\n",
       " WordList(['reached', 'optimal']),\n",
       " WordList(['optimal', 'performance']),\n",
       " WordList(['performance', 'fewer']),\n",
       " WordList(['fewer', 'training']),\n",
       " WordList(['training', 'examples']),\n",
       " WordList(['examples', 'models']),\n",
       " WordList(['models', 'conclusion']),\n",
       " WordList(['conclusion', 'adr']),\n",
       " WordList(['adr', 'detection']),\n",
       " WordList(['detection', 'performance']),\n",
       " WordList(['performance', 'social']),\n",
       " WordList(['social', 'media']),\n",
       " WordList(['media', 'significantly']),\n",
       " WordList(['significantly', 'improved']),\n",
       " WordList(['improved', 'using']),\n",
       " WordList(['using', 'contextually']),\n",
       " WordList(['contextually', 'aware']),\n",
       " WordList(['aware', 'model']),\n",
       " WordList(['model', 'word']),\n",
       " WordList(['word', 'embeddings']),\n",
       " WordList(['embeddings', 'formed']),\n",
       " WordList(['formed', 'large']),\n",
       " WordList(['large', 'unlabeled']),\n",
       " WordList(['unlabeled', 'datasets']),\n",
       " WordList(['datasets', 'approach']),\n",
       " WordList(['approach', 'reduces']),\n",
       " WordList(['reduces', 'manual']),\n",
       " WordList(['manual', 'datalabeling']),\n",
       " WordList(['datalabeling', 'requirements']),\n",
       " WordList(['requirements', 'scalable']),\n",
       " WordList(['scalable', 'large']),\n",
       " WordList(['large', 'social']),\n",
       " WordList(['social', 'media']),\n",
       " WordList(['media', 'datasets'])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(train['Abstract_clean'][0]).ngrams(2) #2-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Term Frequency (TF)</h3>\n",
    "$ TF = (Number\\ of\\ term\\ appears\\ in\\ the\\ text) \\div (number\\ of\\ words\\ in\\ the\\ text)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adr</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>media</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>performance</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words    tf\n",
       "0          adr   6.0\n",
       "1        media   6.0\n",
       "2        model  29.0\n",
       "3       social  11.0\n",
       "4  performance  17.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (train['Abstract_clean'][0:50]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index() #Term Frequency\n",
    "\n",
    "tf1.columns = ['words','tf']\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Inverse Document Frequency (IDF)</h3>\n",
    "$ IDF = log(N/n)$ <br>\n",
    "N=total number of words; n=number of the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adr</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.198673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>media</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.505526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.800778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.282382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>performance</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.589235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words    tf       idf\n",
       "0          adr   6.0  3.198673\n",
       "1        media   6.0  2.505526\n",
       "2        model  29.0  0.800778\n",
       "3       social  11.0  2.282382\n",
       "4  performance  17.0  1.589235"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['Abstract_clean'].str.contains(word)])))\n",
    "\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Term Frequency-Inverse Document Frequency (TF-IDF)</h3>\n",
    "$ TF{-}IDF=TF\\times IDF$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Method 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adr</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.198673</td>\n",
       "      <td>19.192039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>media</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.505526</td>\n",
       "      <td>15.033156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.800778</td>\n",
       "      <td>23.222557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.282382</td>\n",
       "      <td>25.106206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>performance</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.589235</td>\n",
       "      <td>27.016998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words    tf       idf      tfidf\n",
       "0          adr   6.0  3.198673  19.192039\n",
       "1        media   6.0  2.505526  15.033156\n",
       "2        model  29.0  0.800778  23.222557\n",
       "3       social  11.0  2.282382  25.106206\n",
       "4  performance  17.0  1.589235  27.016998"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['Abstract_clean'].str.contains(word)])))\n",
    "\n",
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting\"><h4>Method 2</h4></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49x2690 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5421 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  #Obtain TF-IDF from sklearn\n",
    "tfidf = TfidfVectorizer(lowercase=True, analyzer='word', stop_words= 'english', ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['Abstract_clean'])\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy import sparse #view structure of multiple arrays\n",
    "sparse.save_npz(\"TF-IDF.npz\", train_vect)\n",
    "b = np.load('TF-IDF.npz')\n",
    "list(b)\n",
    "print (b['indices'])\n",
    "print (b['data'])\n",
    "print (b['shape'])\n",
    "print (b['indptr'])\n",
    "print (b['format'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bag of Words</h3>\n",
    "Presence of words within the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  #Bag of words\n",
    "bow = CountVectorizer(lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['Abstract_clean'])\n",
    "\n",
    "Words = bow.get_feature_names()\n",
    "Words = np.asarray(Words)\n",
    "BoW =np.vstack((Words, train_bow.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentiment Analysis</h3>\n",
    "Identifying and categorizing opinions expressed in a piece of text - <a href=\"https://www.dremio.com/trump-twitter-sentiment-analysis/\">Example</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center\" src=\"Picture7.png\" width=400 title=\"Sentiment Analysis\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word Embedding</h3>\n",
    "Representation of text in the form of vectors <a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pairwise similarity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #calcuate similarity using TF-IDF\n",
    "tfidf_ps = TfidfVectorizer().fit_transform(train['Abstract_clean'])\n",
    "pairwise_similarity = tfidf_ps * tfidf_ps.T\n",
    "pairwise_similarity = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Output: Advanced text processing of abstracts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=tf1)\n",
    "df1 = pd.DataFrame(train_vect.toarray(), columns=tfidf.get_feature_names())\n",
    "df2 = pd.DataFrame(data=BoW)\n",
    "df3 = pd.DataFrame(data=pairwise_similarity)\n",
    "writer = pd.ExcelWriter('AutoSession2_Advanced text processing.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name= 'Tf-IDF')\n",
    "df1.to_excel(writer, sheet_name= 'Tf-IDF_Sklearn')\n",
    "df2.to_excel(writer, sheet_name= 'Bag of Words')\n",
    "df3.to_excel(writer, sheet_name= 'pairwise_similarity_Tf-IDF')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1>Take home messages</h1>\n",
    "\n",
    "<h2>Text data can be described by mathematic formats or themselves</h2>\n",
    "\n",
    "- Basic statistics, TF-IDF, word embedding\n",
    "- Bag of words\n",
    "\n",
    "<h2>These will attribute to different strategies to be used in terms of analysing text data</h2>\n",
    "\n",
    "<h2>Different external corpora and packages can be used for processing text data</h2>\n",
    "\n",
    "- NLTK (i.e. Brown Corpus)\n",
    "- Sklearn, nltk, TextBlob\n",
    "- Use of these corpora in the data cleaning and processing can significantly affect the quality of text data and later analyses\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center\" src=\"Picture8.png\" width=800 title=\"Garbage in, garbage out!\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1>Homework</h1>\n",
    "<h2>Python tutorials</h2>\n",
    "<a href=\"https://github.com/chryswoods/siremol.org/blob/master/chryswoods.com/beginning_python/README.md\">UoB ACRC Python 1: Beginning Python</a>\n",
    "<br>\n",
    "<a href=\"https://github.com/chryswoods/siremol.org/blob/master/chryswoods.com/intermediate_python/README.md\">UoB ACRC Python 2: Intermediate Python</a>\n",
    "<br>\n",
    "<a href=\"https://www.codecademy.com/learn/learn-python\">https://www.codecademy.com/learn/learn-python</a>\n",
    "<br>\n",
    "<a href=\"https://www.tutorialspoint.com/python/index.htm\">https://www.tutorialspoint.com/python/index.htm</a>\n",
    "\n",
    "<h2>Text-mining packages to try out</h2>\n",
    "\n",
    "- [Sklearn](https://scikit-learn.org/stable/index.html) \n",
    "- [nltk](https://www.nltk.org/index.html)\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/index.html)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "\n",
    "\n",
    "<h2>Questions and challenges</h2>\n",
    "\n",
    "- What is the pros/cons of using TF-IDF, bag of words and word embedding to analyse text data?\n",
    "- How will you utilise your results (decisions and/or terms) to find important/similar references?\n",
    "- Can you use other information of references to help the analysis? (i.e. authors or PMID)\n",
    "- Can you build your own corpora to help your data processing?\n",
    "\n",
    "\n",
    "<h3>This teaching material is derived from <a href=\"https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python\">Ultimate guide to deal with Text Data (using Python)</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h1>Thank you for coming!</h1>\n",
    "\n",
    "<h3>Any questions please e-mail:</h3>\n",
    "<h5 align=\"center\">Kazeem     <br> b.k.olorisade@bristol.ac.uk</h5>\n",
    "<h5 align=\"center\">Vincent   <br>Vincent.Cheng@bristol.ac.uk</h5>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
