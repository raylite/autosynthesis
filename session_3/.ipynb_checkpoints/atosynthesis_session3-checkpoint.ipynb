{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>AutoSynthesis study group</h1>\n",
    "<h2 align='right'> Session 3 - Modeling </h2>\n",
    "<h3 align='right'> 15th February 2019 </h3>\n",
    "<h3 align='right'> Kazeem </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of last session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul> \n",
    "<li> Loading and analysis of data </li>\n",
    "<li> Basic preprocessing </li>\n",
    "    <ul>\n",
    "        <li> Lowercasing </li>\n",
    "        <li> Stopwords removal </li>\n",
    "        <li> Stemming </li>\n",
    "        <li> Lemmatisation </li>\n",
    "    </ul>\n",
    "<li> Feature representation </li>\n",
    "    <ul>\n",
    "        <li> Bag of words </li>\n",
    "        <li> Tf-idf </li>\n",
    "        <li><font color = 'red'><strong> Binary </strong></font></li>\n",
    "        <li> Word embedding </li>\n",
    "        <li> N-grams </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "train = pd.read_csv('AutoSession2.csv')\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True)\n",
    "tdm = vectorizer.fit_transform(train['Abstract_clean'])\n",
    "words = vectorizer.get_feature_names()\n",
    "words = np.asarray(Words)\n",
    "\n",
    "#BoW =np.vstack((words, tdm.toarray()))\n",
    "tdm_df = pd.DataFrame(data=words)\n",
    "print (tdm_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Why do we need preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Why do we need feature representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In support vector machine (SVM) each data item is plotted as a point in an n-dimensional space (where n is the number of features in the TDM)where the value of each feature is the value of each coordinate. The algorithm  classifies the data by finding the hyperplane that best partition the data.</p>\n",
    "<p>The SVM relies on the data points closest to the hyperplane on both sides to make prediction.</p>\n",
    "\n",
    "<img src=\"SVM_1.png\">\n",
    "\n",
    "<h2>Linear separablity and maximum margin</h2>\n",
    "<div>\n",
    "    <img src=\"SVM_21.png\"></div>\n",
    "    <img src=\"SVM_3.png\"></div>\n",
    "    <img src=\"SVM_4.png\"></div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h2>The Kernel trick</h2>\n",
    "<p> What happens in situations where the data is not linearly separable?\n",
    "<div>\n",
    "    <img src=\"SVM_8.png\"></div>\n",
    "    <img src=\"SVM_9.png\"></div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv('autosynthesis_full.csv')\n",
    "\n",
    "#convert labels to relevant/irrelevant classes\n",
    "\n",
    "data['label'] = data[['Decision']].apply(lambda x: )\n",
    "\n",
    "data['TiAbs'] = data[['Title', 'Abstract', 'Keywords']].apply(lambda x: {} {} {}.format(x[0], x[1], x[2]), axis=1)\n",
    "\n",
    "#separate to train and test set\n",
    "sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['TiAbs'], binary_label, test_size=0.20, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
