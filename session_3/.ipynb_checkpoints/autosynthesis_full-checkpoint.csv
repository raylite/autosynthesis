ID,Authors,Year,Journal,Title,Abstract,Keywords,Decision1,Decision2,label
1,Lerner,2018,Journal of clinical epidemiology,Automatic screening using word embeddings achieved high sensitivity and workload reduction for updating living network meta-analyses,"OBJECTIVE: We aimed to develop and evaluate an algorithm for automatically screening citations when updating living network meta-analysis (NMA). STUDY DESIGN AND SETTING: Our algorithm learns from the initial screening of citations conducted when creating an NMA to automatically identify eligible citations (i.e., needing full-text consideration) when updating the NMA. We evaluated our algorithm on four NMAs from different medical domains. For each NMA, we constructed sets of initially screened citations and citations to screen during an update that took place 2 years after the conduct of the NMA. We encoded free text of citations (title and abstract) using word embeddings. On top of this vectorized representation, we fitted a logistic regression model to the set of initially screened citations to predict the eligibility of citations screened during an update. RESULTS: Our algorithm achieved 100% sensitivity on two NMAs (100% [93-100] and 100% [40-100] sensitivity), and 94% [81-99] and 97% [86-100] on the remaining two others. For all NMAs, our algorithm would have spared to manually screen 1345 of 2530 citations, decreasing the workload by 53% [51-55], while missing 3 of 124 eligible citations (2% [1-7]), none of which were finally included in the NMAs after full-text consideration. CONCLUSION: For updating an NMA after 2 years, our algorithm considerably diminished the workload required for screening, and the number of missed eligible citations remained low.", automatic screening; live cumulative network meta-analysis; machine learning; natural language processing; network meta-analysis; word embeddings,2,2,
2,Zhu,2018,PloS one,Heated humidification did not improve compliance of positive airway pressure and subjective daytime sleepiness in obstructive sleep apnea syndrome: A meta-analysis,"INTRODUCTION: We performed a meta-analysis on whether heated humidification during positive airway pressure (PAP) could improve compliance and subjective daytime sleepiness in obstructive sleep apnea syndrome (OSAS) patients. MATERIALS AND METHODS: We searched PubMed, EMBASE, Medline, Cochrane Library, Clinical Trials, Web of Science and Scopus from inception to Oct 29, 2017. We made meta-analysis on the all available randomized controlled trials (RCTs) which assessed effects of heated humidification intervention on PAP compliance and subjective daytime sleepiness, by subgroups of automatic adjusting positive airway pressure/ continuous positive airway pressure (APAP/CPAP) usage and patients with/without upper airway symptoms prior to PAP therapy. RESULTS: A total of nine RCTs were evaluated finally in this meta-analysis. When all the studies were pooled, heated humidification did not improve PAP usage time [weighted mean difference(WMD) = 13.28, 95% confidence interval(CI): -5.85 to 32.41, P = 0.17] or Epworth sleepiness scale (ESS) score (WMD = -0.63, 95% CI: -1.32 to 0.07, P = 0.08). In terms of PAP usage time, heated humidification failed to enhance compliance in both APAP (WMD = 22.34, 95%CI: -21.08 to 65.77, P = 0.31) or CPAP subgroup (WMD = 11.09, 95%CI: -10.21 to 32.40, P = 0.31) and it was also ineffective among patients with upper airway symptoms prior to PAP therapy (WMD = 22.74, 95% CI: -7.77 to 53.24, P = 0.14) or without (WMD = 13.22, 95%CI: -35.84 to 62.29, P = 0.60). In terms of ESS score, heated humidification did not reduce ESS scores in both APAP (WMD = -1.59, 95% CI: -3.81 to 0.64, P = 0.16) or CPAP subgroup (WMD = -0.39, 95% CI: -1.16 to 0.37, P = 0.32) and it was also helpless among patients with upper airway symptoms prior to PAP therapy (WMD = -1.17, 95% CI: -3.10 to 0.75, P = 0.23) or without (WMD = -0.30, 95%CI: -2.25 to 1.66, P = 0.76). CONCLUSION: Heated humidification during PAP therapy improves neither the compliance nor ESS scores in OSAS patients, no matter what types of PAP or whether the patients had upper airway symptoms prior to PAP therapy. But to the population with upper airway symptoms and the APAP users, the conclusions were limited because of small sample size and possible selection bias. More attentions should be paid to these potentially possible benefited subgroups.", ,-2,-2,
3,Pons,2018,PloS one,Quantifying skeletal muscle volume and shape in humans using MRI: A systematic review of validity and reliability,"AIMS: The aim of this study was to report the metrological qualities of techniques currently used to quantify skeletal muscle volume and 3D shape in healthy and pathological muscles. METHODS: A systematic review was conducted (Prospero CRD42018082708). PubMed, Web of Science, Cochrane and Scopus databases were searched using relevant keywords and inclusion/exclusion criteria. The quality of the articles was evaluated using a customized scale. RESULTS: Thirty articles were included, 6 of which included pathological muscles. Most evaluated lower limb muscles. Partially or completely automatic and manual techniques were assessed in 10 and 24 articles, respectively. Manual slice-by-slice segmentation reliability was good-to-excellent (n = 8 articles) and validity against dissection was moderate to good(n = 1). Manual slice-by-slice segmentation was used as a gold-standard method in the other articles. Reduction of the number of manually segmented slices (n = 6) provided good to excellent validity if a sufficient number of appropriate slices was chosen. Segmentation on one slice (n = 11) increased volume errors. The Deformation of a Parametric Specific Object (DPSO) method (n = 5) decreased the number of manually-segmented slices required for any chosen level of error. Other automatic techniques combined with different statistical shape or atlas/images-based methods (n = 4) had good validity. Some particularities were highlighted for specific muscles. Except for manual slice by slice segmentation, reliability has rarely been reported. CONCLUSIONS: The results of this systematic review help the choice of appropriate segmentation techniques, according to the purpose of the measurement. In healthy populations, techniques that greatly simplified the process of manual segmentation yielded greater errors in volume and shape estimations. Reduction of the number of manually segmented slices was possible with appropriately chosen segmented slices or with DPSO. Other automatic techniques showed promise, but data were insufficient for their validation. More data on the metrological quality of techniques used in the cases of muscle pathology are required.", ,-2,-2,
4,Hinds,2018,PloS one,What demographic attributes do our digital footprints reveal? A systematic review,"To what extent does our online activity reveal who we are? Recent research has demonstrated that the digital traces left by individuals as they browse and interact with others online may reveal who they are and what their interests may be. In the present paper we report a systematic review that synthesises current evidence on predicting demographic attributes from online digital traces. Studies were included if they met the following criteria: (i) they reported findings where at least one demographic attribute was predicted/inferred from at least one form of digital footprint, (ii) the method of prediction was automated, and (iii) the traces were either visible (e.g. tweets) or non-visible (e.g. clickstreams). We identified 327 studies published up until October 2018. Across these articles, 14 demographic attributes were successfully inferred from digital traces; the most studied included gender, age, location, and political orientation. For each of the demographic attributes identified, we provide a database containing the platforms and digital traces examined, sample sizes, accuracy measures and the classification methods applied. Finally, we discuss the main research trends/findings, methodological approaches and recommend directions for future research.", ,-1,-2,
5,Murray,2018,Journal of the American Medical Informatics Association : JAMIA,Automated and flexible identification of complex disease: building a model for systemic lupus erythematosus using noisy labeling,"Accurate and efficient identification of complex chronic conditions in the electronic health record (EHR) is an important but challenging task that has historically relied on tedious clinician review and oversimplification of the disease. Here we adapt methods that allow for automated ""noisy labeling"" of positive and negative controls to create a ""silver standard"" for machine learning to automate identification of systemic lupus erythematosus (SLE). Our final model, which includes both structured data as well as text processing of clinical notes, outperformed all existing algorithms for SLE (AUC 0.97). In addition, we demonstrate how the probabilistic outputs of this model can be adapted to various clinical needs, selecting high thresholds when specificity is the priority and lower thresholds when a more inclusive patient population is desired. Deploying a similar methodology to other complex diseases has the potential to dramatically simplify the landscape of population identification in the EHR. MeSH terms: Electronic Health Records, Machine Learning, Lupus Erythematosus, Phenotype, Algorithms.", ,1,-1,
6,Gregorio,2018,BMC medical research methodology,Prognostic models for intracerebral hemorrhage: systematic review and meta-analysis,"BACKGROUND: Prognostic tools for intracerebral hemorrhage (ICH) patients are potentially useful for ascertaining prognosis and recommended in guidelines to facilitate streamline assessment and communication between providers. In this systematic review with meta-analysis we identified and characterized all existing prognostic tools for this population, performed a methodological evaluation of the conducting and reporting of such studies and compared different methods of prognostic tool derivation in terms of discrimination for mortality and functional outcome prediction. METHODS: PubMed, ISI, Scopus and CENTRAL were searched up to 15th September 2016, with additional studies identified using reference check. Two reviewers independently extracted data regarding the population studied, process of tool derivation, included predictors and discrimination (c statistic) using a predesignated spreadsheet based in the CHARMS checklist. Disagreements were solved by consensus. C statistics were pooled using robust variance estimation and meta-regression was applied for group comparisons using random effect models. RESULTS: Fifty nine studies were retrieved, including 48,133 patients and reporting on the derivation of 72 prognostic tools. Data on discrimination (c statistic) was available for 53 tools, 38 focusing on mortality and 15 focusing on functional outcome. Discrimination was high for both outcomes, with a pooled c statistic of 0.88 for mortality and 0.87 for functional outcome. Forty three tools were regression based and nine tools were derived using machine learning algorithms, with no differences found between the two methods in terms of discrimination (p = 0.490). Several methodological issues however were identified, relating to handling of missing data, low number of events per variable, insufficient length of follow-up, absence of blinding, infrequent use of internal validation, and underreporting of important model performance measures. CONCLUSIONS: Prognostic tools for ICH discriminated well for mortality and functional outcome in derivation studies but methodological issues require confirmation of these findings in validation studies. Logistic regression based risk scores are particularly promising given their good performance and ease of application.", Clinical prediction rules; Intracerebral hemorrhage; Morbidity; Mortality; Prognosis,-2,-2,
7,Taylor,2018,PLoS medicine,Automated detection of moderate and large pneumothorax on frontal chest X-rays using deep convolutional neural networks: A retrospective study,"BACKGROUND: Pneumothorax can precipitate a life-threatening emergency due to lung collapse and respiratory or circulatory distress. Pneumothorax is typically detected on chest X-ray; however, treatment is reliant on timely review of radiographs. Since current imaging volumes may result in long worklists of radiographs awaiting review, an automated method of prioritizing X-rays with pneumothorax may reduce time to treatment. Our objective was to create a large human-annotated dataset of chest X-rays containing pneumothorax and to train deep convolutional networks to screen for potentially emergent moderate or large pneumothorax at the time of image acquisition. METHODS AND FINDINGS: In all, 13,292 frontal chest X-rays (3,107 with pneumothorax) were visually annotated by radiologists. This dataset was used to train and evaluate multiple network architectures. Images showing large- or moderate-sized pneumothorax were considered positive, and those with trace or no pneumothorax were considered negative. Images showing small pneumothorax were excluded from training. Using an internal validation set (n = 1,993), we selected the 2 top-performing models; these models were then evaluated on a held-out internal test set based on area under the receiver operating characteristic curve (AUC), sensitivity, specificity, and positive predictive value (PPV). The final internal test was performed initially on a subset with small pneumothorax excluded (as in training; n = 1,701), then on the full test set (n = 1,990), with small pneumothorax included as positive. External evaluation was performed using the National Institutes of Health (NIH) ChestX-ray14 set, a public dataset labeled for chest pathology based on text reports. All images labeled with pneumothorax were considered positive, because the NIH set does not classify pneumothorax by size. In internal testing, our ""high sensitivity model"" produced a sensitivity of 0.84 (95% CI 0.78-0.90), specificity of 0.90 (95% CI 0.89-0.92), and AUC of 0.94 for the test subset with small pneumothorax excluded. Our ""high specificity model"" showed sensitivity of 0.80 (95% CI 0.72-0.86), specificity of 0.97 (95% CI 0.96-0.98), and AUC of 0.96 for this set. PPVs were 0.45 (95% CI 0.39-0.51) and 0.71 (95% CI 0.63-0.77), respectively. Internal testing on the full set showed expected decreased performance (sensitivity 0.55, specificity 0.90, and AUC 0.82 for high sensitivity model and sensitivity 0.45, specificity 0.97, and AUC 0.86 for high specificity model). External testing using the NIH dataset showed some further performance decline (sensitivity 0.28-0.49, specificity 0.85-0.97, and AUC 0.75 for both). Due to labeling differences between internal and external datasets, these findings represent a preliminary step towards external validation. CONCLUSIONS: We trained automated classifiers to detect moderate and large pneumothorax in frontal chest X-rays at high levels of performance on held-out test data. These models may provide a high specificity screening solution to detect moderate or large pneumothorax on images collected when human review might be delayed, such as overnight. They are not intended for unsupervised diagnosis of all pneumothoraces, as many small pneumothoraces (and some larger ones) are not detected by the algorithm. Implementation studies are warranted to develop appropriate, effective clinician alerts for the potentially critical finding of pneumothorax, and to assess their impact on reducing time to treatment.", ,-1,-1,
8,Salamat,2018,Artificial intelligence in medicine,Diabetic retinopathy techniques in retinal images: A review,"The diabetic retinopathy is the main reason of vision loss in people. Medical experts recognize some clinical, geometrical and haemodynamic features of diabetic retinopathy. These features include the blood vessel area, exudates, microaneurysm, hemorrhages and neovascularization, etc. In Computer Aided Diagnosis (CAD) systems, these features are detected in fundus images using computer vision techniques. In this paper, we review the methods of low, middle and high level vision for automatic detection and classification of diabetic retinopathy.We give a detailed review of 79 algorithms for detecting different features of diabetic retinopathy during the last eight years.", Blood vessels; Computer aided diagnosis; Diabetic retinopathy screening; Exudates; Optic disc,-2,-2,
9,Alla,2018,Systematic reviews,Can automated content analysis be used to assess and improve the use of evidence in mental health policy? A systematic review,"BACKGROUND: This review assesses the utility of applying an automated content analysis method to the field of mental health policy development. We considered the possibility of using the Wordscores algorithm to assess research and policy texts in ways that facilitate the uptake of research into mental health policy. METHODS: The PRISMA framework and the McMaster appraisal tools were used to systematically review and report on the strengths and limitations of the Wordscores algorithm. Nine electronic databases were searched for peer-reviewed journal articles published between 2003 and 2016. Inclusion criteria were (1) articles had to be published in public health, political science, social science or health services disciplines; (2) articles had to be research articles or opinion pieces that used Wordscores; and (3) articles had to discuss both strengths and limitations of using Wordscores for content analysis. RESULTS: The literature search returned 118 results. Twelve articles met the inclusion criteria. These articles explored a range of policy questions and appraised different aspects of the Wordscores method. DISCUSSION: Following synthesis of the material, we identified the following as potential strengths of Wordscores: (1) the Wordscores algorithm can be used at all stages of policy development; (2) it is valid and reliable; (3) it can be used to determine the alignment of health policy drafts with research evidence; (4) it enables existing policies to be revised in the light of research; and (5) it can determine whether changes in policy over time were supported by the evidence. Potential limitations identified were (1) decreased accuracy with short documents, (2) words constitute the unit of analysis and (3) expertise is needed to choose 'reference texts'. CONCLUSIONS: Automated content analysis may be useful in assessing and improving the use of evidence in mental health policies. Wordscores is an automated content analysis option for comparing policy and research texts that could be used by both researchers and policymakers.", Automated content analysis; Evidence-informed policy; Mental health; Research impact; Wordscores,2,-1,
10,Rivas,2018,Journal of medical Internet research,Automatic Classification of Online Doctor Reviews: Evaluation of Text Classifier Algorithms,"BACKGROUND: An increasing number of doctor reviews are being generated by patients on the internet. These reviews address a diverse set of topics (features), including wait time, office staff, doctor's skills, and bedside manners. Most previous work on automatic analysis of Web-based customer reviews assumes that (1) product features are described unambiguously by a small number of keywords, for example, battery for phones and (2) the opinion for each feature has a positive or negative sentiment. However, in the domain of doctor reviews, this setting is too restrictive: a feature such as visit duration for doctor reviews may be expressed in many ways and does not necessarily have a positive or negative sentiment. OBJECTIVE: This study aimed to adapt existing and propose novel text classification methods on the domain of doctor reviews. These methods are evaluated on their accuracy to classify a diverse set of doctor review features. METHODS: We first manually examined a large number of reviews to extract a set of features that are frequently mentioned in the reviews. Then we proposed a new algorithm that goes beyond bag-of-words or deep learning classification techniques by leveraging natural language processing (NLP) tools. Specifically, our algorithm automatically extracts dependency tree patterns and uses them to classify review sentences. RESULTS: We evaluated several state-of-the-art text classification algorithms as well as our dependency tree-based classifier algorithm on a real-world doctor review dataset. We showed that methods using deep learning or NLP techniques tend to outperform traditional bag-of-words methods. In our experiments, the 2 best methods used NLP techniques; on average, our proposed classifier performed 2.19% better than an existing NLP-based method, but many of its predictions of specific opinions were incorrect. CONCLUSIONS: We conclude that it is feasible to classify doctor reviews. Automatically classifying these reviews would allow patients to easily search for doctors based on their personal preference criteria."," patient reported outcome measures; patient satisfaction; quality indicators, health care; supervised machine learning",1,-1,
11,Delgado,2018,Artificial intelligence in medicine,Computational methods for Gene Regulatory Networks reconstruction and analysis: A review,"In the recent years, the vast amount of genetic information generated by new-generation approaches, have led to the need of new data handling methods. The integrative analysis of diverse-nature gene information could provide a much-sought overview to study complex biological systems and processes. In this sense, Gene Regulatory Networks (GRN) arise as an increasingly-promising tool for the modelling and analysis of biological processes. This review is an attempt to summarize the state of the art in the field of GRNs. Essential points in the field are addressed, thereof: (a) the type of data used for network generation, (b) machine learning methods and tools used for network generation, (c) model optimization and (d) computational approaches used for network validation. This survey is intended to provide an overview of the subject for readers to improve their knowledge in the field of GRN for future research.", Gene Network; Gene Network inference; Gene Regulatory Network; Networks validation; Systems biology,-2,-1,
12,Hela,2018,Artificial intelligence in medicine,Early anomaly detection in smart home: A causal association rule-based approach,"As the world's population grows older, an increasing number of people are facing health issues. For the elderly, living alone can be difficult and dangerous. Consequently, smart homes are becoming increasingly popular. A sensor-rich environment can be exploited for healthcare applications, in particular, anomaly detection (AD). The literature review for this paper showed that few works consider environmental factors to detect anomalies. Instead, the focus is on user activity and checking whether it is abnormal, i.e., does not conform to expected behavior. Furthermore, reducing the number of anomalies using early detection is a major issue in many applications. In this context, anomaly-cause discovery may be helpful in recommending actions that may prevent risk. In this paper, we present a novel approach for detecting the risk of anomalies occurring in the environment regarding user activities. The method relies on anomaly-cause extraction from a given dataset using causal association rules mining. These anomaly causes are utilized afterward for real-time analysis to detect the risk of anomalies using the Markov logic network machine learning method. The detected risk allows the method to recommend suitable actions to perform in order to avoid the occurrence of an actual anomaly. The proposed approach is implemented, tested, and evaluated for each contribution using real data obtained from an intelligent environment platform and real data from a clinical datasets. Experimental results prove our approach to be efficient in terms of recognition rate.", Anomaly detection; Causal association rules; Markov logic network; Smart homes,-1,-1,
13,Leroy,2018,Journal of medical Internet research,"Automated Extraction of Diagnostic Criteria From Electronic Health Records for Autism Spectrum Disorders: Development, Evaluation, and Application","BACKGROUND: Electronic health records (EHRs) bring many opportunities for information utilization. One such use is the surveillance conducted by the Centers for Disease Control and Prevention to track cases of autism spectrum disorder (ASD). This process currently comprises manual collection and review of EHRs of 4- and 8-year old children in 11 US states for the presence of ASD criteria. The work is time-consuming and expensive. OBJECTIVE: Our objective was to automatically extract from EHRs the description of behaviors noted by the clinicians in evidence of the diagnostic criteria in the Diagnostic and Statistical Manual of Mental Disorders (DSM). Previously, we reported on the classification of entire EHRs as ASD or not. In this work, we focus on the extraction of individual expressions of the different ASD criteria in the text. We intend to facilitate large-scale surveillance efforts for ASD and support analysis of changes over time as well as enable integration with other relevant data. METHODS: We developed a natural language processing (NLP) parser to extract expressions of 12 DSM criteria using 104 patterns and 92 lexicons (1787 terms). The parser is rule-based to enable precise extraction of the entities from the text. The entities themselves are encompassed in the EHRs as very diverse expressions of the diagnostic criteria written by different people at different times (clinicians, speech pathologists, among others). Due to the sparsity of the data, a rule-based approach is best suited until larger datasets can be generated for machine learning algorithms. RESULTS: We evaluated our rule-based parser and compared it with a machine learning baseline (decision tree). Using a test set of 6636 sentences (50 EHRs), we found that our parser achieved 76% precision, 43% recall (ie, sensitivity), and >99% specificity for criterion extraction. The performance was better for the rule-based approach than for the machine learning baseline (60% precision and 30% recall). For some individual criteria, precision was as high as 97% and recall 57%. Since precision was very high, we were assured that criteria were rarely assigned incorrectly, and our numbers presented a lower bound of their presence in EHRs. We then conducted a case study and parsed 4480 new EHRs covering 10 years of surveillance records from the Arizona Developmental Disabilities Surveillance Program. The social criteria (A1 criteria) showed the biggest change over the years. The communication criteria (A2 criteria) did not distinguish the ASD from the non-ASD records. Among behaviors and interests criteria (A3 criteria), 1 (A3b) was present with much greater frequency in the ASD than in the non-ASD EHRs. CONCLUSIONS: Our results demonstrate that NLP can support large-scale analysis useful for ASD surveillance and research. In the future, we intend to facilitate detailed analysis and integration of national datasets.", Autism Spectrum Disorder; Dsm; complex entity extraction; decision tree; electronic health records; machine learning; natural language processing; parser,1,-1,
14,Thom,2018,Research synthesis methods,Automated methods to test connectedness and quantify indirectness of evidence in network meta-analysis,"Network meta-analysis compares multiple treatments from studies that form a connected network of evidence. However, for complex networks, it is not easy to see if the network is connected. We use simple techniques from graph theory to test the connectedness of evidence networks in network meta-analysis. The method is to build the adjacency matrix for a network, with rows and columns corresponding to the treatments in the network and entries being one or zero depending on whether the treatments have been compared or not, and with zeros along the diagonal. Manipulation of this matrix gives the indirect connection matrix. The entries of this matrix determine whether two treatments can be compared, directly or indirectly. We also describe the distance matrix, which gives the minimum number of steps in the network required to compare a pair of treatments. This is a useful assessment of an indirect comparison as each additional step requires further assumptions of homogeneity in, for example, design and target populations of included trials. If there are no loops in the network, the distance is a measure of the degree of assumptions needed; it is approximately this with loops. We illustrate our methods using several constructed examples and giving R code for computation. We have also implemented the techniques in the Stata package ""network."" The methods provide a fast way to ensure comparisons are only made between connected treatments and to assess the degree of indirectness of a comparison.", Connectedness testing; Disconnected network; Graph theory; Indirect comparison; Network meta-analysis,2,-2,
15,Mavragani,2018,Journal of medical Internet research,"Assessing the Methods, Tools, and Statistical Approaches in Google Trends Research: Systematic Review","BACKGROUND: In the era of information overload, are big data analytics the answer to access and better manage available knowledge? Over the last decade, the use of Web-based data in public health issues, that is, infodemiology, has been proven useful in assessing various aspects of human behavior. Google Trends is the most popular tool to gather such information, and it has been used in several topics up to this point, with health and medicine being the most focused subject. Web-based behavior is monitored and analyzed in order to examine actual human behavior so as to predict, better assess, and even prevent health-related issues that constantly arise in everyday life. OBJECTIVE: This systematic review aimed at reporting and further presenting and analyzing the methods, tools, and statistical approaches for Google Trends (infodemiology) studies in health-related topics from 2006 to 2016 to provide an overview of the usefulness of said tool and be a point of reference for future research on the subject. METHODS: Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines for selecting studies, we searched for the term ""Google Trends"" in the Scopus and PubMed databases from 2006 to 2016, applying specific criteria for types of publications and topics. A total of 109 published papers were extracted, excluding duplicates and those that did not fall inside the topics of health and medicine or the selected article types. We then further categorized the published papers according to their methodological approach, namely, visualization, seasonality, correlations, forecasting, and modeling. RESULTS: All the examined papers comprised, by definition, time series analysis, and all but two included data visualization. A total of 23.1% (24/104) studies used Google Trends data for examining seasonality, while 39.4% (41/104) and 32.7% (34/104) of the studies used correlations and modeling, respectively. Only 8.7% (9/104) of the studies used Google Trends data for predictions and forecasting in health-related topics; therefore, it is evident that a gap exists in forecasting using Google Trends data. CONCLUSIONS: The monitoring of online queries can provide insight into human behavior, as this field is significantly and continuously growing and will be proven more than valuable in the future for assessing behavioral changes and providing ground for research using data that could not have been accessed otherwise.", Google Trends; big data; health assessment; infodemiology; medicine; review; statistical analysis,1,-2,
16,Szczurowski,2018,Studies in health technology and informatics,Emulating Perceptual Experience of Color Vision Deficiency with Virtual Reality,"One of the major goals of Universal Design is to create experiences that are inclusive to all users, including those affected by Color Vision Deficiency. Color Vision Deficiency might have a significant impact on a users' perception of the content or the environment. There is a range of tools already available, that can be used to either aid or automate the process of readability testing for digital interfaces and content in respect to Color Vision Deficiency. Two different approaches to addressing this issue can be found. A brief review of such methodologies is provided in this paper. The first approach (user-end) attempts to solve the problem by altering mediation between the user and the content. The second (design-end) allows the designer to view an image, or color scheme altered to recreate the perceptual experience of a user affected by Color Vision Deficiency and asses the design from the perspective of a color-blind user. With an implemented proof-of-concept we investigate the potential use of Virtual Reality Head-Mounted Displays to employ similar methodology, to allow designers or interior decorators to experience physical environments (i.e.: classroom, library or a cafeteria) from the perspective of a color-blind person. Such tools might increase the designers' empathy towards color-blind users but also allow them to identify visual components, such as infographics or advertisement, in a physical environment that are poorly visible to color-blind users. Such tools could be developed by taking advantage of a modern Head-Mounted Displays six degrees of freedom tracking, a 360 camera and color processing filters applied during post-processing at run-time, allowing a designer to easily switch between different types of colorblindness emulation.", Color Vision Deficiency; Color-blindness; Design; Head Mounted Display; Virtual Reality,-1,-2,
17,Rast,2018,Systematic reviews,Protocol of a systematic review on the application of wearable inertial sensors to quantify everyday life motor activity in people with mobility impairments,"BACKGROUND: People with mobility impairments may have difficulties in everyday life motor activities, and assessing these difficulties is crucial to plan rehabilitation interventions and evaluate their effectiveness. Wearable inertial sensors enable long-term monitoring of motor activities in a patient's habitual environment and complement clinical assessments which are conducted in a standardised environment. The application of wearable sensors requires appropriate data processing algorithms to estimate clinically meaningful outcome measures, and this review will provide an overview of previously published measures, their underlying algorithms, sensor placement, and measurement properties such as validity, reproducibility, and feasibility. METHODS: We will screen the literature for studies which applied inertial sensors to people with mobility impairments in free-living conditions, described the data processing algorithm reproducibly, and calculated everyday life motor activity-related outcome measures. Three databases (MEDLINE, EMBASE, and SCOPUS) will be searched with terms out of four different categories: study population, measurement tool, algorithm, and outcome measure. Abstracts and full texts will be screened independently by the two review authors, and disagreement will be solved by discussion and consensus. Data will be extracted by one of the review authors and verified by the other. It includes the type of outcome measures, the underlying data processing algorithm, the required sensor technology, the corresponding sensor placement, the measurement properties, and the target population. We expect to find a high heterogeneity of outcome measures and will therefore provide a narrative synthesis of the extracted data. DISCUSSION: This review will facilitate the selection of an appropriate sensor setup for future applications, contain recommendations about the design of data processing algorithms as well as their evaluation procedure, and present a gap for innovative, new algorithms, and devices. SYSTEMATIC REVIEW REGISTRATION: International prospective register of systematic reviews (PROSPERO): CRD42017069865 .", Accelerometer; Activities of daily living; Algorithms; Disabled persons; Gyroscope; Inertial measurement unit; Machine learning; Patients; Pattern recognition; Rehabilitation,-2,-1,
18,Waffenschmidt,2018,Systematic reviews,Effective study selection using text mining or a single-screening approach: a study protocol,"BACKGROUND: Systematic information retrieval generally requires a two-step selection process for studies, which is conducted by two persons independently of one another (double-screening approach). To increase efficiency, two methods seem promising, which will be tested in the planned study: the use of text mining to prioritize search results as well as the involvement of only one person in the study selection process (single-screening approach). The aim of the present study is to examine the following questions related to the process of study selection: Can the use of the Rayyan or EPPI Reviewer tools to prioritize the results of study selection increase efficiency? How accurately does a single-screening approach identify relevant studies? Which advantages or disadvantages (e.g., shortened screening time or increase in the number of full texts ordered) does a single-screening versus a double-screening approach have? METHODS: Our study is a prospective analysis of study selection processes based on benefit assessments of drug and non-drug interventions. It consists of two parts: firstly, the evaluation of a single-screening approach based on a sample size calculation (11 study selection processes, including 33 single screenings) and involving different screening tools and, secondly, the evaluation of the conventional double-screening approach based on five conventional study selection processes. In addition, the advantages and disadvantages of the single-screening versus the double-screening approach with regard to the outcomes ""number of full texts ordered"" and ""time required for study selection"" are analyzed. The previous work experience of the screeners is considered as a potential effect modifier. DISCUSSION: No study comparing the features of prioritization tools is currently available. Our study can thus contribute to filling this evidence gap. This study is also the first to investigate a range of questions surrounding the screening process and to include an a priori sample size calculation, thus enabling statistical conclusions. In addition, the impact of missing studies on the conclusion of a benefit assessment is calculated. SYSTEMATIC REVIEW REGISTRATION: Not applicable.", Citation screening; Screening prioritization; Semi-automation; Systematic reviews,2,2,
19,Ienca,2018,PloS one,Considerations for ethics review of big data health research: A scoping review,"Big data trends in biomedical and health research enable large-scale and multi-dimensional aggregation and analysis of heterogeneous data sources, which could ultimately result in preventive, diagnostic and therapeutic benefit. The methodological novelty and computational complexity of big data health research raises novel challenges for ethics review. In this study, we conducted a scoping review of the literature using five databases to identify and map the major challenges of health-related big data for Ethics Review Committees (ERCs) or analogous institutional review boards. A total of 1093 publications were initially identified, 263 of which were included in the final synthesis after abstract and full-text screening performed independently by two researchers. Both a descriptive numerical summary and a thematic analysis were performed on the full-texts of all articles included in the synthesis. Our findings suggest that while big data trends in biomedicine hold the potential for advancing clinical research, improving prevention and optimizing healthcare delivery, yet several epistemic, scientific and normative challenges need careful consideration. These challenges have relevance for both the composition of ERCs and the evaluation criteria that should be employed by ERC members when assessing the methodological and ethical viability of health-related big data studies. Based on this analysis, we provide some preliminary recommendations on how ERCs could adaptively respond to those challenges. This exploration is designed to synthesize useful information for researchers, ERCs and relevant institutional bodies involved in the conduction and/or assessment of health-related big data research.", ,1,-2,
20,Mahmut,2018,Studies in health technology and informatics,A Speech Sound Disorder Screening System Database Structure,"This paper makes a brief review of several database structures of Computer-Based Speech Therapy (CBST) systems and solutions and describes the screening method, an experimental study conducted to validate the screening algorithm and a database structure for the Information Entropy-Based Sound Speech Disorder (SSD) Screening System aimed at by our research project. The final part briefly presents the essential design criteria and further development.", Computer-based Speech Therapy Tools; Dyslalia; Information Entropy; Screening System; Speech Sound Disorders,-2,-2,
21,Liyanage,2018,Studies in health technology and informatics,Common Data Models (CDMs) to Enhance International Big Data Analytics: A Diabetes Use Case to Compare Three CDMs,"Common data models (CDM) have enabled the simultaneous analysis of disparate and large data sources. A literature review identified three relevant CDMs: The Observational Medical Outcomes Partnership (OMOP) was the most cited; next the Sentinel; and then the Patient Centered Outcomes Research Institute (PCORI). We tested these three CDMs with fifteen pre-defined criteria for a diabetes cohort study use case, assessing the benefit (good diabetes control), risk (hypoglycaemia) and cost effectiveness of recently licenced medications. We found all three CDMs have a useful role in planning collaborative research and enhance analysis of data cross jurisdiction. However, the number of pre-defined criteria achieved by these three CDMs varied. OMOP met 14/15, Sentinel 13/15, and PCORI 10/15. None met the privacy level we specified, and most of the other gaps were clinical and cost outcome related data.", Common data models; Costs and cost analysis; Diabetes Mellitus; Medical record systems; Organization and administration; computerized; data harmonisation,1,-2,
22,Chazard,2018,Studies in health technology and informatics,Secondary Use of Healthcare Structured Data: The Challenge of Domain-Knowledge Based Extraction of Features,"Secondary use of clinical structured data takes an important place in healthcare research. It was first described by Fayyad as ""knowledge discovery in databases"". Feature extraction is an important phase but received little attention. The objectives of this paper are: 1) to propose an updated representation of data reuse in healthcare, 2) to illustrate methods and objectives of feature extraction, and 3) to discuss the place of domain-specific knowledge. MATERIAL AND METHODS: an updated representation is proposed. Then, a case study consists of automatically identifying acute renal failure and discovering risk factors, by secondary use of structured data. Finally, a literature review published par Meystre et al. is analyzed. RESULTS: 1) we propose a description of data reuse in 5 phases. Phase 1 is data preprocessing (cleansing, linkage, terminological alignment, unit conversions, deidentification), it enables to construct a data warehouse. Phase 2 is feature extraction. Phase 3 is statistical and graphical mining. Phase 4 consists of expert filtering and reorganization of statistical results. Phase 5 is decision making. 2) The case study illustrates how time-dependent features can be extracted from laboratory results and drug administrations, using domain-specific knowledge. 3) Among the 200 papers cited by Meystre et al., the first and last authors were affiliated to health institutions in 74% (68% for methodological papers, and 79% for applied papers). DISCUSSION: features extraction has a major impact on success of data reuse. Specific knowledge-based reasoning takes an important place in feature extraction, which requires tight collaboration between computer scientists, statisticians, and health professionals.", Data reuse; data transformation; feature extraction,1,-2,
23,Kreiner,2018,Studies in health technology and informatics,Twister: A Tool for Reducing Screening Time in Systematic Literature Reviews,"Systematic reviews are widely used as a tool for decision making to establish new clinical guidelines. Reviews can be time-consuming, potentially leaving authors with thousands of citations to screen. Software tools for assisting reviewers in this process are available, however, only few use text mining techniques to reduce screening time. In this work, we introduce Twister, a web-based tool for semi-automated literature reviews with broad research questions. We discuss how two text mining techniques can be used to (a) extract data elements from clinical abstracts and (b) how citations can be clustered based on a key phrase-extraction to help reviewers reduce screening time. We present the overall system architecture, design consideration and system implementation.", machine learning; systematic review process; text mining,2,2,
24,Dey,2018,PloS one,"Understanding intersections of social determinants of maternal healthcare utilization in Uttar Pradesh, India","OBJECTIVE: To explore intersections of social determinants of maternal healthcare utilization using the Classification and Regression Trees (CART) algorithm which is a machine-learning method used to construct prediction models. METHODS: Institutional review board approval for this study was granted from Public Health Service-Ethical Review Board (PHS-ERB) and from the Health Ministry Screening Committee (HMSC) facilitated by Indian Council for Medical Research (ICMR). IRB review and approval for the current analyses was obtained from University of California, San Diego. Cross-sectional data were collected from women with children aged 0-11 months (n = 5,565) from rural households in 25 districts of Uttar Pradesh, India. Participants were surveyed on maternal healthcare utilization including registration of pregnancy (model-1), receipt of antenatal care (ANC) during pregnancy (model-2), and delivery at health facilities (model -3). Social determinants of health including wealth, social group, literacy, religion, and early age at marriage were captured during the survey. The Classification and Regression Tree (CART) algorithm was used to explore intersections of social determinants of healthcare utilization. RESULTS: CART analyses highlight the intersections, particularly of wealth and literacy, in maternal healthcare utilization in Uttar Pradesh. Model-1 documents that women who are poorer, illiterate and Muslim are less likely to have their pregnancies registered (71.4% vs. 86.0% in the overall sample). Model-2 documents that poorer, illiterate women had the lowest ANC coverage (37.7% vs 45% in the overall sample). Model-3, developed for deliveries at health facilities, highlighted that illiterate and poor women have the lowest representation among facility deliveries (59.6% vs. 69% in the overall sample). CONCLUSION: This paper explores the interactions between determinants of maternal healthcare utilization indicators. The findings in this paper highlights that the interaction of wealth and literacy can play a very strong role in accentuating or diminishing healthcare utilization among women. The study also reveals that religion and women's age at marriage also interact with wealth and literacy to create substantial disparities in utilization. The study provides insights into the effect of intersections of determinants, and highlights the importance of using a more nuanced understanding of the impact of co-occurring forms of marginalization to effectively tackle inequities in healthcare utilization.", ,-2,-2,
25,Pradhan,2019,Journal of clinical epidemiology,Automatic extraction of quantitative data from ClinicalTrials.gov to conduct meta-analyses,"OBJECTIVES: Systematic reviews and meta-analyses are labor-intensive and time-consuming. Automated extraction of quantitative data from primary studies can accelerate this process. ClinicalTrials.gov, launched in 2000, is the world's largest trial repository of results data from clinical trials; it has been used as a source instead of journal articles. We have developed a Web application called EXACT (EXtracting Accurate efficacy and safety information from ClinicalTrials.gov) that allows users without advanced programming skills to automatically extract data from ClinicalTrials.gov in analysis-ready format. We have also used the automatically extracted data to examine the reproducibility of meta-analyses in three published systematic reviews. STUDY DESIGN AND SETTING: We developed a Python-based software application (EXACT) that automatically extracts data required for meta-analysis from the ClinicalTrials.gov database in a spreadsheet format. We confirmed the accuracy of the extracted data and then used those data to repeat meta-analyses in three published systematic reviews. To ensure that we used the same statistical methods and outcomes as the published systematic reviews, we repeated the meta-analyses using data manually extracted from the relevant journal articles. For the outcomes whose results we were able to reproduce using those journal article data, we examined the usability of ClinicalTrials.gov data. RESULTS: EXACT extracted data at ClincalTrials.gov with 100% accuracy, and it required 60% less time than the usual practice of manually extracting data from journal articles. We found that 87% of the data elements extracted using EXACT matched those extracted manually from the journal articles. We were able to reproduce 24 of 28 outcomes using the journal article data. Of these 24 outcomes, we were able to reproduce 83.3% of the published estimates using data at ClinicalTrials.gov. CONCLUSION: EXACT (http://bio-nlp.org/EXACT) automatically and accurately extracted data elements from ClinicalTrials.gov and thus reduced time in data extraction. The ClinicalTrials.gov data reproduced most meta-analysis results in our study, but this conclusion needs further validation.", Automatic data extraction; ClinicalTrials.gov; Meta-analysis; Reproducibility; Simeprevir; Systematic review; Trametinib; Vortioxetine,2,2,
26,Pereira,2018,Artificial intelligence in medicine,A survey on computer-assisted Parkinson's Disease diagnosis,"BACKGROUND AND OBJECTIVE: In this work, we present a systematic review concerning the recent enabling technologies as a tool to the diagnosis, treatment and better quality of life of patients diagnosed with Parkinson's Disease (PD), as well as an analysis of future trends on new approaches to this end. METHODS: In this review, we compile a number of works published at some well-established databases, such as Science Direct, IEEEXplore, PubMed, Plos One, Multidisciplinary Digital Publishing Institute (MDPI), Association for Computing Machinery (ACM), Springer and Hindawi Publishing Corporation. Each selected work has been carefully analyzed in order to identify its objective, methodology and results. RESULTS: The review showed the majority of works make use of signal-based data, which are often acquired by means of sensors. Also, we have observed the increasing number of works that employ virtual reality and e-health monitoring systems to increase the life quality of PD patients. Despite the different approaches found in the literature, almost all of them make use of some sort of machine learning mechanism to aid the automatic PD diagnosis. CONCLUSIONS: The main focus of this survey is to consider computer-assisted diagnosis, and how effective they can be when handling the problem of PD identification. Also, the main contribution of this review is to consider very recent works only, mainly from 2015 and 2016.", Machine Learning; Parkinson's Disease; Parkinsonian,-1,-2,
27,Kal,2018,PloS one,Does implicit motor learning lead to greater automatization of motor skills compared to explicit motor learning? A systematic review,"BACKGROUND: Implicit motor learning is considered to be particularly effective for learning sports-related motor skills. It should foster movement automaticity and thereby facilitate performance in multitasking and high-pressure environments. To scrutinize this hypothesis, we systematically reviewed all studies that compared the degree of automatization achieved (as indicated by dual-task performance) after implicit compared to explicit interventions for sports-related motor tasks. METHODS: For this systematic review (CRD42016038249) conventional (MEDLINE, CENTRAL, Embase, PsycINFO, SportDiscus, Web of Science) and grey literature were searched. Two reviewers independently screened reports, extracted data, and performed risk of bias assessment. Implicit interventions of interest were analogy-, errorless-, dual-task-, and external focus learning. Data analysis involved descriptive synthesis of group comparisons on absolute motor dual-task (DT) performance, and motor DT performance relative to single-task motor performance (motor DTCs). RESULTS: Of the 4125 reports identified, we included 25 controlled trials that described 39 implicit-explicit group comparisons. Risk of bias was unclear across trials. Most comparisons did not show group differences. Some comparisons showed superior absolute motor DT performance (N = 2), superior motor DTCs (N = 4), or both (N = 3) for the implicit compared to the explicit group. The explicit group showed superior absolute motor DT performance in two comparisons. CONCLUSIONS: Most comparisons did not show group differences in automaticity. The remaining comparisons leaned more toward a greater degree of movement automaticity after implicit learning than explicit learning. However, due to an overall unclear risk of bias the strength of the evidence is level 3. Motor learning-specific guidelines for design and especially reporting are warranted to further strengthen the evidence and facilitate low-risk-of-bias trials.", ,-2,-2,
28,Cheung,2018,Research synthesis methods,Some reflections on combining meta-analysis and structural equation modeling,"Meta-analysis and structural equation modeling (SEM) are 2 of the most prominent statistical techniques employed in the behavioral, medical, and social sciences. They each have their own well-established research communities, terminologies, statistical models, software packages, and journals (Research Synthesis Methods and Structural Equation Modeling: A Multidisciplinary Journal). In this paper, I will provide some personal reflections on combining meta-analysis and SEM in the forms of meta-analytic SEM and SEM-based meta-analysis. The critical contributions of Becker (1992), Shadish (1992), and Viswesvaran and Ones (1995) in the early development of meta-analytic SEM are highlighted. Another goal of the paper is to illustrate how meta-analysis can be extended and integrated with other techniques to address new research questions such as the analysis of Big Data. I hope that this paper may stimulate more research development in the area of combining meta-analysis and SEM.", SEM-based meta-analysis; meta-analysis; meta-analytic structural equation modeling; structural equation modeling,1,-2,
29,Ngoo,2018,International journal of medical informatics,Fighting Melanoma with Smartphones: A Snapshot of Where We are a Decade after App Stores Opened Their Doors,"BACKGROUND: Smartphone applications (""apps"") exist for primary and secondary prevention of melanoma. Our aim was to review currently available apps for community, patient and generalist clinician users. DESIGN: Prospective study, April 2017 - May 2017. MAIN OUTCOMES: Appropriate apps available to Android and Apple smartphones were assessed in regards to app specific information (target user, cost, store rating, last update), functions offered and clinician, professional or scientific input and or peer review. Comparison was made with a similar 2014 review of the app market. RESULTS: 43 apps meeting inclusion criteria were found. Compared to 2014, 24 of 43 (55.8%) were new, and apps performing automated image analysis declined from 46.1% to 23.3% market share. 23 of 43 (53.4%) were free to download, 48.8% (n = 20) required payments of some form. The most common functionality was monitoring/tracking with 24 of 43 (55.8%) apps performing this. 15 of 43 apps (34.9%) reported clinician, professional or scientific input; in 2014 it was only 4 of 39 (10.3%). 2 of 43 apps (5%) mentioned peer-reviewed evidence along with professional input. Not all apps had ratings. On Android 20 of 22 apps had ratings; average app rating was 3.5, range 1.6 to 4.6. On Apple, 13 of 13 had ratings; average rating was 3.5; range 1- 5. CONCLUSIONS: Since 2014 there have been an expanding and changing landscape of apps targeting melanoma diagnosis. There remains a lack of evidence backing their efficacy. This is concerning given their public availability and the gravity of their subject matter.", ,-2,-2,
30,Anderson,2018,Research synthesis methods,Systematic reviews and tech mining: A methodological comparison with case study,"When the Medical Library Association identified questions critical for the future of the profession, it assigned groups to use systematic reviews to find the answers to these questions. Group 6, whose question was on emerging technologies, recognized early on that the systematic review process would not work well for this question, which looks forward to predict future trends, whereas the systematic review process looks back in time. We searched for new methodologies that were more appropriate to our question, developing a process that combined systematic review, text mining, and visualization techniques. We then discovered tech mining, which is very similar to the process we had created. In this paper, we describe our research design and compare tech mining and systematic review methodologies. There are similarities and differences in each process: Both use a defined research question, deliberate database selection, careful and iterative search strategy development, broad data collection, and thoughtful data analysis. However, the focus of the research differs significantly, with systematic reviews looking to the past and tech mining mainly to the future. Our comparison demonstrates that each process can be enhanced from a purposeful consideration of the procedures of the other. Tech mining would benefit from the inclusion of a librarian on their research team and a greater attention to standards and collaboration in the research project. Systematic reviews would gain from the use of tech mining tools to enrich their data analysis and corporate management communication techniques to promote the adoption of their findings.", research methodologies; systematic reviews; tech mining,2,-1,
31,Pacheco,2018,Journal of the American Medical Informatics Association : JAMIA,A case study evaluating the portability of an executable computable phenotype algorithm across multiple institutions and electronic health record environments,"Electronic health record (EHR) algorithms for defining patient cohorts are commonly shared as free-text descriptions that require human intervention both to interpret and implement. We developed the Phenotype Execution and Modeling Architecture (PhEMA, http://projectphema.org) to author and execute standardized computable phenotype algorithms. With PhEMA, we converted an algorithm for benign prostatic hyperplasia, developed for the electronic Medical Records and Genomics network (eMERGE), into a standards-based computable format. Eight sites (7 within eMERGE) received the computable algorithm, and 6 successfully executed it against local data warehouses and/or i2b2 instances. Blinded random chart review of cases selected by the computable algorithm shows PPV >/=90%, and 3 out of 5 sites had >90% overlap of selected cases when comparing the computable algorithm to their original eMERGE implementation. This case study demonstrates potential use of PhEMA computable representations to automate phenotyping across different EHR systems, but also highlights some ongoing challenges.", ,1,-2,
32,Li,2018,Journal of medical Internet research,Understanding Users' Vaping Experiences from Social Media: Initial Study Using Sentiment Opinion Summarization Techniques,"BACKGROUND: E-liquid is one of the main components in electronic nicotine delivery systems (ENDS). ENDS review comments could serve as an early warning on use patterns and even function to serve as an indicator of problems or adverse events pertaining to the use of specific e-liquids-much like types of responses tracked by the Food and Drug Administration (FDA) regarding medications. OBJECTIVE: This study aimed to understand users' ""vaping"" experience using sentiment opinion summarization techniques, which can help characterize how consumers think about specific e-liquids and their characteristics (eg, flavor, throat hit, and vapor production). METHODS: We collected e-liquid reviews on JuiceDB from June 27, 2013 to December 31, 2017 using its public application programming interface. The dataset contains 27,070 reviews for 8058 e-liquid products. Each review is accompanied by an overall rating and a set of 4 aspect ratings of an e-liquid, each on a scale of 1-5: flavor accuracy, throat hit, value, and cloud production. An iterative dichotomiser 3 (ID3)-based influential aspect analysis model was adopted to learn the key elements that impact e-liquid use. Then, fine-grained sentiment analysis was employed to mine opinions on various aspects of vaping experience related to e-liquids. RESULTS: We found that flavor accuracy and value were the two most important aspects that affected users' sentiments toward e-liquids. Of reviews in JuiceDB, 67.83% (18,362/27,070) were positive, while 12.67% (3430/27,070) were negative. This indicates that users generally hold positive attitudes toward e-liquids. Among the 9 flavors, fruity and sweet were the two most popular. Great and sweet tastes, reasonable value, and strong throat hit made users satisfied with fruity and sweet flavors, whereas ""strange"" tastes made users dislike those flavors. Meanwhile, users complained about some e-liquids' steep or expensive prices, bad quality, and harsh throat hit. There were 2342 fruity e-liquids and 2049 sweet e-liquids. There were 55.81% (1307/2342) and 59.83% (1226/2049) positive sentiments and 13.62% (319/2342) and 12.88% (264/2049) negative sentiments toward fruity e-liquids and sweet e-liquids, respectively. Great flavors and good vapors contributed to positive reviews of fruity and sweet products. However, bad tastes such as ""sour"" or ""bitter"" resulted in negative reviews. These findings can help businesses and policy makers to further improve product quality and formulate effective policy. CONCLUSIONS: This study provides an effective mechanism for analyzing users' ENDS vaping experience based on sentiment opinion summarization techniques. Sentiment opinions on aspect and products can be found using our method, which is of great importance to monitor e-liquid products and improve work efficiency.", JuiceDB; e-cigarette; e-liquid; electronic nicotine delivery systems; infodemiology; sentiment opinion summarization; social media; vaping,-1,-2,
33,Langlois,2018,Research synthesis methods,Discriminating between empirical studies and nonempirical works using automated text classification,"OBJECTIVE: Identify the most performant automated text classification method (eg, algorithm) for differentiating empirical studies from nonempirical works in order to facilitate systematic mixed studies reviews. METHODS: The algorithms were trained and validated with 8050 database records, which had previously been manually categorized as empirical or nonempirical. A Boolean mixed filter developed for filtering MEDLINE records (title, abstract, keywords, and full texts) was used as a baseline. The set of features (eg, characteristics from the data) included observable terms and concepts extracted from a metathesaurus. The efficiency of the approaches was measured using sensitivity, precision, specificity, and accuracy. RESULTS: The decision trees algorithm demonstrated the highest performance, surpassing the accuracy of the Boolean mixed filter by 30%. The use of full texts did not result in significant gains compared with title, abstract, keywords, and records. Results also showed that mixing concepts with observable terms can improve the classification. SIGNIFICANCE: Screening of records, identified in bibliographic databases, for relevant studies to include in systematic reviews can be accelerated with automated text classification.", automated text classification; decision tree; health care; research method; support vector machine; systematic review,2,1,
34,Wanner,2018,Research synthesis methods,Design and implementation of a tool for conversion of search strategies between PubMed and Ovid MEDLINE,"BACKGROUND: Both PubMed and Ovid MEDLINE contain records from the MEDLINE database. However, there are subtle differences in content, functionality, and search syntax between the two. There are many instances in which researchers may wish to search both interfaces, such as when conducting supplementary searching for a systematic review to retrieve a unique content from PubMed or when using a previously published search strategy from a different interface, but little guidance on how to best conduct these searches. The aim of this project is to describe differences in search functionality between Ovid MEDLINE and PubMed, provide guidance for converting search strategies between the two, and develop an easy-to-use, freely available web-based tool to automate search syntax translations. CASE PRESENTATION: In this paper, we present a custom-built freely available online tool, Medline Transpose, to streamline the process of converting search strategies between Ovid MEDLINE and PubMed. With this tool, users can paste a strategy formatted for one interface into the search box and immediately retrieve an output formatted for use in the other interface, with recommendations for changes that users can make to the strategy where an exact translation does not exist. CONCLUSION: This novel approach has the potential to reduce time and errors that database users spend translating search strategies.", Medline; PubMed; bibliographic databases; information science; information storage and retrieval; literature searching; software tool; systematic review,1,-1,
35,Tvardik,2018,International journal of medical informatics,Accuracy of using natural language processing methods for identifying healthcare-associated infections,"OBJECTIVE: There is a growing interest in using natural language processing (NLP) for healthcare-associated infections (HAIs) monitoring. A French project consortium, SYNODOS, developed a NLP solution for detecting medical events in electronic medical records for epidemiological purposes. The objective of this study was to evaluate the performance of the SYNODOS data processing chain for detecting HAIs in clinical documents. MATERIALS AND METHODS: The collection of textual records in these hospitals was carried out between October 2009 and December 2010 in three French University hospitals (Lyon, Rouen and Nice). The following medical specialties were included in the study: digestive surgery, neurosurgery, orthopedic surgery, adult intensive-care units. Reference Standard surveillance was compared with the results of automatic detection using NLP. Sensitivity on 56 HAI cases and specificity on 57 non-HAI cases were calculated. RESULTS: The accuracy rate was 84% (n=95/113). The overall sensitivity of automatic detection of HAIs was 83.9% (CI 95%: 71.7-92.4) and the specificity was 84.2% (CI 95%: 72.1-92.5). The sensitivity varies from one specialty to the other, from 69.2% (CI 95%: 38.6-90.9) for intensive care to 93.3% (CI 95%: 68.1-99.8) for orthopedic surgery. The manual review of classification errors showed that the most frequent cause was an inaccurate temporal labeling of medical events, which is an important factor for HAI detection. CONCLUSION: This study confirmed the feasibility of using NLP for the HAI detection in hospital facilities. Automatic HAI detection algorithms could offer better surveillance standardization for hospital comparisons."," Decision support systems, Clinical; Epidemiology; Healthcare-associated infections; Medical records systems, computerized; Natural language processing",1,-2,
36,Richter,2018,Artificial intelligence in medicine,A review of statistical and machine learning methods for modeling cancer risk using structured clinical data,"Advancements are constantly being made in oncology, improving prevention and treatment of cancers. To help reduce the impact and deadliness of cancers, they must be detected early. Additionally, there is a risk of cancers recurring after potentially curative treatments are performed. Predictive models can be built using historical patient data to model the characteristics of patients that developed cancer or relapsed. These models can then be deployed into clinical settings to determine if new patients are at high risk for cancer development or recurrence. For large-scale predictive models to be built, structured data must be captured for a wide range of diverse patients. This paper explores current methods for building cancer risk models using structured clinical patient data. Trends in statistical and machine learning techniques are explored, and gaps are identified for future research. The field of cancer risk prediction is a high-impact one, and research must continue for these models to be embraced for clinical decision support of both practitioners and patients.", Cancer prediction; Cancer recurrence; Cancer relapse; Data mining; Electronic health records; Machine learning,1,-2,
37,Lanera,2018,Journal of clinical epidemiology,Extending PubMed searches to ClinicalTrials.gov through a machine learning approach for systematic reviews,"OBJECTIVES: Despite their essential role in collecting and organizing published medical literature, indexed search engines are unable to cover all relevant knowledge. Hence, current literature recommends the inclusion of clinical trial registries in systematic reviews (SRs). This study aims to provide an automated approach to extend a search on PubMed to the ClinicalTrials.gov database, relying on text mining and machine learning techniques. STUDY DESIGN AND SETTING: The procedure starts from a literature search on PubMed. Next, it considers the training of a classifier that can identify documents with a comparable word characterization in the ClinicalTrials.gov clinical trial repository. Fourteen SRs, covering a broad range of health conditions, are used as case studies for external validation. A cross-validated support-vector machine (SVM) model was used as the classifier. RESULTS: The sensitivity was 100% in all SRs except one (87.5%), and the specificity ranged from 97.2% to 99.9%. The ability of the instrument to distinguish on-topic from off-topic articles ranged from an area under the receiver operator characteristic curve of 93.4% to 99.9%. CONCLUSION: The proposed machine learning instrument has the potential to help researchers identify relevant studies in the SR process by reducing workload, without losing sensitivity and at a small price in terms of specificity.", Clinical trial registry; Indexed search engine; Machine learning; Meta-analysis; Systematic review; Text mining,2,2,
38,Schwendimann,2018,BMC health services research,"The occurrence, types, consequences and preventability of in-hospital adverse events - a scoping review","BACKGROUND: Adverse events (AEs) seriously affect patient safety and quality of care, and remain a pressing global issue. This study had three objectives: (1) to describe the proportions of patients affected by in-hospital AEs; (2) to explore the types and consequences of observed AEs; and (3) to estimate the preventability of in-hospital AEs. METHODS: We applied a scoping review method and concluded a comprehensive literature search in PubMed and CINAHL in May 2017 and in February 2018. Our target was retrospective medical record review studies applying the Harvard method-or similar methods using screening criteria-conducted in acute care hospital settings on adult patients (>/=18 years). RESULTS: We included a total of 25 studies conducted in 27 countries across six continents. Overall, a median of 10% patients were affected by at least one AE (range: 2.9-21.9%), with a median of 7.3% (range: 0.6-30%) of AEs being fatal. Between 34.3 and 83% of AEs were considered preventable (median: 51.2%). The three most common types of AEs reported in the included studies were operative/surgical related, medication or drug/fluid related, and healthcare-associated infections. CONCLUSIONS: Evidence regarding the occurrence of AEs confirms earlier estimates that a tenth of inpatient stays include adverse events, half of which are preventable. However, the incidence of in-hospital AEs varied considerably across studies, indicating methodological and contextual variations regarding this type of retrospective chart review across health care systems. For the future, automated methods for identifying AE using electronic health records have the potential to overcome various methodological issues and biases related to retrospective medical record review studies and to provide accurate data on their occurrence.", Cross Infection/epidemiology; Data Accuracy; Data Collection; Electronic Health Records; Hospitalization/*statistics & numerical data; Hospitals/statistics & numerical data; Humans; Incidence; Medical Errors/prevention & control/*statistics & numerical data; Patient Safety/standards/statistics & numerical data; Retrospective Studies; *Adverse events; *Hospitals; *Medical error; *Patient safety; *Scoping review,-2,-2,
39,Khalifa,2018,Studies in health technology and informatics,"Health Analytics Types, Functions and Levels: A Review of Literature","Health analytics is a business-driven term that encompasses a wide spectrum of aspects and dimensions of business intelligence applications and big data analysis. Healthcare organizations recently are eager to know whether they are getting the full value from the massive amounts of data and information they already have, to achieve their strategic effectiveness goals and operational efficiency objectives. It is very crucial to learn more about the diverse functions, types and levels through which health analytics can support such tasks. A careful review of literature was conducted, and a qualitative analysis was used to classify health analytics. Five main types of analytics could be identified; these are descriptive, diagnostic, predictive, prescriptive and discovery analytics, each has its own distinct role in improving healthcare. In addition to the five types, health analytics could also be classified into three levels of performance and engagement, these are the operational, tactical and strategic health analytics."," Data Interpretation, Statistical; Delivery of Health Care/*statistics & numerical data; *Quality Assurance, Health Care; Big Data; Business Intelligence; Health Analytics; Hospitals",-2,-2,
40,Mahmut,2018,Studies in health technology and informatics,A Computer-Based Speech Sound Disorder Screening System Architecture,"This paper reviews several architectures of Computer-Based Speech Therapy (CBST) systems and solutions and describes an architecture for an Entropy-Based Sound Speech Disorder (SSD) Screening System aimed at by our research project. The proposed architecture and data flow scenario aim to provide a fully-automated Entropy-based SSD Screening System, to be connected with CBSTs and to be used as a research infrastructure for further refinement of the objectives of our research project."," *Diagnosis, Computer-Assisted; Humans; Language Development Disorders; Speech; Speech Disorders; Speech Sound Disorder/*diagnosis; *Speech Therapy; Computer-based Speech Therapy Tools; Gamification; Screening; Speech Processing; Speech Sound Disorder",-1,-2,
41,Przybyla,2018,Research synthesis methods,Prioritising references for systematic reviews with RobotAnalyst: A user study,"Screening references is a time-consuming step necessary for systematic reviews and guideline development. Previous studies have shown that human effort can be reduced by using machine learning software to prioritise large reference collections such that most of the relevant references are identified before screening is completed. We describe and evaluate RobotAnalyst, a Web-based software system that combines text-mining and machine learning algorithms for organising references by their content and actively prioritising them based on a relevancy classification model trained and updated throughout the process. We report an evaluation over 22 reference collections (most are related to public health topics) screened using RobotAnalyst with a total of 43 610 abstract-level decisions. The number of references that needed to be screened to identify 95% of the abstract-level inclusions for the evidence review was reduced on 19 of the 22 collections. Significant gains over random sampling were achieved for all reviews conducted with active prioritisation, as compared with only two of five when prioritisation was not used. RobotAnalyst's descriptive clustering and topic modelling functionalities were also evaluated by public health analysts. Descriptive clustering provided more coherent organisation than topic modelling, and the content of the clusters was apparent to the users across a varying number of clusters. This is the first large-scale study using technology-assisted screening to perform new reviews, and the positive results provide empirical evidence that RobotAnalyst can accelerate the identification of relevant studies. The results also highlight the issue of user complacency and the need for a stopping criterion to realise the work savings.", ,2,2,
42,O'Connor,2018,PloS one,The study design elements employed by researchers in preclinical animal experiments from two research domains and implications for automation of systematic reviews,"Systematic reviews are increasingly using data from preclinical animal experiments in evidence networks. Further, there are ever-increasing efforts to automate aspects of the systematic review process. When assessing systematic bias and unit-of-analysis errors in preclinical experiments, it is critical to understand the study design elements employed by investigators. Such information can also inform prioritization of automation efforts that allow the identification of the most common issues. The aim of this study was to identify the design elements used by investigators in preclinical research in order to inform unique aspects of assessment of bias and error in preclinical research. Using 100 preclinical experiments each related to brain trauma and toxicology, we assessed design elements described by the investigators. We evaluated Methods and Materials sections of reports for descriptions of the following design elements: 1) use of comparison group, 2) unit of allocation of the interventions to study units, 3) arrangement of factors, 4) method of factor allocation to study units, 5) concealment of the factors during allocation and outcome assessment, 6) independence of study units, and 7) nature of factors. Many investigators reported using design elements that suggested the potential for unit-of-analysis errors, i.e., descriptions of repeated measurements of the outcome (94/200) and descriptions of potential for pseudo-replication (99/200). Use of complex factor arrangements was common, with 112 experiments using some form of factorial design (complete, incomplete or split-plot-like). In the toxicology dataset, 20 of the 100 experiments appeared to use a split-plot-like design, although no investigators used this term. The common use of repeated measures and factorial designs means understanding bias and error in preclinical experimental design might require greater expertise than simple parallel designs. Similarly, use of complex factor arrangements creates novel challenges for accurate automation of data extraction and bias and error assessment in preclinical experiments.", ,1,1,
43,Ley,2018,PloS one,"Triclosan and triclocarban exposure, infectious disease symptoms and antibiotic prescription in infants-A community-based randomized intervention","BACKGROUND: Triclosan and triclocarban (TCs) are broad-spectrum antimicrobials that, until recently, were found in a wide variety of household and personal wash products. Popular with consumers, TCs have not been shown to protect against infectious diseases. OBJECTIVES: To determine whether use of TC-containing wash products reduces incidence of infection in children less than one year of age. METHODS: Starting in 2011, we nested a randomized intervention of wash products with and without TCs within a multiethnic birth cohort. Maternal reports of infectious disease symptoms and antibiotic use were collected weekly by automated survey; household visits occurred every four months. Antibiotic prescriptions were identified by medical chart review. Urinary triclosan levels were measured in a participant subset. Differences by intervention group in reported infectious disease (primary outcome) and antibiotic use (secondary outcome) were assessed using mixed effects logistic regression and Fisher's Exact tests, respectively. RESULTS: Infectious illness occurred in 6% of weeks, with upper respiratory illness the predominant syndrome. Among 60 (45%) TC-exposed and 73 (55%) non-TC-exposed babies, infectious disease reports did not differ in frequency between groups (likelihood ratio test: p = 0.88). Medical visits with antibiotic prescriptions were less common in the TC group than in the non-TC group (7.8% vs. 16.6%, respectively; p = 0.02). CONCLUSIONS: Although randomization to TC-containing wash products was not associated with decreased infectious disease reports by mothers, TCs were associated with decreased antibiotic prescriptions, suggesting a benefit against bacterial infection. The recent removal of TCs from consumer wash products makes further elucidation of benefits and risks impracticable.", ,-2,,
44,Lee,2018,BMC health services research,The impact of telehealth remote patient monitoring on glycemic control in type 2 diabetes: a systematic review and meta-analysis of systematic reviews of randomised controlled trials,"BACKGROUND: There is a growing body of evidence to support the use of telehealth in monitoring HbA1c levels in people living with type 2 diabetes. However, the overall magnitude of effect is yet unclear due to variable results reported in existing systematic reviews. The objective of this study is to conduct a systematic review and meta-analysis of systematic reviews of randomised controlled trials to create an evidence-base for the effectiveness of telehealth interventions on glycemic control in adults with type 2 diabetes. METHODS: Electronic databases including The Cochrane Library, MEDLINE, EMBASE, HMIC, and PsychINFO were searched to identify relevant systematic reviews published between 1990 and April 2016, supplemented by references search from the relevant reviews. Two independent reviewers selected and reviewed the eligible studies. Of the 3279 references retrieved, 4 systematic reviews reporting in total 29 unique studies relevant to our review were included. Both conventional pairwise meta-analyses and network meta-analyses were performed. RESULTS: Evidence from pooling four systematic reviews found that telehealth interventions produced a small but significant improvement in HbA1c levels compared with usual care (MD: -0.55, 95% CI: -0.73 to - 0.36). The greatest effect was seen in telephone-delivered interventions, followed by Internet blood glucose monitoring system interventions and lastly interventions involving automatic transmission of SMBG using a mobile phone or a telehealth unit. CONCLUSION: Current evidence suggests that telehealth is effective in controlling HbA1c levels in people living with type 2 diabetes. However there is need for better quality primary studies as well as systematic reviews of RCTs in order to confidently conclude on the impact of telehealth on glycemic control in type 2 diabetes."," Blood Glucose Self-Monitoring; Cell Phone; Diabetes Mellitus, Type 2/*metabolism/therapy; Evidence-Based Medicine; Glycated Hemoglobin A/*metabolism; Humans; Internet; Qualitative Research; Randomized Controlled Trials as Topic; Systematic Reviews as Topic; *Telemedicine; *Blood glucose; *Remote; *Telehealth; *Type 2 diabetes mellitus",-2,-2,
45,Huang,2018,PloS one,Efficacy and safety of low-molecular-weight heparin after knee arthroscopy: A meta-analysis,"BACKGROUND: Venous thromboembolism (VTE) is considered a potentially serious complication of knee arthroscopy and leads to conditions such as deep venous thrombosis (DVT) and pulmonary embolism (PE). Low-molecular-weight heparin (LMWH) is widely employed in knee arthroscopy to reduce perioperative thromboembolic complications. However, the efficacy and safety of LMWH in knee arthroscopy remains unclear. METHODS: Seven randomized controlled clinical trials on LMWH in knee arthroscopy were identified and included in this meta-analysis. The main outcomes of the effectiveness (prevention of DVT and PE) and complications (death, major bleeding, and minor bleeding) of LMWH in knee arthroscopic surgery were assessed using Review Manager 5.3 software. RESULTS: The meta-analysis indicated that LMWH prophylaxis comprised 79% of asymptomatic DVT. No association was found in symptomatic VTE (RR: 0.90; 95% confidence interval [CI]: 0.39-2.08; P = 0.80), symptomatic DVT (RR: 0.79; 95% CI: 0.28-2.23; P = 0.66), symptomatic PE (RR: 1.36; 95% CI: 0.37-4.97; P = 0.64) and major bleeding (RR: 0.70; 95% CI: 0.12-3.95; P = 0.68) risk during LMWH prophylaxis were identified. Death was not reported in these studies. Moreover, there was a lower incidence of minor bleeding (RR: 0.64; 95% CI: 0.49 to 0.83; P = 0.001) in the control group than in the LMWH group. CONCLUSION: Compared with the control group, the group treated with LMWH after knee arthroscopy was no association in reducing the symptomatic VTE rate, symptomatic DVT rate or symptomatic PE rate. The symptomatic VTE rate was 0.5% (11/2,166) in the LMWH group versus 0.6% (10/1,713) in the control group. Although the limitations of this meta-analysis cannot be ignored, the results of our study show that LMWH after knee arthroscopy is ineffective. We recommend that LMWH should not be routinely provided for knee arthroscopy. TRIAL REGISTRATION: ClinicalTrials.gov NCT03164746."," Arthroscopy/*adverse effects; Heparin, Low-Molecular-Weight/*adverse effects/*pharmacology; Humans; Knee/*surgery; *Safety; Venous Thromboembolism/etiology/prevention & control",-2,-2,
46,Turk,2018,PloS one,"Quality of reporting web-based and non-web-based survey studies: What authors, reviewers and consumers should consider","BACKGROUND: Several influential aspects of survey research have been under-investigated and there is a lack of guidance on reporting survey studies, especially web-based projects. In this review, we aim to investigate the reporting practices and quality of both web- and non-web-based survey studies to enhance the quality of reporting medical evidence that is derived from survey studies and to maximize the efficiency of its consumption. METHODS: Reporting practices and quality of 100 random web- and 100 random non-web-based articles published from 2004 to 2016 were assessed using the SUrvey Reporting GuidelinE (SURGE). The CHERRIES guideline was also used to assess the reporting quality of Web-based studies. RESULTS: Our results revealed a potential gap in the reporting of many necessary checklist items in both web-based and non-web-based survey studies including development, description and testing of the questionnaire, the advertisement and administration of the questionnaire, sample representativeness and response rates, incentives, informed consent, and methods of statistical analysis. CONCLUSION: Our findings confirm the presence of major discrepancies in reporting results of survey-based studies. This can be attributed to the lack of availability of updated universal checklists for quality of reporting standards. We have summarized our findings in a table that may serve as a roadmap for future guidelines and checklists, which will hopefully include all types and all aspects of survey research.", *Data Mining; *Evidence-Based Medicine; Humans; *Internet,-1,-2,
47,Jones,2018,PloS one,Epidemiology of nontuberculous mycobacterial infections in the U.S. Veterans Health Administration,"OBJECTIVE: We identified patients with non-tuberculous mycobacterial (NTM) disease in the US Veterans Health Administration (VHA), examined the distribution of diseases by NTM species, and explored the association between NTM disease and the frequency of clinic visits and mortality. METHODS: We combined mycobacterial isolate (from natural language processing) with ICD-9-CM diagnoses from VHA data between 2008 and 2012 and then applied modified ATS/IDSA guidelines for NTM diagnosis. We performed validation against a reference standard of chart review. Incidence rates were calculated. Two nested case-control studies (matched by age and location) were used to measure the association between NTM disease and each of 1) the frequency of outpatient clinic visits and 2) mortality, both adjusted by chronic obstructive pulmonary disease (COPD), other structural lung diseases, and immunomodulatory factors. RESULTS: NTM cases were identified with a sensitivity of 94%, a specificity of >99%. The incidence of NTM was 12.6/100k patient-years. COPD was present in 68% of pulmonary NTM. NTM incidence was highest in the southeastern US. Extra-pulmonary NTM rates increased during the study period. The incidence rate ratio of clinic visits in the first year after diagnosis was 1.3 [95%CI 1.34-1.35]. NTM patients had a hazard ratio of mortality of 1.4 [95%CI 1.1-1.9] in the 6 months after NTM identification compared to controls and 1.99 [95%CI 1.8-2.3] thereafter. CONCLUSIONS: In VHA, pulmonary NTM disease is commonly associated with COPD, with the highest rates in the southeastern US. After adjustment, NTM patients had more clinic visits and greater mortality compared to matched patients."," Female; Humans; Male; Middle Aged; Mycobacterium Infections, Nontuberculous/*epidemiology/mortality; Patient Acceptance of Health Care/statistics & numerical data; United States/epidemiology; *United States Department of Veterans Affairs",-2,-2,
48,Kornfield,2018,Journal of medical Internet research,Detecting Recovery Problems Just in Time: Application of Automated Linguistic Analysis and Supervised Machine Learning to an Online Substance Abuse Forum,"BACKGROUND: Online discussion forums allow those in addiction recovery to seek help through text-based messages, including when facing triggers to drink or use drugs. Trained staff (or ""moderators"") may participate within these forums to offer guidance and support when participants are struggling but must expend considerable effort to continually review new content. Demands on moderators limit the scalability of evidence-based digital health interventions. OBJECTIVE: Automated identification of recovery problems could allow moderators to engage in more timely and efficient ways with participants who are struggling. This paper aimed to investigate whether computational linguistics and supervised machine learning can be applied to successfully flag, in real time, those discussion forum messages that moderators find most concerning. METHODS: Training data came from a trial of a mobile phone-based health intervention for individuals in recovery from alcohol use disorder, with human coders labeling discussion forum messages according to whether or not authors mentioned problems in their recovery process. Linguistic features of these messages were extracted via several computational techniques: (1) a Bag-of-Words approach, (2) the dictionary-based Linguistic Inquiry and Word Count program, and (3) a hybrid approach combining the most important features from both Bag-of-Words and Linguistic Inquiry and Word Count. These features were applied within binary classifiers leveraging several methods of supervised machine learning: support vector machines, decision trees, and boosted decision trees. Classifiers were evaluated in data from a later deployment of the recovery support intervention. RESULTS: To distinguish recovery problem disclosures, the Bag-of-Words approach relied on domain-specific language, including words explicitly linked to substance use and mental health (""drink,"" ""relapse,"" ""depression,"" and so on), whereas the Linguistic Inquiry and Word Count approach relied on language characteristics such as tone, affect, insight, and presence of quantifiers and time references, as well as pronouns. A boosted decision tree classifier, utilizing features from both Bag-of-Words and Linguistic Inquiry and Word Count performed best in identifying problems disclosed within the discussion forum, achieving 88% sensitivity and 82% specificity in a separate cohort of patients in recovery. CONCLUSIONS: Differences in language use can distinguish messages disclosing recovery problems from other message types. Incorporating machine learning models based on language use allows real-time flagging of concerning content such that trained staff may engage more efficiently and focus their attention on time-sensitive issues.", health communication; self-help groups; social support; substance-related disorders; supervised machine learning,1,-2,
49,da Costa,2018,Artificial intelligence in medicine,Internet of Health Things: Toward intelligent vital signs monitoring in hospital wards,"BACKGROUND: Large amounts of patient data are routinely manually collected in hospitals by using standalone medical devices, including vital signs. Such data is sometimes stored in spreadsheets, not forming part of patients' electronic health records, and is therefore difficult for caregivers to combine and analyze. One possible solution to overcome these limitations is the interconnection of medical devices via the Internet using a distributed platform, namely the Internet of Things. This approach allows data from different sources to be combined in order to better diagnose patient health status and identify possible anticipatory actions. METHODS: This work introduces the concept of the Internet of Health Things (IoHT), focusing on surveying the different approaches that could be applied to gather and combine data on vital signs in hospitals. Common heuristic approaches are considered, such as weighted early warning scoring systems, and the possibility of employing intelligent algorithms is analyzed. RESULTS: As a result, this article proposes possible directions for combining patient data in hospital wards to improve efficiency, allow the optimization of resources, and minimize patient health deterioration. CONCLUSION: It is concluded that a patient-centered approach is critical, and that the IoHT paradigm will continue to provide more optimal solutions for patient management in hospital wards.", Early Warning Score; Health records; Internet of Things; Machine learning; Vital signs; Wireless sensor networks,-2,-2,
50,Contreras,2018,Journal of medical Internet research,Artificial Intelligence for Diabetes Management and Decision Support: Literature Review,"BACKGROUND: Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis. OBJECTIVE: The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges. METHODS: A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review. RESULTS: We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results. CONCLUSIONS: We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients' quality of life.", artificial intelligence; blood glucose; diabetes management; machine learning; mobile computing,-1,-2,
51,Musy,2018,Journal of medical Internet research,Trigger Tool-Based Automated Adverse Event Detection in Electronic Health Records: Systematic Review,"BACKGROUND: Adverse events in health care entail substantial burdens to health care systems, institutions, and patients. Retrospective trigger tools are often manually applied to detect AEs, although automated approaches using electronic health records may offer real-time adverse event detection, allowing timely corrective interventions. OBJECTIVE: The aim of this systematic review was to describe current study methods and challenges regarding the use of automatic trigger tool-based adverse event detection methods in electronic health records. In addition, we aimed to appraise the applied studies' designs and to synthesize estimates of adverse event prevalence and diagnostic test accuracy of automatic detection methods using manual trigger tool as a reference standard. METHODS: PubMed, EMBASE, CINAHL, and the Cochrane Library were queried. We included observational studies, applying trigger tools in acute care settings, and excluded studies using nonhospital and outpatient settings. Eligible articles were divided into diagnostic test accuracy studies and prevalence studies. We derived the study prevalence and estimates for the positive predictive value. We assessed bias risks and applicability concerns using Quality Assessment tool for Diagnostic Accuracy Studies-2 (QUADAS-2) for diagnostic test accuracy studies and an in-house developed tool for prevalence studies. RESULTS: A total of 11 studies met all criteria: 2 concerned diagnostic test accuracy and 9 prevalence. We judged several studies to be at high bias risks for their automated detection method, definition of outcomes, and type of statistical analyses. Across all the 11 studies, adverse event prevalence ranged from 0% to 17.9%, with a median of 0.8%. The positive predictive value of all triggers to detect adverse events ranged from 0% to 100% across studies, with a median of 40%. Some triggers had wide ranging positive predictive value values: (1) in 6 studies, hypoglycemia had a positive predictive value ranging from 15.8% to 60%; (2) in 5 studies, naloxone had a positive predictive value ranging from 20% to 91%; (3) in 4 studies, flumazenil had a positive predictive value ranging from 38.9% to 83.3%; and (4) in 4 studies, protamine had a positive predictive value ranging from 0% to 60%. We were unable to determine the adverse event prevalence, positive predictive value, preventability, and severity in 40.4%, 10.5%, 71.1%, and 68.4% of the studies, respectively. These studies did not report the overall number of records analyzed, triggers, or adverse events; or the studies did not conduct the analysis. CONCLUSIONS: We observed broad interstudy variation in reported adverse event prevalence and positive predictive value. The lack of sufficiently described methods led to difficulties regarding interpretation. To improve quality, we see the need for a set of recommendations to endorse optimal use of research designs and adequate reporting of future adverse event detection studies."," electronic health records; patient harm; patient safety; review, systematic",-2,-1,
52,Faust,2018,BMC bioinformatics,Visualizing histopathologic deep learning classification and anomaly detection using nonlinear feature space dimensionality reduction,"BACKGROUND: There is growing interest in utilizing artificial intelligence, and particularly deep learning, for computer vision in histopathology. While accumulating studies highlight expert-level performance of convolutional neural networks (CNNs) on focused classification tasks, most studies rely on probability distribution scores with empirically defined cutoff values based on post-hoc analysis. More generalizable tools that allow humans to visualize histology-based deep learning inferences and decision making are scarce. RESULTS: Here, we leverage t-distributed Stochastic Neighbor Embedding (t-SNE) to reduce dimensionality and depict how CNNs organize histomorphologic information. Unique to our workflow, we develop a quantitative and transparent approach to visualizing classification decisions prior to softmax compression. By discretizing the relationships between classes on the t-SNE plot, we show we can super-impose randomly sampled regions of test images and use their distribution to render statistically-driven classifications. Therefore, in addition to providing intuitive outputs for human review, this visual approach can carry out automated and objective multi-class classifications similar to more traditional and less-transparent categorical probability distribution scores. Importantly, this novel classification approach is driven by a priori statistically defined cutoffs. It therefore serves as a generalizable classification and anomaly detection tool less reliant on post-hoc tuning. CONCLUSION: Routine incorporation of this convenient approach for quantitative visualization and error reduction in histopathology aims to accelerate early adoption of CNNs into generalized real-world applications where unanticipated and previously untrained classes are often encountered.", *Artificial intelligence; *Cancer; *Convolutional neural networks; *Deep learning; *Diagnostics; *Digital pathology; *Glioblastoma; *Machine learning; *Neuropathology; *t-SNE,-2,-2,
53,Kumar,2018,PloS one,Automated and real-time segmentation of suspicious breast masses using convolutional neural network,"In this work, a computer-aided tool for detection was developed to segment breast masses from clinical ultrasound (US) scans. The underlying Multi U-net algorithm is based on convolutional neural networks. Under the Mayo Clinic Institutional Review Board protocol, a prospective study of the automatic segmentation of suspicious breast masses was performed. The cohort consisted of 258 female patients who were clinically identified with suspicious breast masses and underwent clinical US scan and breast biopsy. The computer-aided detection tool effectively segmented the breast masses, achieving a mean Dice coefficient of 0.82, a true positive fraction (TPF) of 0.84, and a false positive fraction (FPF) of 0.01. By avoiding positioning of an initial seed, the algorithm is able to segment images in real time (13-55 ms per image), and can have potential clinical applications. The algorithm is at par with a conventional seeded algorithm, which had a mean Dice coefficient of 0.84 and performs significantly better (P< 0.0001) than the original U-net algorithm."," Adult; Aged; Aged, 80 and over; Algorithms; Breast/*diagnostic imaging; Breast Neoplasms/*diagnostic imaging; Carcinoma, Ductal, Breast/diagnostic imaging; Carcinoma, Intraductal, Noninfiltrating/diagnostic imaging; Carcinoma, Lobular/diagnostic imaging; Female; Humans; Image Processing, Computer-Assisted/*methods; Mammography/methods; Middle Aged; *Neural Networks (Computer); *Pattern Recognition, Automated; Prospective Studies; Ultrasonography, Mammary/methods; Young Adult",-2,-2,
54,Shen,2018,Journal of medical Internet research,Effectiveness of Internet-Based Interventions on Glycemic Control in Patients With Type 2 Diabetes: Meta-Analysis of Randomized Controlled Trials,"BACKGROUND: The popularity of internet as an area of research has grown manifold over the years. Given its rapid development and increasing coverage worldwide, internet-based interventions seem to offer a promising option to ameliorate huge burdens brought by type 2 diabetes mellitus. However, studies conducted by different researchers have provided contradictory results on the effect of internet-based interventions in glycemic control. OBJECTIVE: This meta-analysis aims to summarize currently available evidence and evaluate the overall impact of internet-based interventions on glycemic management of type 2 diabetic patients. METHODS: A systematic literature search was performed in PubMed, ScienceDirect, and Web of Science. Randomized controlled trials that used glycosylated hemoglobin values as the outcome measure of glycemic control were considered. Risk of bias and publication bias were evaluated. RESULTS: Of the 492 studies, 35 were included in meta-analysis, and results indicated that the weighted mean difference (WMD) between usual care and internet-based interventions at endpoint was -0.426% (95% CI -0.540 to -0.312; P<.001). Subgroup analyses revealed that intervention duration </=3 months yielded optimal performance (WMD -0.51%; 95% CI -0.71 to -0.31; P<.001). Combined mobile and website interventions were substantially superior to solely Web-based and mobile-based interventions in glycemic control (combined WMD -0.77%, 95% CI -1.07 to -0.47; P<.001; Web only: WMD -0.48%; 95% CI -0.71 to -0.24, P<.001; mobile only WMD -0.31%, 95% CI -0.49 to -0.14; P<.001). Furthermore, the effect of interventions with automated feedbacks was similar to those with manual feedbacks, and studies with internet-based educational contents were more effective in glycemic control. The assessment revealed a low risk of bias. CONCLUSIONS: In conclusion, utilization of internet-based intervention is beneficial for patients with type 2 diabetes mellitus, and taking full advantage of this type of intervention may substantially reduce the incidence of complications and improve quality of life. TRIAL REGISTRATION: International Prospective Register of Systematic Reviews (PROSPERO): CRD42017058032; https://www.crd.york.ac.uk/PROSPERO/display_record.php?RecordID=58032 (Archived by WebCite at http://www.webcitation.org/6yY7eQNHr).", HbA1c; internet; meta-analysis; randomized controlled trial; type 2 diabetes mellitus,-2,-2,
55,Denecke,2018,Studies in health technology and informatics,Facilitating the Information Exchange Using a Modular Electronic Discharge Summary,"BACKGROUND: Discharge summaries are a standard communication tool delivering important clinical information from inpatient to ambulatory care. To ensure a high quality, correctness and completeness, the generation process is time consuming. It requires also contributions of multiple persons. This is problematic since the primary care provider needs the information from the discharge summary for continuing the intended treatment. To address this challenge, we developed a concept for exchanging a modular electronic discharge summary. METHODS: Through a literature review and interviews with multiple stakeholders, we analysed existing processes and derived requirements for an improved communication of the discharge summary. RESULTS: In this paper, we suggest a concept of a modular electronic discharge summary that is exchanged through the electronic patient dossier in CDA CH level 2 documents. Until 2020, all Swiss hospitals are obliged to connect to the electronic patient dossier. Our concept allows to access already completed modules of the discharge summary from the primary care side, before the entire report is entirely finalised. The data is automatically merged with the local patient record on the physician side and prepared for data integration into the practice information system. CONCLUSION: Our concept offers the opportunity not only to improve the information exchange between hospital and primary care, but it also provides a potential use case and demonstrates a benefit of the electronic patient dossier for primary care providers who are so far not obliged to connect to the patient dossier in Switzerland.", Communication; *Electronic Health Records; Humans; *Information Systems; *Patient Discharge; Switzerland; Discharge summary; Electronic health record; Electronic patient record; Information exchange; Medical documentation; Transition of care,-2,-1,
56,Tso,2018,PloS one,"The ""social brain"" is highly sensitive to the mere presence of social information: An automated meta-analysis and an independent study","How the human brain processes social information is an increasingly researched topic in psychology and neuroscience, advancing our understanding of basic human cognition and psychopathologies. Neuroimaging studies typically seek to isolate one specific aspect of social cognition when trying to map its neural substrates. It is unclear if brain activation elicited by different social cognitive processes and task instructions are also spontaneously elicited by general social information. In this study, we investigated whether these brain regions are evoked by the mere presence of social information using an automated meta-analysis and confirmatory data from an independent study of simple appraisal of social vs. non-social images. Results of 1,000 published fMRI studies containing the keyword of ""social"" were subject to an automated meta-analysis (http://neurosynth.org). To confirm that significant brain regions in the meta-analysis were driven by a social effect, these brain regions were used as regions of interest (ROIs) to extract and compare BOLD fMRI signals of social vs. non-social conditions in the independent study. The NeuroSynth results indicated that the dorsal and ventral medial prefrontal cortex, posterior cingulate cortex, bilateral amygdala, bilateral occipito-temporal junction, right fusiform gyrus, bilateral temporal pole, and right inferior frontal gyrus are commonly engaged in studies with a prominent social element. The social-non-social contrast in the independent study showed a strong resemblance to the NeuroSynth map. ROI analyses revealed that a social effect was credible in 9 out of the 11 NeuroSynth regions in the independent dataset. The findings support the conclusion that the ""social brain"" is highly sensitive to the mere presence of social information.", Adult; Brain/*physiology; Brain Mapping; Cognition/*physiology; Female; Humans; Interpersonal Relations; Magnetic Resonance Imaging; Male; Middle Aged; *Social Behavior; Social Change; Social Conditions; Theory of Mind/physiology; Young Adult,-2,1,
57,Tsafnat,2018,Systematic reviews,Automated screening of research studies for systematic reviews using study characteristics,"BACKGROUND: Screening candidate studies for inclusion in a systematic review is time-consuming when conducted manually. Automation tools could reduce the human effort devoted to screening. Existing methods use supervised machine learning which train classifiers to identify relevant words in the abstracts of candidate articles that have previously been labelled by a human reviewer for inclusion or exclusion. Such classifiers typically reduce the number of abstracts requiring manual screening by about 50%. METHODS: We extracted four key characteristics of observational studies (population, exposure, confounders and outcomes) from the text of titles and abstracts for all articles retrieved using search strategies from systematic reviews. Our screening method excluded studies if they did not meet a predefined set of characteristics. The method was evaluated using three systematic reviews. Screening results were compared to the actual inclusion list of the reviews. RESULTS: The best screening threshold rule identified studies that mentioned both exposure (E) and outcome (O) in the study abstract. This screening rule excluded 93.7% of retrieved studies with a recall of 98%. CONCLUSIONS: Filtering studies for inclusion in a systematic review based on the detection of key study characteristics in abstracts significantly outperformed standard approaches to automated screening and appears worthy of further development and evaluation.", Automation of systematic reviews; Evidence screening; Study characterisation; Study selection,2,2,
58,Al-Saffar,2018,PloS one,Malay sentiment analysis based on combined classification approaches and Senti-lexicon algorithm,"Sentiment analysis techniques are increasingly exploited to categorize the opinion text to one or more predefined sentiment classes for the creation and automated maintenance of review-aggregation websites. In this paper, a Malay sentiment analysis classification model is proposed to improve classification performances based on the semantic orientation and machine learning approaches. First, a total of 2,478 Malay sentiment-lexicon phrases and words are assigned with a synonym and stored with the help of more than one Malay native speaker, and the polarity is manually allotted with a score. In addition, the supervised machine learning approaches and lexicon knowledge method are combined for Malay sentiment classification with evaluating thirteen features. Finally, three individual classifiers and a combined classifier are used to evaluate the classification accuracy. In experimental results, a wide-range of comparative experiments is conducted on a Malay Reviews Corpus (MRC), and it demonstrates that the feature extraction improves the performance of Malay sentiment analysis based on the combined classification. However, the results depend on three factors, the features, the number of features and the classification approach."," *Algorithms; Attitude/*ethnology; Attitude to Computers; *Emotions; Humans; Knowledge; *Language; *Machine Learning/classification; Malaysia; Models, Theoretical; *Semantics; Social Media; Supervised Machine Learning/classification; User-Computer Interface",-2,1,
59,Maranhao,2018,Studies in health technology and informatics,Challenges in Design and Creation of Genetic openEHR-Archetype,"Since the Human Genomic Project discovered the sequencing of human genome, the interest about genome content in clinical practice has increased. Genetic information has become a key point to understand diseases or improve treatments, for example, the nutrigenomic and nutrigenetics. However, the huge amount of data generated raises the need for Electronic Health Record (EHR) improvements as it becomes increasingly necessary that it includes more specific genetic information. Thus, we aim to propose standard genetic archetypes (in openEHR) and describe our main challenges in this context. We assessed 2 bibliographical databases (Pubmed and Web of science) to determine the main clinical statements needed to create the archetypes. The clinical statements were organized in archetype-concepts, and they were created in openEHR archetype editor. One archetype - genetic test results - was created from a set of genetic data and submitted to CKM repository for review. Based on the modeled archetypes, an openEHR template can be created from the proposed archetype, mainly in the nutrigenomic area, genetic labs and others related to genetic.", *Data Mining; *Electronic Health Records; *Genomics; Humans; Semantics; Archetype; Electronic Health Record; Genetics; Nutrition; openEHR,-2,-1,
60,Williams,2018,Studies in health technology and informatics,Process Mining in Primary Care: A Literature Review,"Process mining is the discipline of discovering processes from event logs, checking the conformance of real world events to idealized processes, and ultimately finding ways to improve those processes. It was originally applied to business processes and has recently been applied to healthcare. It can reveal insights into clinical care pathways and inform the redesign of healthcare services. We reviewed the literature on process mining, to investigate the extent to which process mining has been applied to primary care, and to identify specific challenges that may arise in this setting. We identified 143 relevant papers, of which only a small minority (n=7) focused on primary care settings. Reported challenges included data quality (consistency and completeness of routinely collected data); selection of appropriate algorithms and tools; presentation of results; and utilization of results in real-world applications.", Algorithms; *Critical Pathways; Data Accuracy; Data Mining; Humans; *Primary Health Care; Process mining; care pathways; primary care; workflow,-2,1,
61,Mehta,2018,International journal of medical informatics,Concurrence of big data analytics and healthcare: A systematic review,"BACKGROUND: The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care. PURPOSE: This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges. DATA SOURCES: A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered. STUDY SELECTION: Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected. DATA EXTRACTION: Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare. RESULTS: A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare. CONCLUSION: This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.", Analytics; Big data; Evidence-based medicine; Healthcare; Predictive analytics,-2,-1,
62,Araujo,2018,PloS one,Parametric model fitting-based approach for retinal blood vessel caliber estimation in eye fundus images,"BACKGROUND: Changes in the retinal vessel caliber are associated with a variety of major diseases, namely diabetes, hypertension and atherosclerosis. The clinical assessment of these changes in fundus images is tiresome and prone to errors and thus automatic methods are desirable for objective and precise caliber measurement. However, the variability of blood vessel appearance, image quality and resolution make the development of these tools a non-trivial task. METHOLODOGY: A method for the estimation of vessel caliber in eye fundus images via vessel cross-sectional intensity profile model fitting is herein proposed. First, the vessel centerlines are determined and individual segments are extracted and smoothed by spline approximation. Then, the corresponding cross-sectional intensity profiles are determined, post-processed and ultimately fitted by newly proposed parametric models. These models are based on Difference-of-Gaussians (DoG) curves modified through a multiplying line with varying inclination. With this, the proposed models can describe profile asymmetry, allowing a good adjustment to the most difficult profiles, namely those showing central light reflex. Finally, the parameters of the best-fit model are used to determine the vessel width using ensembles of bagged regression trees with random feature selection. RESULTS AND CONCLUSIONS: The performance of our approach is evaluated on the REVIEW public dataset by comparing the vessel cross-sectional profile fitting of the proposed modified DoG models with 7 and 8 parameters against a Hermite model with 6 parameters. Results on different goodness of fitness metrics indicate that our models are constantly better at fitting the vessel profiles. Furthermore, our width measurement algorithm achieves a precision close to the observers, outperforming state-of-the art methods, and retrieving the highest precision when evaluated using cross-validation. This high performance supports the robustness of the algorithm and validates its use in retinal vessel width measurement and possible integration in a system for retinal vasculature assessment."," Algorithms; Databases, Factual; *Fundus Oculi; Humans; *Image Processing, Computer-Assisted/methods; *Models, Theoretical; Reproducibility of Results; Retinal Vessels/*diagnostic imaging",-2,-2,
63,Yang,2018,PloS one,A Kriging based spatiotemporal approach for traffic volume data imputation,"Along with the rapid development of Intelligent Transportation Systems, traffic data collection technologies have progressed fast. The emergence of innovative data collection technologies such as remote traffic microwave sensor, Bluetooth sensor, GPS-based floating car method, and automated license plate recognition, has significantly increased the variety and volume of traffic data. Despite the development of these technologies, the missing data issue is still a problem that poses great challenge for data based applications such as traffic forecasting, real-time incident detection, dynamic route guidance, and massive evacuation optimization. A thorough literature review suggests most current imputation models either focus on the temporal nature of the traffic data and fail to consider the spatial information of neighboring locations or assume the data follow a certain distribution. These two issues reduce the imputation accuracy and limit the use of the corresponding imputation methods respectively. As a result, this paper presents a Kriging based data imputation approach that is able to fully utilize the spatiotemporal correlation in the traffic data and that does not assume the data follow any distribution. A set of scenarios with different missing rates are used to evaluate the performance of the proposed method. The performance of the proposed method was compared with that of two other widely used methods, historical average and K-nearest neighborhood. Comparison results indicate that the proposed method has the highest imputation accuracy and is more flexible compared to other methods."," Algorithms; Humans; *Models, Theoretical; *Spatio-Temporal Analysis; *Transportation",-2,-2,
64,Rat,2018,Journal of medical Internet research,Use of Smartphones for Early Detection of Melanoma: Systematic Review,"BACKGROUND: The early diagnosis of melanoma is associated with decreased mortality. The smartphone, with its apps and the possibility of sending photographs to a dermatologist, could improve the early diagnosis of melanoma. OBJECTIVE: The aim of our review was to report the evidence on (1) the diagnostic performance of automated smartphone apps and store-and-forward teledermatology via a smartphone in the early detection of melanoma, (2) the impact on the patient's medical-care course, and (3) the feasibility criteria (focusing on the modalities of picture taking, transfer of data, and time to get a reply). METHODS: We conducted a systematic search of PubMed for the period from January 1, 2007 (launch of the first smartphone) to November 1, 2017. RESULTS: The results of the 25 studies included 13 concentrated on store-and-forward teledermatology, and 12 analyzed automated smartphone apps. Store-and-forward teledermatology opens several new perspectives, such as it accelerates the care course (less than 10 days vs 80 days), and the related procedures were assessed in primary care populations. However, the concordance between the conclusion of a teledermatologist and the conclusion of a dermatologist who conducts a face-to-face examination depended on the study (the kappa coefficient range was .20 to .84, median kappa=.60). The use of a dermoscope may improve the concordance (the kappa coefficient range was .29 to .87, median kappa=.74). Regarding automated smartphone apps, the major concerns are the lack of assessment in clinical practice conditions, the lack of assessment in primary care populations, and their low sensitivity, ranging from 7% to 87% (median 69%). In this literature review, up to 20% of the photographs transmitted were of insufficient quality. The modalities of picture taking and encryption of the data were only partially reported. CONCLUSIONS: The use of store-and-forward teledermatology could improve access to a dermatology consultation by optimizing the care course. Our review confirmed the absence of evidence of the safety and efficacy of automated smartphone medical apps. Further research is required to determine quality criteria, as there was major variability among the studies.", melanoma; mobile app; screening; smartphone; teledermatology; telemedicine,-2,-2,
65,Wang,2018,Journal of the American Medical Informatics Association : JAMIA,Toward a normalized clinical drug knowledge base in China-applying the RxNorm model to Chinese clinical drugs,"Objective: In recent years, electronic health record systems have been widely implemented in China, making clinical data available electronically. However, little effort has been devoted to making drug information exchangeable among these systems. This study aimed to build a Normalized Chinese Clinical Drug (NCCD) knowledge base, by applying and extending the information model of RxNorm to Chinese clinical drugs. Methods: Chinese drugs were collected from 4 major resources-China Food and Drug Administration, China Health Insurance Systems, Hospital Pharmacy Systems, and China Pharmacopoeia-for integration and normalization in NCCD. Chemical drugs were normalized using the information model in RxNorm without much change. Chinese patent drugs (i.e., Chinese herbal extracts), however, were represented using an expanded RxNorm model to incorporate the unique characteristics of these drugs. A hybrid approach combining automated natural language processing technologies and manual review by domain experts was then applied to drug attribute extraction, normalization, and further generation of drug names at different specification levels. Lastly, we reported the statistics of NCCD, as well as the evaluation results using several sets of randomly selected Chinese drugs. Results: The current version of NCCD contains 16 976 chemical drugs and 2663 Chinese patent medicines, resulting in 19 639 clinical drugs, 250 267 unique concepts, and 2 602 760 relations. By manual review of 1700 chemical drugs and 250 Chinese patent drugs randomly selected from NCCD (about 10%), we showed that the hybrid approach could achieve an accuracy of 98.60% for drug name extraction and normalization. Using a collection of 500 chemical drugs and 500 Chinese patent drugs from other resources, we showed that NCCD achieved coverages of 97.0% and 90.0% for chemical drugs and Chinese patent drugs, respectively. Conclusion: Evaluation results demonstrated the potential to improve interoperability across various electronic drug systems in China.", ,-2,-2,
66,Abdellaoui,2018,Journal of medical Internet research,Detection of Cases of Noncompliance to Drug Treatment in Patient Forum Posts: Topic Model Approach,"BACKGROUND: Medication nonadherence is a major impediment to the management of many health conditions. A better understanding of the factors underlying noncompliance to treatment may help health professionals to address it. Patients use peer-to-peer virtual communities and social media to share their experiences regarding their treatments and diseases. Using topic models makes it possible to model themes present in a collection of posts, thus to identify cases of noncompliance. OBJECTIVE: The aim of this study was to detect messages describing patients' noncompliant behaviors associated with a drug of interest. Thus, the objective was the clustering of posts featuring a homogeneous vocabulary related to nonadherent attitudes. METHODS: We focused on escitalopram and aripiprazole used to treat depression and psychotic conditions, respectively. We implemented a probabilistic topic model to identify the topics that occurred in a corpus of messages mentioning these drugs, posted from 2004 to 2013 on three of the most popular French forums. Data were collected using a Web crawler designed by Kappa Sante as part of the Detec't project to analyze social media for drug safety. Several topics were related to noncompliance to treatment. RESULTS: Starting from a corpus of 3650 posts related to an antidepressant drug (escitalopram) and 2164 posts related to an antipsychotic drug (aripiprazole), the use of latent Dirichlet allocation allowed us to model several themes, including interruptions of treatment and changes in dosage. The topic model approach detected cases of noncompliance behaviors with a recall of 98.5% (272/276) and a precision of 32.6% (272/844). CONCLUSIONS: Topic models enabled us to explore patients' discussions on community websites and to identify posts related with noncompliant behaviors. After a manual review of the messages in the noncompliance topics, we found that noncompliance to treatment was present in 6.17% (276/4469) of the posts.", compliance; depression; infodemiology; medication adherence; peer-to-peer support; psychosis; social media; text mining; virtual community,-2,-2,
67,Gates,2018,Systematic reviews,Technology-assisted title and abstract screening for systematic reviews: a retrospective evaluation of the Abstrackr machine learning tool,"BACKGROUND: Machine learning tools can expedite systematic review (SR) processes by semi-automating citation screening. Abstrackr semi-automates citation screening by predicting relevant records. We evaluated its performance for four screening projects. METHODS: We used a convenience sample of screening projects completed at the Alberta Research Centre for Health Evidence, Edmonton, Canada: three SRs and one descriptive analysis for which we had used SR screening methods. The projects were heterogeneous with respect to search yield (median 9328; range 5243 to 47,385 records; interquartile range (IQR) 15,688 records), topic (Antipsychotics, Bronchiolitis, Diabetes, Child Health SRs), and screening complexity. We uploaded the records to Abstrackr and screened until it made predictions about the relevance of the remaining records. Across three trials for each project, we compared the predictions to human reviewer decisions and calculated the sensitivity, specificity, precision, false negative rate, proportion missed, and workload savings. RESULTS: Abstrackr's sensitivity was > 0.75 for all projects and the mean specificity ranged from 0.69 to 0.90 with the exception of Child Health SRs, for which it was 0.19. The precision (proportion of records correctly predicted as relevant) varied by screening task (median 26.6%; range 14.8 to 64.7%; IQR 29.7%). The median false negative rate (proportion of records incorrectly predicted as irrelevant) was 12.6% (range 3.5 to 21.2%; IQR 12.3%). The workload savings were often large (median 67.2%, range 9.5 to 88.4%; IQR 23.9%). The proportion missed (proportion of records predicted as irrelevant that were included in the final report, out of the total number predicted as irrelevant) was 0.1% for all SRs and 6.4% for the descriptive analysis. This equated to 4.2% (range 0 to 12.2%; IQR 7.8%) of the records in the final reports. CONCLUSIONS: Abstrackr's reliability and the workload savings varied by screening task. Workload savings came at the expense of potentially missing relevant records. How this might affect the results and conclusions of SRs needs to be evaluated. Studies evaluating Abstrackr as the second reviewer in a pair would be of interest to determine if concerns for reliability would diminish. Further evaluations of Abstrackr's performance and usability will inform its refinement and practical utility.", *Automation; *Machine learning; *Methodology; *Systematic review,2,2,
68,Ruano,2018,Systematic reviews,Evaluating characteristics of PROSPERO records as predictors of eventual publication of non-Cochrane systematic reviews: a meta-epidemiological study protocol,"BACKGROUND: Epidemiology and the reporting characteristics of systematic reviews (SRs) and meta-analyses (MAs) are well known. However, no study has analyzed the influence of protocol features on the probability that a study's results will be finally reported, thereby indirectly assessing the reporting bias of International Prospective Register of Systematic Reviews (PROSPERO) registration records. OBJECTIVE: The objective of this study is to explore which factors are associated with a higher probability that results derived from a non-Cochrane PROSPERO registration record for a systematic review will be finally reported as an original article in a scientific journal. METHODS/DESIGN: The PROSPERO repository will be web scraped to automatically and iteratively obtain all completed non-Cochrane registration records stored from February 2011 to December 2017. Downloaded records will be screened, and those with less than 90% fulfilled or are duplicated (i.e., those sharing titles and reviewers) will be excluded. Manual and human-supervised automatic methods will be used for data extraction, depending on the data source (fields of PROSPERO registration records, bibliometric databases, etc.). Records will be classified into published, discontinued, and abandoned review subgroups. All articles derived from published reviews will be obtained through multiple parallel searches using the full protocol ""title"" and/or ""list reviewers"" in MEDLINE/PubMed databases and Google Scholar. Reviewer, author, article, and journal metadata will be obtained using different sources. R and Python programming and analysis languages will be used to describe the datasets; perform text mining, machine learning, and deep learning analyses; and visualize the data. We will report the study according to the recommendations for meta-epidemiological studies adapted from the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement for SRs and MAs. DISCUSSION: This meta-epidemiological study will explore, for the first time, characteristics of PROSPERO records that may be associated with the publication of a completed systematic review. The evidence may help to improve review workflow performance in terms of research topic selection, decision-making regarding team selection, planning relationships with funding sources, implementing literature search strategies, and efficient data extraction and analysis. We expect to make our results, datasets, and R and Python code scripts publicly available during the third quarter of 2018.", *Deep learning; *Meta-epidemiology; *Prospero; *Predictive models; *Systematic review protocols; *Web scraping,1,-1,
69,Adroher,2018,PloS one,"All metrics are equal, but some metrics are more equal than others: A systematic search and review on the use of the term 'metric'","OBJECTIVE: To examine the use of the term 'metric' in health and social sciences' literature, focusing on the interval scale implication of the term in Modern Test Theory (MTT). MATERIALS AND METHODS: A systematic search and review on MTT studies including 'metric' or 'interval scale' was performed in the health and social sciences literature. The search was restricted to 2001-2005 and 2011-2015. A Text Mining algorithm was employed to operationalize the eligibility criteria and to explore the uses of 'metric'. The paradigm of each included article (Rasch Measurement Theory (RMT), Item Response Theory (IRT) or both), as well as its type (Theoretical, Methodological, Teaching, Application, Miscellaneous) were determined. An inductive thematic analysis on the first three types was performed. RESULTS: 70.6% of the 1337 included articles were allocated to RMT, and 68.4% were application papers. Among the number of uses of 'metric', it was predominantly a synonym of 'scale'; as adjective, it referred to measurement or quantification. Three incompatible themes 'only RMT/all MTT/no MTT models can provide interval measures' were identified, but 'interval scale' was considerably more mentioned in RMT than in IRT. CONCLUSION: 'Metric' is used in many different ways, and there is no consensus on which MTT metric has interval scale properties. Nevertheless, when using the term 'metric', the authors should specify the level of the metric being used (ordinal, ordered, interval, ratio), and justify why according to them the metric is at that level.", Data Mining; Humans; Research; *Statistics as Topic; *Terminology as Topic,-2,-1,
70,Piza,2018,International journal of medical informatics,Assessing team effectiveness and affective learning in a datathon,"BACKGROUND: Datathons are increasingly organized in the healthcare field. The goal is to assemble people with different backgrounds to work together as a team and engage in clinically relevant research or develop algorithms using health-related datasets. Criteria to assess the return of investment on such events have traditionally included publications produced, patents for prediction, classification, image recognition and other types of software, and start-up companies around the application of machine learning in healthcare. Previous studies have not evaluated whether a datathon can promote affective learning and effective teamwork. METHODS: Fifty participants of a health datathon event in Sao Paulo, Brazil at Hospital Israelita Albert Einstein (HIAE) were divided into 8 groups. A survey with 25 questions, using the Affective Learning Scale and Team-Review Questionnaire, was administered to assess team effectiveness and affective learning during the event. Multivariate regression models and Pearson's correlation tests were performed to evaluate the effect of affective learning on teamwork. RESULTS: Majority of the participants were male 76% (37/49); 32% (16/49) were physicians. The mean score for learning (scale from 1 to 10) was 8.38, while that for relevance of the perceived teamwork was 1.20 (scale from 1 to 5; ""1"" means most relevant). Pearson's correlation between the learning score and perception of teamwork showed moderate association (r=0.36, p=0.009). Five learning and 10 teamwork variables were on average positively graded in the event. The final regression model includes all learning and teamwork variables. Effective leadership was strongly correlated with affective learning (beta=-0.27, p<0.01, R(2)=75%). Effective leadership, team accomplishment, criticism, individual development and creativity were the variables significantly associated with higher levels of affective learning. CONCLUSION: It is feasible to enhance affective knowledge and the skill to work in a team during a datathon. We found that teamwork is associated with higher affective learning from participants' perspectives. Effective leadership is essential for teamwork and is a significant predictor of learning.", Affective learning; Datathon; Effective leadership; Hackathon; Team effectiveness,-2,-2,
71,Li,2018,PloS one,Comparison of NREM sleep and intravenous sedation through local information processing and whole brain network to explore the mechanism of general anesthesia,"BACKGROUND: The mechanism of general anesthesia (GA) has been explored for hundreds of years, but unclear. Previous studies indicated a possible correlation between NREM sleep and GA. The purpose of this study is to compare them by in vivo human brain function to probe the neuromechanism of consciousness, so as to find out a clue to GA mechanism. METHODS: 24 healthy participants were equally assigned to sleep or propofol sedation group by sleeping ability. EEG and Ramsay Sedation Scale were applied to determine sleep stage and sedation depth respectively. Resting-state functional magnetic resonance imaging (RS-fMRI) was acquired at each status. Regional homogeneity (ReHo) and seed-based whole brain functional connectivity maps (WB-FC maps) were compared. RESULTS: During sleep, ReHo primarily weakened on frontal lobe (especially preoptic area), but strengthened on brainstem. While during sedation, ReHo changed in various brain areas, including cingulate, precuneus, thalamus and cerebellum. Cingulate, fusiform and insula were concomitance of sleep and sedation. Comparing to sleep, FCs between the cortex and subcortical centers (centralized in cerebellum) were significantly attenuated under sedation. As sedation deepening, cerebellum-based FC maps were diminished, while thalamus- and brainstem-based FC maps were increased. CONCLUSION: There're huge distinctions in human brain function between sleep and GA. Sleep mainly rely on brainstem and frontal lobe function, while sedation is prone to affect widespread functional network. The most significant differences exist in the precuneus and cingulate, which may play important roles in mechanisms of inducing unconciousness by anesthetics. TRIAL REGISTRATION: Institutional Review Board (IRB) ChiCTR-IOC-15007454."," Adult; *Anesthesia, General; Electroencephalography; Female; Humans; Hypnotics and Sedatives/*administration & dosage; Male; Monitoring, Physiologic; *Sleep; Young Adult",-2,-2,
72,Sharifi,2018,PloS one,Integration of machine learning and meta-analysis identifies the transcriptomic bio-signature of mastitis disease in cattle,"Gram-negative bacteria such as Escherichia coli (E. coli) are assumed to be among the main agents that cause severe mastitis disease with clinical signs in dairy cattle. Rapid detection of this disease is so important in order to prevent transmission to other cows and helps to reduce inappropriate use of antibiotics. With the rapid progress in high-throughput technologies, and accumulation of various kinds of '-omics' data in public repositories, there is an opportunity to retrieve, integrate, and reanalyze these resources to improve the diagnosis and treatment of different diseases and to provide mechanistic insights into host resistance in an efficient way. Meta-analysis is a relatively inexpensive option with good potential to increase the statistical power and generalizability of single-study analysis. In the current meta-analysis research, six microarray-based studies that investigate the transcriptome profile of mammary gland tissue after induced mastitis by E. coli infection were used. This meta-analysis not only reinforced the findings in individual studies, but also several novel terms including responses to hypoxia, response to drug, anti-apoptosis and positive regulation of transcription from RNA polymerase II promoter enriched by up-regulated genes. Finally, in order to identify the small sets of genes that are sufficiently informative in E. coli mastitis, the differentially expressed gene introduced by meta-analysis were prioritized by using ten different attribute weighting algorithms. Twelve meta-genes were detected by the majority of attribute weighting algorithms (with weight above 0.7) as most informative genes including CXCL8 (IL8), NFKBIZ, HP, ZC3H12A, PDE4B, CASP4, CXCL2, CCL20, GRO1(CXCL1), CFB, S100A9, and S100A8. Interestingly, the results have been demonstrated that all of these genes are the key genes in the immune response, inflammation or mastitis. The Decision tree models efficiently discovered the best combination of the meta-genes as bio-signature and confirmed that some of the top-ranked genes -ZC3H12A, CXCL2, GRO, CFB- as biomarkers for E. coli mastitis (with the accuracy 83% in average). This research properly indicated that by combination of two novel data mining tools, meta-analysis and machine learning, increased power to detect most informative genes that can help to improve the diagnosis and treatment strategies for E. coli associated with mastitis in cattle."," Algorithms; Animals; Cattle; Data Mining/statistics & numerical data; Databases, Genetic; Decision Trees; Escherichia coli/genetics; Escherichia coli Infections/genetics/*veterinary; Female; Gene Expression Profiling/statistics & numerical data; Machine Learning; Mastitis, Bovine/*genetics/*microbiology; Meta-Analysis as Topic; Oligonucleotide Array Sequence Analysis/statistics & numerical data; Transcriptome",-1,-1,
73,Grimaldo,2018,PloS one,Fragments of peer review: A quantitative analysis of the literature (1969-2015),"This paper examines research on peer review between 1969 and 2015 by looking at records indexed from the Scopus database. Although it is often argued that peer review has been poorly investigated, we found that the number of publications in this field doubled from 2005. A half of this work was indexed as research articles, a third as editorial notes and literature reviews and the rest were book chapters or letters. We identified the most prolific and influential scholars, the most cited publications and the most important journals in the field. Co-authorship network analysis showed that research on peer review is fragmented, with the largest group of co-authors including only 2.1% of the whole community. Co-citation network analysis indicated a fragmented structure also in terms of knowledge. This shows that despite its central role in research, peer review has been examined only through small-scale research projects. Our findings would suggest that there is need to encourage collaboration and knowledge sharing across different research communities."," Abstracting and Indexing as Topic/history/*methods; Animals; Data Mining/history/*methods; *Databases, Bibliographic; History, 20th Century; History, 21st Century; Humans; Peer Review/*methods",-2,-1,
74,Gehrmann,2018,PloS one,Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives,"In secondary analysis of electronic health records, a crucial task consists in correctly identifying the patient cohort under investigation. In many cases, the most valuable and relevant information for an accurate classification of medical conditions exist only in clinical narratives. Therefore, it is necessary to use natural language processing (NLP) techniques to extract and evaluate these narratives. The most commonly used approach to this problem relies on extracting a number of clinician-defined medical concepts from text and using machine learning techniques to identify whether a particular patient has a certain condition. However, recent advances in deep learning and NLP enable models to learn a rich representation of (medical) language. Convolutional neural networks (CNN) for text classification can augment the existing techniques by leveraging the representation of language to learn which phrases in a text are relevant for a given medical condition. In this work, we compare concept extraction based methods with CNNs and other commonly used models in NLP in ten phenotyping tasks using 1,610 discharge summaries from the MIMIC-III database. We show that CNNs outperform concept extraction based methods in almost all of the tasks, with an improvement in F1-score of up to 26 and up to 7 percentage points in area under the ROC curve (AUC). We additionally assess the interpretability of both approaches by presenting and evaluating methods that calculate and extract the most salient phrases for a prediction. The results indicate that CNNs are a valid alternative to existing approaches in patient phenotyping and cohort identification, and should be further investigated. Moreover, the deep learning approach presented in this paper can be used to assist clinicians during chart review or support the extraction of billing codes from text by identifying and highlighting relevant phrases for various medical conditions.", Humans; *Language; *Learning; *Phenotype,2,-1,
75,Fernandez-Luque,2018,International journal of medical informatics,Humanitarian health computing using artificial intelligence and social media: A narrative literature review,"INTRODUCTION: According to the World Health Organization (WHO), over 130 million people are in constant need of humanitarian assistance due to natural disasters, disease outbreaks, and conflicts, among other factors. These health crises can compromise the resilience of healthcare systems, which are essential for achieving the health objectives of the sustainable development goals (SDGs) of the United Nations (UN). During a humanitarian health crisis, rapid and informed decision making is required. This is often challenging due to information scarcity, limited resources, and strict time constraints. Moreover, the traditional approach to digital health development, which involves a substantial requirement analysis, a feasibility study, and deployment of technology, is ill-suited for many crisis contexts. The emergence of Web 2.0 technologies and social media platforms in the past decade, such as Twitter, has created a new paradigm of massive information and misinformation, in which new technologies need to be developed to aid rapid decision making during humanitarian health crises. OBJECTIVE: Humanitarian health crises increasingly require the analysis of massive amounts of information produced by different sources, such as social media content, and, hence, they are a prime case for the use of artificial intelligence (AI) techniques to help identify relevant information and make it actionable. To identify challenges and opportunities for using AI in humanitarian health crises, we reviewed the literature on the use of AI techniques to process social media. METHODOLOGY: We performed a narrative literature review aimed at identifying examples of the use of AI in humanitarian health crises. Our search strategy was designed to get a broad overview of the different applications of AI in a humanitarian health crisis and their challenges. A total of 1459 articles were screened, and 24 articles were included in the final analysis. RESULTS: Successful case studies of AI applications in a humanitarian health crisis have been reported, such as for outbreak detection. A commonly shared concern in the reviewed literature is the technical challenge of analyzing large amounts of data in real time. Data interoperability, which is essential to data sharing, is also a barrier with regard to the integration of online and traditional data sources. Human and organizational aspects that might be key factors for the adoption of AI and social media remain understudied. There is also a publication bias toward high-income countries, as we identified few examples in low-income countries. Further, we did not identify any examples of certain types of major crisis, such armed conflicts, in which misinformation might be more common. CONCLUSIONS: The feasibility of using AI to extract valuable information during a humanitarian health crisis is proven in many cases. There is a lack of research on how to integrate the use of AI into the work-flow and large-scale deployments of humanitarian aid during a health crisis.", Artificial intelligence; Epidemiology; Global health; Health emergency; Internet; Machine learning; Social media,2,-2,
76,Alagoz,2018,BMC health services research,The use of external change agents to promote quality improvement and organizational change in healthcare organizations: a systematic review,"BACKGROUND: External change agents can play an essential role in healthcare organizational change efforts. This systematic review examines the role that external change agents have played within the context of multifaceted interventions designed to promote organizational change in healthcare-specifically, in primary care settings. METHODS: We searched PubMed, CINAHL, Cochrane, Web of Science, and Academic Search Premier Databases in July 2016 for randomized trials published (in English) between January 1, 2005 and June 30, 2016 in which external agents were part of multifaceted organizational change strategies. The review was conducted according to PRISMA guidelines. A total of 477 abstracts were identified and screened by 2 authors. Full text articles of 113 studies were reviewed. Twenty-one of these studies were selected for inclusion. RESULTS: Academic detailing (AD) is the most prevalently used organizational change strategy employed as part of multi-component implementation strategies. Out of 21 studies, nearly all studies integrate some form of audit and feedback into their interventions. Eleven studies that included practice facilitation into their intervention reported significant effects in one or more primary outcomes. CONCLUSIONS: Our results demonstrate that practice facilitation with regular, tailored follow up is a powerful component of a successful organizational change strategy. Academic detailing alone or combined with audit and feedback alone is ineffective without intensive follow up. Provision of educational materials and use of audit and feedback are often integral components of multifaceted implementation strategies. However, we didn't find examples where those relatively limited strategies were effective as standalone interventions. System-level support through technology (such as automated reminders or alerts) is potentially helpful, but must be carefully tailored to clinic needs.", Delivery of Health Care/*organization & administration; Health Services/*standards; Humans; Organizational Innovation; Quality Improvement/*organization & administration; *Academic detailing; *External change agents; *Organizational change; *Practice facilitation; *Quality improvement,-2,-2,
77,Martinez-Garcia,2018,PloS one,A systematic approach to analyze the social determinants of cardiovascular disease,"Cardiovascular diseases are the leading cause of human mortality worldwide. Among the many factors associated with the etiology, incidence, and evolution of such diseases; social and environmental issues constitute an important and often overlooked component. Understanding to a greater extent the scope to which such social determinants of cardiovascular diseases (SDCVD) occur as well as the connections among them would be useful for public health policy making. Here, we will explore the historical trends and associations among the main SDCVD in the published literature. Our aim will be finding meaningful relations among those that will help us to have an integrated view on this complex phenomenon by providing historical context and a relational framework. To uncover such relations, we used a data mining approach to the current literature, followed by network analysis of the interrelationships discovered. To this end, we systematically mined the PubMed/MEDLINE database for references of published studies on the subject, as outlined by the World Health Organization's framework on social determinants of health. The analyzed structured corpus consisted in circa 1190 articles categorized by means of the Medical Subheadings (MeSH) content-descriptor. The use of data analytics techniques allowed us to find a number of non-trivial connections among SDCVDs. Such relations may be relevant to get a deeper understanding of the social and environmental issues associated with cardiovascular disease and are often overlooked by traditional literature survey approaches, such as systematic reviews and meta-analyses."," Cardiovascular Diseases/epidemiology/*etiology/mortality; Data Interpretation, Statistical; Data Mining/methods; Female; Global Health; Humans; Medline; Male; Risk Factors; Semantic Web; *Social Determinants of Health; World Health Organization",2,1,
78,Chen,2018,Journal of the American Medical Informatics Association : JAMIA,DataMed - an open source discovery index for finding biomedical datasets,"Objective: Finding relevant datasets is important for promoting data reuse in the biomedical domain, but it is challenging given the volume and complexity of biomedical data. Here we describe the development of an open source biomedical data discovery system called DataMed, with the goal of promoting the building of additional data indexes in the biomedical domain. Materials and Methods: DataMed, which can efficiently index and search diverse types of biomedical datasets across repositories, is developed through the National Institutes of Health-funded biomedical and healthCAre Data Discovery Index Ecosystem (bioCADDIE) consortium. It consists of 2 main components: (1) a data ingestion pipeline that collects and transforms original metadata information to a unified metadata model, called DatA Tag Suite (DATS), and (2) a search engine that finds relevant datasets based on user-entered queries. In addition to describing its architecture and techniques, we evaluated individual components within DataMed, including the accuracy of the ingestion pipeline, the prevalence of the DATS model across repositories, and the overall performance of the dataset retrieval engine. Results and Conclusion: Our manual review shows that the ingestion pipeline could achieve an accuracy of 90% and core elements of DATS had varied frequency across repositories. On a manually curated benchmark dataset, the DataMed search engine achieved an inferred average precision of 0.2033 and a precision at 10 (P@10, the number of relevant results in the top 10 search results) of 0.6022, by implementing advanced natural language processing and terminology services. Currently, we have made the DataMed system publically available as an open source package for the biomedical community."," data discovery index, metadata, dataset, information storage and retrieval,; information dissemination",-2,-1,
79,McDonald,2018,PloS one,"Gadolinium-enhanced cardiac MR exams of human subjects are associated with significant increases in the DNA repair marker 53BP1, but not the damage marker gammaH2AX","Magnetic resonance imaging is considered low risk, yet recent studies have raised a concern of potential damage to DNA in peripheral blood leukocytes. This prospective Institutional Review Board-approved study examined potential double-strand DNA damage by analyzing changes in the DNA damage and repair markers gammaH2AX and 53BP1 in patients who underwent a 1.5 T gadolinium-enhanced cardiac magnetic resonance (MR) exam. Sixty patients were enrolled (median age 55 years, 39 males). Patients with history of malignancy or who were receiving chemotherapy, radiation therapy, or steroids were excluded. MR sequence data were recorded and blood samples obtained immediately before and after MR exposure. An automated immunofluorescence assay quantified gammaH2AX or 53BP1 foci number in isolated peripheral blood mononuclear cells. Changes in foci number were analyzed using the Wilcoxon signed-rank test. Clinical and MR procedural characteristics were compared between patients who had a >10% increase in gammaH2AX or 53BP1 foci numbers and patients who did not. The number of gammaH2AX foci did not significantly change following cardiac MR (median foci per cell pre-MR = 0.11, post-MR = 0.11, p = .90), but the number of 53BP1 foci significantly increased following MR (median foci per cell pre-MR = 0.46, post-MR = 0.54, p = .0140). Clinical and MR characteristics did not differ significantly between patients who had at least a 10% increase in foci per cell and those who did not. We conclude that MR exposure leads to a small (median 25%) increase in 53BP1 foci, however the clinical relevance of this increase is unknown and may be attributable to normal variation instead of MR exposure.", Adult; Aged; Biomarkers/*metabolism; *DNA Damage; *DNA Repair; Female; Gadolinium/*administration & dosage; Heart/*diagnostic imaging; Histones/*metabolism; Humans; Magnetic Resonance Imaging/*methods; Male; Middle Aged; Prospective Studies; Retrospective Studies; Tumor Suppressor p53-Binding Protein 1/*metabolism,-2,-2,
80,Frost,2018,PloS one,Wheelchair tiedown and occupant restraint practices in paratransit vehicles,"The purpose of this study was to characterize wheelchair tiedown and occupant restraint system (WTORS) usage in paratransit vehicles based on observations of wheelchair and scooter (wheeled mobility devices, collectively, ""WhMD"") passenger trips. A retrospective review of on-board video monitoring recordings of WhMD trips was conducted. Four hundred seventy-five video recordings were collected for review and analysis. The use of all four tiedowns to secure the WhMD was observed more frequently for power WhMDs (82%) and manual WhMDs (80%) compared to scooters (39%), and this difference was significant (p< 0.01). Nonuse or misuse of the occupant restraint system occurred during 88% of WhMD trips, and was most frequently due to vehicle operator neglect in applying the shoulder belt. Despite the absence of incidents or injuries in this study, misuse and nonuse of WTORS potentially place WhMD seated passengers at higher risk of injury during transit. These findings support the need for improved vehicle operator training and passenger education on the proper use of WTORS and development of WTORS with improved usability and/or alternative technologies that can be automated or used independently."," Accidents, Traffic/prevention & control; Disabled Persons; Humans; Kentucky; *Motor Vehicles; Protective Devices/*statistics & numerical data; Retrospective Studies; Safety; *Seat Belts/statistics & numerical data; Self-Help Devices/statistics & numerical data; Video Recording; *Wheelchairs",-2,-2,
81,Giles,2017,BMC bioinformatics,ALE: automated label extraction from GEO metadata,"BACKGROUND: NCBI's Gene Expression Omnibus (GEO) is a rich community resource containing millions of gene expression experiments from human, mouse, rat, and other model organisms. However, information about each experiment (metadata) is in the format of an open-ended, non-standardized textual description provided by the depositor. Thus, classification of experiments for meta-analysis by factors such as gender, age of the sample donor, and tissue of origin is not feasible without assigning labels to the experiments. Automated approaches are preferable for this, primarily because of the size and volume of the data to be processed, but also because it ensures standardization and consistency. While some of these labels can be extracted directly from the textual metadata, many of the data available do not contain explicit text informing the researcher about the age and gender of the subjects with the study. To bridge this gap, machine-learning methods can be trained to use the gene expression patterns associated with the text-derived labels to refine label-prediction confidence. RESULTS: Our analysis shows only 26% of metadata text contains information about gender and 21% about age. In order to ameliorate the lack of available labels for these data sets, we first extract labels from the textual metadata for each GEO RNA dataset and evaluate the performance against a gold standard of manually curated labels. We then use machine-learning methods to predict labels, based upon gene expression of the samples and compare this to the text-based method. CONCLUSION: Here we present an automated method to extract labels for age, gender, and tissue from textual metadata and GEO data using both a heuristic approach as well as machine learning. We show the two methods together improve accuracy of label assignment to GEO samples."," Age Factors; *Algorithms; Animals; Automation; Databases, Genetic; Female; *Gene Expression; Gene Ontology; Humans; Machine Learning; Male; *Metadata; Middle Aged; Molecular Sequence Annotation; Rats; Reference Standards; *Gene expression omnibus; *Meta-analysis; *Text mining",-2,-1,
82,Liang,2017,Studies in health technology and informatics,Automated Classification of Multi-Labeled Patient Safety Reports: A Shift from Quantity to Quality Measure,"Over the past two decades, there have seen an ever-increasing amount of patient safety reports yet the capacity of extracting useful information from the reports remains limited. Classification of patient safety reports is the first step of performing a downstream analysis. In practice, the manual review processes for classification are labor-intense. Studies have shown that the reports are often mislabeled or unclassifiable based on the pre-defined categories, which presents a notable data quality problem. In this study, we investigated the multi-labeled nature of patient safety reports. We argue that understanding multi-labeled nature of reports is a key to disclose the complex relations between many components during the courses and development of medical errors. Accordingly, we developed automated multi-label text classifiers to process patient safety reports. The experiments demonstrated feasibility and efficiency of a combination of multi-label algorithms in the benchmark comparison. Grounded on our experiments and results, we provided suggestions on how to implement automated classification of patient safety reports in the clinical settings."," *Algorithms; Automation; Humans; *Patient Safety; Quality Assurance, Health Care; Machine Learning; Patient Safety",-2,-1,
83,Skube,2017,Studies in health technology and informatics,Characterizing Surgical Site Infection Signals in Clinical Notes,"Surgical site infections (SSIs) are the most common and costly of hospital acquired infections. An important step in reducing SSIs is accurate SSI detection, which enables measurement quality improvement, but currently remains expensive through manual chart review. Building off of previous work for automated and semi-automated SSI detection using expert-derived ""strong features"" from clinical notes, we hypothesized that additional SSI phrases may be contained in clinical notes. We systematically characterized phrases and expressions associated with SSIs. While 83% of expert-derived original terms overlapped with new terms and modifiers, an additional 362 modifiers associated with both positive and negative SSI signals were identified and 62 new base observations and actions were identified. Clinical note queries with the most common base terms revealed another 49 modifiers. Clinical notes contain a wide variety of expressions describing infections occurring among surgical specialties which may provide value in improving the performance of SSI detection algorithms.", Algorithms; Electronic Health Records; Humans; *Quality Improvement; Risk Factors; *Surgical Wound Infection/diagnosis/therapy; Quality and Safety; Surgical Wound Infection; Text-mining,-2,-2,
84,Nelson,2017,Studies in health technology and informatics,Interoperability of Medication Classification Systems: Lessons Learned Mapping Established Pharmacologic Classes (EPCs) to SNOMED CT,"Interoperability among medication classification systems is known to be limited. We investigated the mapping of the Established Pharmacologic Classes (EPCs) to SNOMED CT. We compared lexical and instance-based methods to an expert-reviewed reference standard to evaluate contributions of these methods. Of the 543 EPCs, 284 had an equivalent SNOMED CT class, 205 were more specific, and 54 could not be mapped. Precision, recall, and F1 score were 0.416, 0.620, and 0.498 for lexical mapping and 0.616, 0.504, and 0.554 for instance-based mapping. Each automatic method has strengths, weaknesses, and unique contributions in mapping between medication classification systems. In our experience, it was beneficial to consider the mapping provided by both automated methods for identifying potential matches, gaps, inconsistencies, and opportunities for quality improvement between classifications. However, manual review by subject matter experts is still needed to select the most relevant mappings.", Humans; *Medication Systems; Quality Improvement; *Systematized Nomenclature of Medicine; Pharmaceutical Databases; Topical,-2,-1,
85,He,2017,Studies in health technology and informatics,Perceiving the Usefulness of the National Cancer Institute Metathesaurus for Enriching NCIt with Topological Patterns,"The National Cancer Institute Thesaurus (NCIt), developed and maintained by the National Cancer Institute, is an important reference terminology in the cancer domain. As a controlled terminology needs to continuously incorporate new concepts to enrich its conceptual content, automated and semi-automated methods for identifying potential new concepts are in high demand. We have previously developed a topological-pattern-based method for identifying new concepts in a controlled terminology to enrich another terminology, using the UMLS Metathesaurus. In this work, we utilize this method with the National Cancer Institute Metathesaurus to identify new concepts for NCIt. While previous work was only oriented towards identifying candidate import concepts for human review, we are now also adding an algorithmic method to evaluate candidate concepts and reject a well defined group of them."," Algorithms; Humans; *National Cancer Institute (U.S.); *Neoplasms; *Unified Medical Language System; United States; *Vocabulary, Controlled; Biomedical Ontologies; Quality Assurance; Vocabulary",-2,1,
86,Zhou,2017,Studies in health technology and informatics,Design a Learning-Oriented Fall Event Reporting System Based on Kirkpatrick Model,"Patient fall has been a severe problem in healthcare facilities around the world due to its prevalence and cost. Routine fall prevention training programs are not as effective as expected. Using event reporting systems is the trend for reducing patient safety events such as falls, although some limitations of the systems exist at current stage. We summarized these limitations through literature review, and developed an improved web-based fall event reporting system. The Kirkpatrick model, widely used in the business area for training program evaluation, has been integrated during the design of our system. Different from traditional event reporting systems that only collect and store the reports, our system automatically annotates and analyzes the reported events, and provides users with timely knowledge support specific to the reported event. The paper illustrates the design of our system and how its features are intended to reduce patient falls by learning from previous errors.", Accidental Falls/*prevention & control; Humans; *Internet; Program Evaluation; Accidental Falls; Incidental Findings; Patient Safety,-2,-2,
87,Boyd,2017,Studies in health technology and informatics,Developing Visual Thinking in the Electronic Health Record,"The purpose of this vision paper is to identify how data visualization could transform healthcare. Electronic Health Records (EHRs) are maturing with new technology and tools being applied. Researchers are reaping the benefits of data visualization to better access compilations of EHR data for enhanced clinical research. Data visualization, while still primarily the domain of clinical researchers, is beginning to show promise for other stakeholders. A non-exhaustive review of the literature indicates that respective to the growth and development of the EHR, the maturity of data visualization in healthcare is in its infancy. Visual analytics has been only cursorily applied to healthcare. A fundamental issue contributing to fragmentation and poor coordination of healthcare delivery is that each member of the healthcare team, including patients, has a different view. Summarizing all of this care comprehensively for any member of the healthcare team is a ""wickedly hard"" visual analytics and data visualization problem to solve.", *Electronic Health Records; Humans; *Patient Care Team; Data Display; Data Mining; Electronic Health Records,-2,1,
88,Mangesius,2017,Studies in health technology and informatics,Dynamic Creation of Patient Summaries: A CDA and IHE XDS Based Approach for Regional EHRs,"Cooperative healthcare is regarded as one of the major goals for providing adequate health related information to physicians. To achieve this goal, national authorities and large hospital organizations are introducing large scale, standard based transactional EHR systems. Those systems record and distribute a significant amount of medical related data. This raises the concern of information overload for the intended users. The objective is to elaborate on an architecture and consequently a workflow that allows the generation of an automatic patient summary in a standard based IHE XDS environment. A literature review evaluating the current state of research is conducted. Current eHealth projects, laws and technical background are analyzed. An architecture is suggested, prototyped and compared using SAAM (Software Architecture Analysis Method) against alternative approaches. A technical workflow built on IHE XDR and HL7 FHIR observations is suggested introducing two new services within an IHE XDS product for extracting observations from CDA documents and storing the data on domain level scope. The information is published as an OnDemand Document in the IHE XDS infrastructure.", Delivery of Health Care; *Electronic Health Records; Humans; *Software; Systems Integration; *Telemedicine; Electronic Health Record; Information Management,-2,-2,
89,Gates,2018,Journal of clinical epidemiology,Technology-assisted risk of bias assessment in systematic reviews: a prospective cross-sectional evaluation of the RobotReviewer machine learning tool,"OBJECTIVES: To evaluate the reliability of RobotReviewer's risk of bias judgments. STUDY DESIGN AND SETTING: In this prospective cross-sectional evaluation, we used RobotReviewer to assess risk of bias among 1,180 trials. We computed reliability with human reviewers using Cohen's kappa coefficient and calculated sensitivity and specificity. We investigated differences in reliability by risk of bias domain, topic, and outcome type using the chi-square test in meta-analysis. RESULTS: Reliability (95% CI) was moderate for random sequence generation (0.48 [0.43, 0.53]), allocation concealment (0.45 [0.40, 0.51]), and blinding of participants and personnel (0.42 [0.36, 0.47]); fair for overall risk of bias (0.34 [0.25, 0.44]); and slight for blinding of outcome assessors (0.10 [0.06, 0.14]), incomplete outcome data (0.14 [0.08, 0.19]), and selective reporting (0.02 [-0.02, 0.05]). Reliability for blinding of participants and personnel (P < 0.001), blinding of outcome assessors (P = 0.005), selective reporting (P < 0.001), and overall risk of bias (P < 0.001) differed by topic. Sensitivity and specificity (95% CI) ranged from 0.20 (0.18, 0.23) to 0.76 (0.72, 0.80) and from 0.61 (0.56, 0.65) to 0.95 (0.93, 0.96), respectively. CONCLUSION: Risk of bias appraisal is subjective. Compared with reliability between author groups, RobotReviewer's reliability with human reviewers was similar for most domains and better for allocation concealment, blinding of participants and personnel, and overall risk of bias.", Automation; Bias; Clinical trials; Evaluation; Evidence-based medicine; Machine learning,2,1,
90,Faggion,2017,BMC medical research methodology,A survey of prevalence of narrative and systematic reviews in five major medical journals,"BACKGROUND: Systematic reviews may provide less biased evidence than narrative reviews because they observe a strict methodology, similarly to primary studies. Hence, for clinical research questions, systematic reviews should be the study design of choice. It would be important to evaluate the prevalence and characteristics of narrative and systematic reviews published in prominent medical journals. Researchers and clinicians give great value to articles published in such scientific journals. This study sought to evaluate the prevalence and characteristics of narrative and systematic reviews in the five highest-ranked general medical journals and investigate the associations among type of review, number of citations, and impact factor (IF). METHODS: We surveyed the five highest-ranked medical journals (The New England Journal of Medicine, The Lancet, The Journal of the American Medical Association, The BMJ, and Annals of Internal Medicine) for narrative and systematic reviews published between June 2015 and June 2016. We independently selected and extracted the data from the reviews by strictly following the pre-determined eligibility criteria (Systematic and narrative reviews that focused on the management of diseases). We conducted regression analyses to investigate the associations among review type, number of citations, and IF. We also descriptively reported narrative reviews containing some methodology that might be reproducible. RESULTS: Two hundred seventy-five reviews were included: 75 (27%) systematic; 126 (46%) narrative with some methodology reported, and 74 (27%) narrative reviews. In comparison to systematic reviews, narrative reviews were more frequently published in journals with higher IF (risk ratio [RR] = 1.114 (95% CI 1.080 to 1.149). Systematic reviews received more citations than narrative reviews (group formed by narrative and narrative with some methodology reported (RR = 0.985 95% CI 0.978 to 0.991). CONCLUSIONS: Non-systematic evidence is the most prevalent type of evidence in reviews published in the five highest-ranked general medical journals. Narrative reviews were more frequently published in journals with higher IF. We recommend that journals limit their space for narrative information, and to address clinical research questions, these journals consider publishing systematic evidence exclusively.", Data Mining/*methods; Humans; Medical Writing/standards; *Medicine; Periodicals as Topic/standards/*statistics & numerical data; Publishing/standards/*statistics & numerical data; *Review Literature as Topic; *Bias; *Journal impact factor; *Methodological study; *Review; *Systematic review,-2,-1,
91,Wang,2017,Journal of medical Internet research,Understanding a Nonlinear Causal Relationship Between Rewards and Physicians' Contributions in Online Health Care Communities: Longitudinal Study,"BACKGROUND: The online health care community is not just a place for the public to share physician reviews or medical knowledge, but also a physician-patient communication platform. The medical resources of developing countries are relatively inadequate, and the online health care community is a potential solution to alleviate the phenomenon of long hospital queues and the lack of medical resources in rural areas. However, the success of the online health care community depends on online contributions by physicians. OBJECTIVE: The aim of this study is to examine the effect of incentive mechanisms on physician's online contribution behavior in the online health community. We addressed the following questions: (1) from which specialty area are physicians more likely to participate in online health care community activities, (2) what are the factors affecting physician online contributions, and (3) do incentive mechanisms, including psychological and material rewards, result in differences of physician online contributions? METHODS: We designed a longitudinal study involving a data sample in three waves. All data were collected from the Good Doctor website, which is the largest online health care community in China. We first used descriptive statistics to investigate the physician online contribution behavior in its entirety. Then multiple linear and quadratic regression models were applied to verify the causal relationship between rewards and physician online contribution. RESULTS: Our sample included 40,300 physicians from 3607 different hospitals, 10 different major specialty areas, and 31 different provinces or municipalities. Based on the multiple quadratic regression model, we found that the coefficients of the control variables, past physician online contributions, doctor review rating, clinic title, hospital level, and city level, were .415, .189, -.099, -.106, and -.143, respectively. For the psychological (or material) rewards, the standardized coefficient of the main effect was 0.261 (or 0.688) and the standardized coefficient of the quadratic effect was -0.015 (or -0.049). All estimates were statistically significant (P<.001). CONCLUSIONS: Physicians with more past physician online contribution, with higher review ratings, coming from lower level clinics, not coming from tertiary hospitals, and not coming from big cities were more willing to participate in online health care community activities. To promote physician online contribution, it is necessary to establish an appropriate incentive mechanism including psychological and material rewards. Finally, our findings suggest two guidelines for designing a useful incentive mechanism to facilitate physician online contribution. First, material reward is more useful than psychological reward. Second, as indicated by the concave-down-increasing causal relationship between rewards and physician online contribution, although an appropriate reward is effective in encouraging willingness on the part of physicians to contribute to the online health care community, the effect of additional rewards is limited.", *material reward; *online health care community; *physician online contribution; *psychological reward,-2,-2,
92,Viswanathan,2018,Journal of clinical epidemiology,Recommendations for assessing the risk of bias in systematic reviews of health-care interventions,"OBJECTIVES: Risk-of-bias assessment is a central component of systematic reviews, but little conclusive empirical evidence exists on the validity of such assessments. In the context of such uncertainty, we present pragmatic recommendations that promote transparency and reproducibility in processes, address methodological advances in the risk-of-bias assessment, and can be applied consistently across review topics. STUDY DESIGN AND SETTING: Epidemiological study design principles; available empirical evidence, risk-of-bias tools, and guidance; and workgroup consensus. RESULTS: We developed recommendations for assessing the risk of bias of studies of health-care interventions specific to framing the focus and scope of risk-of-bias assessment; selecting the risk-of-bias categories; choosing assessment instruments; and conducting, analyzing, and presenting results of risk-of-bias assessments. Key recommendations include transparency and reproducibility of judgments, separating risk of bias from other constructs such as applicability and precision, and evaluating the risk of bias per outcome. We recommend against certain past practices, such as focusing on reporting quality, relying solely on study design or numerical quality scores, and automatically downgrading for industry sponsorship. CONCLUSION: Risk-of-bias assessment remains a challenging but essential step in systematic reviews. We presented standards to promote transparency of judgments.", Critical appraisal; Evidence-based practice; Health-care interventions; Meta-analyses; Risk-of-bias guidance; Systematic reviews,-2,-2,
93,Magadzire,2017,BMC health services research,Analyzing implementation dynamics using theory-driven evaluation principles: lessons learnt from a South African centralized chronic dispensing model,"BACKGROUND: Centralized dispensing of essential medicines is one of South Africa's strategies to address the shortage of pharmacists, reduce patients' waiting times and reduce over-crowding at public sector healthcare facilities. This article reports findings of an evaluation of the Chronic Dispensing Unit (CDU) in one province. The objectives of this process evaluation were to: (1) compare what was planned versus the actual implementation and (2) establish the causal elements and contextual factors influencing implementation. METHODS: This qualitative study employed key informant interviews with the intervention's implementers (clinicians, managers and the service provider) [N = 40], and a review of policy and program documents. Data were thematically analyzed by identifying the main influences shaping the implementation process. Theory-driven evaluation principles were applied as a theoretical framework to explain implementation dynamics. RESULTS: The overall participants' response about the CDU was positive and the majority of informants concurred that the establishment of the CDU to dispense large volumes of medicines is a beneficial strategy to address healthcare barriers because mechanical functions are automated and distribution of medicines much quicker. However, implementation was influenced by the context and discrepancies between planned activities and actual implementation were noted. Procurement inefficiencies at central level caused medicine stock-outs and affected CDU activities. At the frontline, actors were aware of the CDU's implementation guidelines regarding patient selection, prescription validity and management of non-collected medicines but these were adapted to accommodate practical realities and to meet performance targets attached to the intervention. Implementation success was a result of a combination of 'hardware' (e.g. training, policies, implementation support and appropriate infrastructure) and 'software' (e.g. ownership, cooperation between healthcare practitioners and trust) factors. CONCLUSION: This study shows that health system interventions have unpredictable paths of implementation. Discrepancies between planned and actual implementation reinforce findings in existing literature suggesting that while tools and defined operating procedures are necessary for any intervention, their successful application depends crucially on the context and environment in which implementation occurs. We anticipate that this evaluation will stimulate wider thinking about the implementation of similar models in low- and middle-income countries."," Delivery of Health Care/standards; Drugs, Essential/*supply & distribution; Health Services Accessibility/standards; Humans; Pharmacies/*organization & administration; Pharmacists/statistics & numerical data; Prescription Drugs/supply & distribution; Qualitative Research; South Africa; Access to medicines; Centralized dispensing; Chronic Dispensing Unit; Medicines supply chain, theory-driven evaluation; Western Cape",-2,-2,
94,Buchanan,2017,PloS one,Considering the ethics of big data research: A case of Twitter and ISIS/ISIL,"This is a formal commentary, responding to Matthew Curran Benigni, Kenneth Joseph, and Kathleen Carley's contribution, ""Online extremism and the communities that sustain it: Detecting the ISIS supporting community on Twitter"". This brief review reflects on the ethics of big data research methodologies, and how novel methods complicate long-standing principles of research ethics. Specifically, the concept of the ""data subject"" as a corollary, or replacement, of ""human subject"" is considered."," *Ethics, Research; Humans; *Islam; *Social Media",-2,-2,
95,Gunhan,2018,Research synthesis methods,A design-by-treatment interaction model for network meta-analysis and meta-regression with integrated nested Laplace approximations,"Network meta-analysis (NMA) is gaining popularity for comparing multiple treatments in a single analysis. Generalized linear mixed models provide a unifying framework for NMA, allow us to analyze datasets with dichotomous, continuous or count endpoints, and take into account multiarm trials, potential heterogeneity between trials and network inconsistency. To perform inference within such NMA models, the use of Bayesian methods is often advocated. The standard inference tool is Markov chain Monte Carlo (MCMC), which is computationally expensive and requires convergence diagnostics. A deterministic approach to do fully Bayesian inference for latent Gaussian models can be achieved by integrated nested Laplace approximations (INLA), which is a fast and accurate alternative to MCMC. We show how NMA models fit in the class of latent Gaussian models and how NMA models are implemented using INLA and demonstrate that the estimates obtained by INLA are in close agreement with the ones obtained by MCMC. Specifically, we emphasize the design-by-treatment interaction model with random inconsistency parameters (also known as the Jackson model). Also, we have proposed a network meta-regression model, which is constructed by incorporating trial-level covariates to the Jackson model to explain possible sources of heterogeneity and/or inconsistency in the network. A publicly available R package, nmaINLA, is developed to automate the INLA implementation of NMA models, which are considered in this paper. Three applications illustrate the use of INLA for a NMA."," Algorithms; Bayes Theorem; Diabetes Mellitus/*drug therapy; Humans; Markov Chains; Models, Statistical; Monte Carlo Method; *Network Meta-Analysis; Normal Distribution; Randomized Controlled Trials as Topic; Reproducibility of Results; Research Design; Smoking Cessation/*methods; Stroke/prevention & control; Treatment Outcome; Bayesian inference; Inla; design-by-treatment interaction model; network meta-analysis; network meta-regression",-2,-1,
96,Rabinovich,2017,PLoS medicine,malERA: An updated research agenda for malaria elimination and eradication,"Achieving a malaria-free world presents exciting scientific challenges as well as overwhelming health, equity, and economic benefits. WHO and countries are setting ambitious goals for reducing the burden and eliminating malaria through the ""Global Technical Strategy"" and 21 countries are aiming to eliminate malaria by 2020. The commitment to achieve these targets should be celebrated. However, the need for innovation to achieve these goals, sustain elimination, and free the world of malaria is greater than ever. Over 180 experts across multiple disciplines are engaged in the Malaria Eradication Research Agenda (malERA) Refresh process to address problems that need to be solved. The result is a research and development agenda to accelerate malaria elimination and, in the longer term, transform the malaria community's ability to eradicate it globally.", Animals; Antimalarials/pharmacology/therapeutic use; Biomedical Research/*methods/trends; Disease Eradication/*methods; Global Health/trends; Humans; Malaria/*epidemiology/*prevention & control; Mosquito Control/trends; Plasmodium vivax/drug effects,-2,-2,
97,Sochat,2017,PloS one,Enhancing reproducibility in scientific computing: Metrics and registry for Singularity containers,"Here we present Singularity Hub, a framework to build and deploy Singularity containers for mobility of compute, and the singularity-python software with novel metrics for assessing reproducibility of such containers. Singularity containers make it possible for scientists and developers to package reproducible software, and Singularity Hub adds automation to this workflow by building, capturing metadata for, visualizing, and serving containers programmatically. Our novel metrics, based on custom filters of content hashes of container contents, allow for comparison of an entire container, including operating system, custom software, and metadata. First we will review Singularity Hub's primary use cases and how the infrastructure has been designed to support modern, common workflows. Next, we conduct three analyses to demonstrate build consistency, reproducibility metric and performance and interpretability, and potential for discovery. This is the first effort to demonstrate a rigorous assessment of measurable similarity between containers and operating systems. We provide these capabilities within Singularity Hub, as well as the source software singularity-python that provides the underlying functionality. Singularity Hub is available at https://singularity-hub.org, and we are excited to provide it as an openly available platform for building, and deploying scientific containers.", *Computers; Reproducibility of Results; *Software,-2,-1,
98,Rathbone,2017,Systematic reviews,Expediting citation screening using PICo-based title-only screening for identifying studies in scoping searches and rapid reviews,"BACKGROUND: Citation screening for scoping searches and rapid review is time-consuming and inefficient, often requiring days or sometimes months to complete. We examined the reliability of PICo-based title-only screening using keyword searches based on the PICo elements-Participants, Interventions, and Comparators, but not the Outcomes. METHODS: A convenience sample of 10 datasets, derived from the literature searches of completed systematic reviews, was used to test PICo-based title-only screening. Search terms for screening were generated from the inclusion criteria of each review, specifically the PICo elements-Participants, Interventions and Comparators. Synonyms for the PICo terms were sought, including alternatives for clinical conditions, trade names of generic drugs and abbreviations for clinical conditions, interventions and comparators. The MeSH database, Wikipedia, Google searches and online thesauri were used to assist generating terms. Title-only screening was performed by five reviewers independently in Endnote X7 reference management software using OR Boolean operator. Outcome measures were recall of included studies and the reduction in screening effort. Recall is the proportion of included studies retrieved using PICo title-only screening out of the total number of included studies in the original reviews. The percentage reduction in screening effort is the proportion of records not needing screening because the method eliminates them from the screen set. RESULTS: Across the 10 reviews, the reduction in screening effort ranged from 11 to 78% with a median reduction of 53%. In nine systematic reviews, the recall of included studies was 100%. In one review (oxygen therapy), four of five reviewers missed the same included study (median recall 67%). A post hoc analysis was performed on the dataset with the lowest reduction in screening effort (11%), and it was rescreened using only the intervention and comparator keywords and omitting keywords for participants. The reduction in screening effort increased to 57%, and the recall of included studies was maintained (100%). CONCLUSIONS: In this sample of datasets, PICo-based title-only screening was able to expedite citation screening for scoping searches and rapid reviews by reducing the number of citations needed to screen but requires a thorough workup of the potential synonyms and alternative terms. Further research which evaluates the feasibility of this technique with heterogeneous datasets in different fields would be useful to inform the generalisability of this technique."," *Databases, Bibliographic; Humans; Information Storage and Retrieval/*methods; *Review Literature as Topic; Expediting citation screening; PICo; Rapid review; Scoping search; Semi-automation; Systematic review; Title screening",2,1,
99,Skandarajah,2017,PloS one,Mobile microscopy as a screening tool for oral cancer in India: A pilot study,"Oral cancer is the most common type of cancer among men in India and other countries in South Asia. Late diagnosis contributes significantly to this mortality, highlighting the need for effective and specific point-of-care diagnostic tools. The same regions with high prevalence of oral cancer have seen extensive growth in mobile phone infrastructure, which enables widespread access to telemedicine services. In this work, we describe the evaluation of an automated tablet-based mobile microscope as an adjunct for telemedicine-based oral cancer screening in India. Brush biopsy, a minimally invasive sampling technique was combined with a simplified staining protocol and a tablet-based mobile microscope to facilitate local collection of digital images and remote evaluation of the images by clinicians. The tablet-based mobile microscope (CellScope device) combines an iPad Mini with collection optics, LED illumination and Bluetooth-controlled motors to scan a slide specimen and capture high-resolution images of stained brush biopsy samples. Researchers at the Mazumdar Shaw Medical Foundation (MSMF) in Bangalore, India used the instrument to collect and send randomly selected images of each slide for telepathology review. Evaluation of the concordance between gold standard histology, conventional microscopy cytology, and remote pathologist review of the images was performed as part of a pilot study of mobile microscopy as a screening tool for oral cancer. Results indicated that the instrument successfully collected images of sufficient quality to enable remote diagnoses that show concordance with existing techniques. Further studies will evaluate the effectiveness of oral cancer screening with mobile microscopy by minimally trained technicians in low-resource settings."," Adult; Aged; Automation; *Cell Phone; Demography; Early Detection of Cancer/*methods; Female; Humans; Image Processing, Computer-Assisted; India; Male; Microscopy/*methods; Middle Aged; Mouth Neoplasms/*diagnosis/pathology; Pilot Projects; Sensitivity and Specificity; User-Computer Interface; Young Adult",-2,-2,
100,Bouayad,2017,Journal of medical Internet research,Patient Health Record Systems Scope and Functionalities: Literature Review and Future Directions,"BACKGROUND: A new generation of user-centric information systems is emerging in health care as patient health record (PHR) systems. These systems create a platform supporting the new vision of health services that empowers patients and enables patient-provider communication, with the goal of improving health outcomes and reducing costs. This evolution has generated new sets of data and capabilities, providing opportunities and challenges at the user, system, and industry levels. OBJECTIVE: The objective of our study was to assess PHR data types and functionalities through a review of the literature to inform the health care informatics community, and to provide recommendations for PHR design, research, and practice. METHODS: We conducted a review of the literature to assess PHR data types and functionalities. We searched PubMed, Embase, and MEDLINE databases from 1966 to 2015 for studies of PHRs, resulting in 1822 articles, from which we selected a total of 106 articles for a detailed review of PHR data content. RESULTS: We present several key findings related to the scope and functionalities in PHR systems. We also present a functional taxonomy and chronological analysis of PHR data types and functionalities, to improve understanding and provide insights for future directions. Functional taxonomy analysis of the extracted data revealed the presence of new PHR data sources such as tracking devices and data types such as time-series data. Chronological data analysis showed an evolution of PHR system functionalities over time, from simple data access to data modification and, more recently, automated assessment, prediction, and recommendation. CONCLUSIONS: Efforts are needed to improve (1) PHR data quality through patient-centered user interface design and standardized patient-generated data guidelines, (2) data integrity through consolidation of various types and sources, (3) PHR functionality through application of new data analytics methods, and (4) metrics to evaluate clinical outcomes associated with automated PHR system use, and costs associated with PHR data storage and analytics."," Electronic Health Records/*statistics & numerical data; Health Records, Personal/*psychology; Humans; *data analytics; *electronic health records; *health platforms; *health records, personal; *medical informatics; *multiorganizational systems; *patient-centered care; *personal health record systems; *review; *ultralarge systems",-2,-2,
101,Gallego-Ortiz,2017,PloS one,Using quantitative features extracted from T2-weighted MRI to improve breast MRI computer-aided diagnosis (CAD),"Computer-aided diagnosis (CAD) has been proposed for breast MRI as a tool to standardize evaluation, to automate time-consuming analysis, and to aid the diagnostic decision process by radiologists. T2w MRI findings are diagnostically complementary to T1w DCE-MRI findings in the breast and prior research showed that measuring the T2w intensity of a lesion relative to a tissue of reference improves diagnostic accuracy. The diagnostic value of this information in CAD has not been yet quantified. This study proposes an automatic method of assessing relative T2w lesion intensity without the need to select a reference region. We also evaluate the effect of adding this feature to other T2w and T1w image features in the predictive performance of a breast lesion classifier for differential diagnosis of benign and malignant lesions. An automated feature of relative T2w lesion intensity was developed using a quantitative regression model. The diagnostic performance of the proposed feature in addition to T2w texture was compared to the performance of a conventional breast MRI CAD system based on T1w DCE-MRI features alone. The added contribution of T2w features to more conventional T1w-based features was investigated using classification rules extracted from the lesion classifier. After institutional review board approval that waived informed consent, we identified 627 breast lesions (245 malignant, 382 benign) diagnosed after undergoing breast MRI at our institution between 2007 and 2014. An increase in diagnostic performance in terms of area under the curve (AUC) from the receiver operating characteristic (ROC) analysis was observed with the additional T2w features and the proposed quantitative feature of relative T2w lesion intensity. AUC increased from 0.80 to 0.83 and this difference was statistically significant (adjusted p-value = 0.020)."," Adult; Aged; Breast/*diagnostic imaging; Diagnosis, Computer-Assisted/*methods; Female; Humans; Magnetic Resonance Imaging/*methods; Middle Aged",-2,-2,
102,Le Reste,2017,PloS one,"Multimorbid outpatients: A high frequency of FP appointments and/or family difficulties, should alert FPs to the possibility of death or acute hospitalization occurring within six months; A primary care feasibility study","BACKGROUND: The European General Practitioners Research Network (EGPRN) designed and validated a comprehensive definition of multimorbidity using a systematic literature review and qualitative research throughout Europe. This definition was tested as a model to assess death or acute hospitalization in multimorbid outpatients. OBJECTIVE: To assess which criteria in the EGPRN concept of multimorbidity could detect outpatients at risk of death or acute hospitalization in a primary care cohort at a 6-month follow-up and to assess whether a large scale cohort with FPs would be feasible. METHOD: Family Physicians included a random sample of multimorbid patients who attended appointments in their offices from July to December 2014. Inclusion criteria were those of the EGPRN definition of Multimorbidity. Exclusion criteria were patients under legal protection and those unable to complete the 2-year follow-up. Statistical analysis was undertaken with uni- and multivariate analysis at a 6-month follow-up using a combination of approaches including both automatic classification and expert decision making. A Multiple Correspondence Analysis (MCA) completed the process with a projection of illustrative variables. A logistic regression was finally performed in order to identify and quantify risk factors for decompensation. RESULTS: 19 FPs participated in the study. 96 patients were analyzed. 3 different clusters were identified. MCA showed the central function of psychosocial factors and peaceful versus conflictual relationships with relatives in all clusters. While taking into account the limit of a small cohort, age, frequency of family physician visits and extent of family difficulties were the factors which predicted death or acute hospitalization. CONCLUSION: A large scale cohort seems feasible in primary care. A sense of alarm should be triggered to prevent death or acute hospitalization in multimorbid older outpatients who have frequent family physician visits and who experience family difficulties.", Aged; *Appointments and Schedules; Europe; Feasibility Studies; Female; *General Practitioners; *Hospitalization; Humans; Male; *Multiple Chronic Conditions/mortality; Occupations; *Outpatients; Risk Factors,-2,-2,
103,Kim,2017,Journal of medical Internet research,Scaling Up Research on Drug Abuse and Addiction Through Social Media Big Data,"BACKGROUND: Substance use-related communication for drug use promotion and its prevention is widely prevalent on social media. Social media big data involve naturally occurring communication phenomena that are observable through social media platforms, which can be used in computational or scalable solutions to generate data-driven inferences. Despite the promising potential to utilize social media big data to monitor and treat substance use problems, the characteristics, mechanisms, and outcomes of substance use-related communications on social media are largely unknown. Understanding these aspects can help researchers effectively leverage social media big data and platforms for observation and health communication outreach for people with substance use problems. OBJECTIVE: The objective of this critical review was to determine how social media big data can be used to understand communication and behavioral patterns of problematic use of prescription drugs. We elaborate on theoretical applications, ethical challenges and methodological considerations when using social media big data for research on drug abuse and addiction. Based on a critical review process, we propose a typology with key initiatives to address the knowledge gap in the use of social media for research on prescription drug abuse and addiction. METHODS: First, we provided a narrative summary of the literature on drug use-related communication on social media. We also examined ethical considerations in the research processes of (1) social media big data mining, (2) subgroup or follow-up investigation, and (3) dissemination of social media data-driven findings. To develop a critical review-based typology, we searched the PubMed database and the entire e-collection theme of ""infodemiology and infoveillance"" in the Journal of Medical Internet Research / JMIR Publications. Studies that met our inclusion criteria (eg, use of social media data concerning non-medical use of prescription drugs, data informatics-driven findings) were reviewed for knowledge synthesis. User characteristics, communication characteristics, mechanisms and predictors of such communications, and the psychological and behavioral outcomes of social media use for problematic drug use-related communications are the dimensions of our typology. In addition to ethical practices and considerations, we also reviewed the methodological and computational approaches used in each study to develop our typology. RESULTS: We developed a typology to better understand non-medical, problematic use of prescription drugs through the lens of social media big data. Highly relevant studies that met our inclusion criteria were reviewed for knowledge synthesis. The characteristics of users who shared problematic substance use-related communications on social media were reported by general group terms, such as adolescents, Twitter users, and Instagram users. All reviewed studies examined the communication characteristics, such as linguistic properties, and social networks of problematic drug use-related communications on social media. The mechanisms and predictors of such social media communications were not directly examined or empirically identified in the reviewed studies. The psychological or behavioral consequence (eg, increased behavioral intention for mimicking risky health behaviors) of engaging with and being exposed to social media communications regarding problematic drug use was another area of research that has been understudied. CONCLUSIONS: We offer theoretical applications, ethical considerations, and empirical evidence within the scope of social media communication and prescription drug abuse and addiction. Our critical review suggests that social media big data can be a tremendous resource to understand, monitor and intervene on drug abuse and addiction problems."," Adolescent; Behavior, Addictive/*diagnosis; Humans; Internet/*statistics & numerical data; Research Design/*standards; Social Media/*statistics & numerical data; Substance-Related Disorders/*diagnosis; *Facebook; *Instagram; *Twitter; *addiction; *big data; *ethics; *opioid crisis; *opioid epidemic; *opioid-related disorders; *prescription drug misuse; *substance use; *substance-related disorders",-2,-2,
104,Bramer,2018,Research synthesis methods,Evaluation of a new method for librarian-mediated literature searches for systematic reviews,"To evaluate and validate the time of completion and results of a new method of searching for systematic reviews, the exhaustive search method (ESM), using a pragmatic comparison. METHODS: Single-line search strategies were prepared in a text document. Term completeness was ensured with a novel optimization technique. Macros in MS Word converted the syntaxes between databases and interfaces almost automatically. We compared search characteristics, such as number of search terms and databases, and outcomes, such as number of included and retrieved references and precision, from ESM searches and other Dutch academic hospitals identified by searching PubMed for systematic reviews published between 2014 and 2016. We compared time to perform the ESM with a secondary comparator of recorded search times from published literature and contact with authors to acquire unpublished data. RESULTS: We identified 73 published Erasmus MC systematic reviews and 258 published by other Dutch academic hospitals meeting our criteria. We pooled search time data from 204 other systematic reviews. The ESM searches differed by using 2 times more databases, retrieving 44% more references, including 20% more studies in the final systematic review, but the time needed for the search was 8% of that of the control group. Similarities between methods include precision and the number of search terms. CONCLUSIONS: The evaluated similarities and differences suggest that the ESM is a highly efficient way to locate more references meeting the specified selection criteria in systematic reviews than traditional search methods. Further prospective research is required.", bibliographic databases; information storage and retrieval; quality improvement; review literature as topic,-2,1,
105,Kuo,2017,Journal of the American Medical Informatics Association : JAMIA,Blockchain distributed ledger technologies for biomedical and health care applications,"Objectives: To introduce blockchain technologies, including their benefits, pitfalls, and the latest applications, to the biomedical and health care domains. Target Audience: Biomedical and health care informatics researchers who would like to learn about blockchain technologies and their applications in the biomedical/health care domains. Scope: The covered topics include: (1) introduction to the famous Bitcoin crypto-currency and the underlying blockchain technology; (2) features of blockchain; (3) review of alternative blockchain technologies; (4) emerging nonfinancial distributed ledger technologies and applications; (5) benefits of blockchain for biomedical/health care applications when compared to traditional distributed databases; (6) overview of the latest biomedical/health care applications of blockchain technologies; and (7) discussion of the potential challenges and proposed solutions of adopting blockchain technologies in biomedical/health care domains.", Algorithms; Commerce; *Computer Security; Confidentiality; *Data Mining/economics; *Medical Informatics; blockchain; distributed ledger technology; health information exchange; interoperability; security,-2,-2,
106,Canan,2017,Journal of the American Medical Informatics Association : JAMIA,Automatable algorithms to identify nonmedical opioid use using electronic data: a systematic review,"Objective: Improved methods to identify nonmedical opioid use can help direct health care resources to individuals who need them. Automated algorithms that use large databases of electronic health care claims or records for surveillance are a potential means to achieve this goal. In this systematic review, we reviewed the utility, attempts at validation, and application of such algorithms to detect nonmedical opioid use. Materials and Methods: We searched PubMed and Embase for articles describing automatable algorithms that used electronic health care claims or records to identify patients or prescribers with likely nonmedical opioid use. We assessed algorithm development, validation, and performance characteristics and the settings where they were applied. Study variability precluded a meta-analysis. Results: Of 15 included algorithms, 10 targeted patients, 2 targeted providers, 2 targeted both, and 1 identified medications with high abuse potential. Most patient-focused algorithms (67%) used prescription drug claims and/or medical claims, with diagnosis codes of substance abuse and/or dependence as the reference standard. Eleven algorithms were developed via regression modeling. Four used natural language processing, data mining, audit analysis, or factor analysis. Discussion: Automated algorithms can facilitate population-level surveillance. However, there is no true gold standard for determining nonmedical opioid use. Users must recognize the implications of identifying false positives and, conversely, false negatives. Few algorithms have been applied in real-world settings. Conclusion: Automated algorithms may facilitate identification of patients and/or providers most likely to need more intensive screening and/or intervention for nonmedical opioid use. Additional implementation research in real-world settings would clarify their utility."," *Algorithms; Data Mining/*methods; Databases, Pharmaceutical; *Electronic Health Records; Humans; *Insurance Claim Reporting; *Opioid-Related Disorders; *Prescription Drug Misuse; United States; automated algorithms; electronic claims data; electronic health record; nonmedical opioid use; screening",-2,-1,
107,Trivedi,2018,Journal of the American Medical Informatics Association : JAMIA,NLPReViz: an interactive tool for natural language processing on clinical text,"The gap between domain experts and natural language processing expertise is a barrier to extracting understanding from clinical text. We describe a prototype tool for interactive review and revision of natural language processing models of binary concepts extracted from clinical notes. We evaluated our prototype in a user study involving 9 physicians, who used our tool to build and revise models for 2 colonoscopy quality variables. We report changes in performance relative to the quantity of feedback. Using initial training sets as small as 10 documents, expert review led to final F1scores for the ""appendiceal-orifice"" variable between 0.78 and 0.91 (with improvements ranging from 13.26% to 29.90%). F1for ""biopsy"" ranged between 0.88 and 0.94 (-1.52% to 11.74% improvements). The average System Usability Scale score was 70.56. Subjective feedback also suggests possible design improvements.", electronic health records; machine learning; medical informatics; natural language processing (NLP); user-computer interface,-2,1,
108,DeMasi,2017,PloS one,Meaningless comparisons lead to false optimism in medical machine learning,"A new trend in medicine is the use of algorithms to analyze big datasets, e.g. using everything your phone measures about you for diagnostics or monitoring. However, these algorithms are commonly compared against weak baselines, which may contribute to excessive optimism. To assess how well an algorithm works, scientists typically ask how well its output correlates with medically assigned scores. Here we perform a meta-analysis to quantify how the literature evaluates their algorithms for monitoring mental wellbeing. We find that the bulk of the literature ( approximately 77%) uses meaningless comparisons that ignore patient baseline state. For example, having an algorithm that uses phone data to diagnose mood disorders would be useful. However, it is possible to explain over 80% of the variance of some mood measures in the population by simply guessing that each patient has their own average mood-the patient-specific baseline. Thus, an algorithm that just predicts that our mood is like it usually is can explain the majority of variance, but is, obviously, entirely useless. Comparing to the wrong (population) baseline has a massive effect on the perceived quality of algorithms and produces baseless optimism in the field. To solve this problem we propose ""user lift"" that reduces these systematic errors in the evaluation of personalized medical monitoring.", Algorithms; Humans; *Machine Learning; Mood Disorders/diagnosis,-2,-2,
109,Eveson,2017,Systematic reviews,Role of bispectral index monitoring and burst suppression in prognostication following out-of-hospital cardiac arrest: a systematic review protocol,"BACKGROUND: Out-of-hospital cardiac arrest (OHCA) is associated with significant mortality or may have a poor neurological outcome. Various community-training programmes have improved practices like bystander cardiopulmonary resuscitation (CPR) and early defibrillation using automated external defibrillator (AED). Post-resuscitation care has also changed significantly in the millennium. Interventions like targeted temperature management (TTM), avoidance of hyperoxia and emergency cardiac catheterisation have given patients a chance of a better neurological outcome. Despite these timely interventions, it is still very difficult to predict neurological outcome. The European Resuscitation Council and European Society of Intensive Care Medicine (ERC-ESICM) published guidance in 2015 with a strong recommendation to delay prognostication for at least 72 h and with an emphasis to adapt a multimodal approach, which includes neurological examination, biomarkers, electroencephalogram (EEG) and radiological tests. These interventions not only have cost attached to them, but the unpredictability has a significant emotional impact on family members. Bispectral index (BIS) monitoring device acts on the principle of EEG and converts the waveform into an absolute number and also measures the burst suppression. We hypothesize that patients who have a low BIS value and high burst suppression within 24 h of presentation will have a poor neurological outcome. The primary objective of this review is to look at BIS monitor as a tool, which could help bring forward the timing of prognostication. METHODS: Electronic databases will be systematically searched for randomised controlled trials and prospective or retrospective cohort studies with no language restrictions. The search will be supplemented with grey literature searches of thesis, dissertations and hand searching of relevant journals. Two independent reviewers will screen, select and perform analysis according to the Preferred Reporting Items for Systematic Review and Meta-analysis (PRISMA) method. The selected studies will be analysed using the Grades of Recommendation, Assessment, Development and Evaluation (GRADE) system. Meta-analysis will be performed if suitable. DISCUSSION: This review will synthesize the evidence on the use of BIS monitors within 24 h of achieving return of spontaneous circulation (ROSC) and may help in early prognostication. SYSTEMATIC REVIEW REGISTRATION: PROSPERO CRD 42016050224 ."," Cardiopulmonary Resuscitation/methods/*mortality; Consciousness Monitors/*statistics & numerical data; Electric Countershock; Electroencephalography; Humans; Hyperoxia/prevention & control; Hypothermia, Induced/statistics & numerical data; Out-of-Hospital Cardiac Arrest/mortality/*therapy; *Prognosis; Systematic Reviews as Topic; *Bis; *Bispectral index; *Burst suppression; *Ohca; *Out-of-hospital cardiac arrest; *Prognostication",-2,-2,
110,Mrowinski,2017,PloS one,Artificial intelligence in peer review: How can evolutionary computation support journal editors?,"With the volume of manuscripts submitted for publication growing every year, the deficiencies of peer review (e.g. long review times) are becoming more apparent. Editorial strategies, sets of guidelines designed to speed up the process and reduce editors' workloads, are treated as trade secrets by publishing houses and are not shared publicly. To improve the effectiveness of their strategies, editors in small publishing groups are faced with undertaking an iterative trial-and-error approach. We show that Cartesian Genetic Programming, a nature-inspired evolutionary algorithm, can dramatically improve editorial strategies. The artificially evolved strategy reduced the duration of the peer review process by 30%, without increasing the pool of reviewers (in comparison to a typical human-developed strategy). Evolutionary computation has typically been used in technological processes or biological ecosystems. Our results demonstrate that genetic programs can improve real-world social systems that are usually much harder to understand and control than physical systems."," Algorithms; *Artificial Intelligence; *Computer Simulation; *Editorial Policies; Evolution, Molecular; Humans; Models, Genetic; Peer Review, Research/*methods; Publishing/*standards; Quality Control",-2,-2,
111,Bennett,2017,BMC medical research methodology,"Systematic review of statistical approaches to quantify, or correct for, measurement error in a continuous exposure in nutritional epidemiology","BACKGROUND: Several statistical approaches have been proposed to assess and correct for exposure measurement error. We aimed to provide a critical overview of the most common approaches used in nutritional epidemiology. METHODS: MEDLINE, EMBASE, BIOSIS and CINAHL were searched for reports published in English up to May 2016 in order to ascertain studies that described methods aimed to quantify and/or correct for measurement error for a continuous exposure in nutritional epidemiology using a calibration study. RESULTS: We identified 126 studies, 43 of which described statistical methods and 83 that applied any of these methods to a real dataset. The statistical approaches in the eligible studies were grouped into: a) approaches to quantify the relationship between different dietary assessment instruments and ""true intake"", which were mostly based on correlation analysis and the method of triads; b) approaches to adjust point and interval estimates of diet-disease associations for measurement error, mostly based on regression calibration analysis and its extensions. Two approaches (multiple imputation and moment reconstruction) were identified that can deal with differential measurement error. CONCLUSIONS: For regression calibration, the most common approach to correct for measurement error used in nutritional epidemiology, it is crucial to ensure that its assumptions and requirements are fully met. Analyses that investigate the impact of departures from the classical measurement error model on regression calibration estimates can be helpful to researchers in interpreting their findings. With regard to the possible use of alternative methods when regression calibration is not appropriate, the choice of method should depend on the measurement error model assumed, the availability of suitable calibration study data and the potential for bias due to violation of the classical measurement error model assumptions. On the basis of this review, we provide some practical advice for the use of methods to assess and adjust for measurement error in nutritional epidemiology.", Bias; Calibration; Diet Surveys/*methods/*statistics & numerical data; Humans; *Nutrition Assessment; Nutritional Physiological Phenomena; Reproducibility of Results; Statistics as Topic/*methods; Continuous exposure; Measurement error; Nutrition; Statistical methods,-2,-2,
112,Thomas,2017,Journal of clinical epidemiology,Living systematic reviews: 2. Combining human and machine effort,"New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities (""crowds"") as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential-and limitations-of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine ""technologies"" are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing.", Data Mining/*methods; Evidence-Based Medicine; Humans; *Machine Learning; *Review Literature as Topic; Automation; Citizen science; Crowdsourcing; Machine learning; Systematic review; Text mining,2,2,
113,Barton,2017,Studies in health technology and informatics,A Review of Physical Activity Monitoring and Activity Trackers for Older Adults,"The objective assessment of physical activity levels through wearable inertial-based motion detectors for an automatic, continuous and long-term monitoring of people in free-living environments is a well-known research area in literature. However, their application to older adults can present particular constraints. This paper reviews the methods of measuring physical activity, adoption of wearable devices in older adults, describes and compares existing commercial products encompassing activity trackers tailored for older participants."," Aged; Exercise; *Fitness Trackers; Humans; *Monitoring, Ambulatory; Physical activity monitoring; activity monitoring and tracking; older adult(s)",-2,-2,
114,Zhang,2017,International journal of medical informatics,Advancing Alzheimer's research: A review of big data promises,"OBJECTIVE: To review the current state of science using big data to advance Alzheimer's disease (AD) research and practice. In particular, we analyzed the types of research foci addressed, corresponding methods employed and study findings reported using big data in AD. METHOD: Systematic review was conducted for articles published in PubMed from January 1, 2010 through December 31, 2015. Keywords with AD and big data analytics were used for literature retrieval. Articles were reviewed and included if they met the eligibility criteria. RESULTS: Thirty-eight articles were included in this review. They can be categorized into seven research foci: diagnosing AD or mild cognitive impairment (MCI) (n=10), predicting MCI to AD conversion (n=13), stratifying risks for AD (n=5), mining the literature for knowledge discovery (n=4), predicting AD progression (n=2), describing clinical care for persons with AD (n=3), and understanding the relationship between cognition and AD (n=3). The most commonly used datasets are AD Neuroimaging Initiative (ADNI) (n=16), electronic health records (EHR) (n=11), MEDLINE (n=3), and other research datasets (n=8). Logistic regression (n=9) and support vector machine (n=8) are the most used methods for data analysis. CONCLUSION: Big data are increasingly used to address AD-related research questions. While existing research datasets are frequently used, other datasets such as EHR data provide a unique, yet under-utilized opportunity for advancing AD research."," Alzheimer Disease/*diagnosis/diagnostic imaging/*prevention & control; *Biomedical Research; Data Mining; *Databases, Factual; Disease Progression; Humans; *Neuroimaging; *Alzheimer's disease; *Alzheimer's disease neuroimaging initiative; *Electronic health records; *Healthcare big data; *Healthcare data analytics",-2,-2,
115,Suurmond,2017,Research synthesis methods,"Introduction, comparison, and validation of Meta-Essentials: A free and simple tool for meta-analysis","We present a new tool for meta-analysis, Meta-Essentials, which is free of charge and easy to use. In this paper, we introduce the tool and compare its features to other tools for meta-analysis. We also provide detailed information on the validation of the tool. Although free of charge and simple, Meta-Essentials automatically calculates effect sizes from a wide range of statistics and can be used for a wide range of meta-analysis applications, including subgroup analysis, moderator analysis, and publication bias analyses. The confidence interval of the overall effect is automatically based on the Knapp-Hartung adjustment of the DerSimonian-Laird estimator. However, more advanced meta-analysis methods such as meta-analytical structural equation modelling and meta-regression with multiple covariates are not available. In summary, Meta-Essentials may prove a valuable resource for meta-analysts, including researchers, teachers, and students."," Algorithms; Computers; Humans; *Meta-Analysis as Topic; Models, Statistical; Programming Languages; Publication Bias; Regression Analysis; Software; Validation Studies as Topic; Microsoft Excel; freeware; meta-analysis; research synthesis; tool",-2,-1,
116,Ling,2017,Journal of medical Internet research,An Internet-Based Method for Extracting Nursing Home Resident Sedative Medication Data From Pharmacy Packing Systems: Descriptive Evaluation,"BACKGROUND: Inappropriate use of sedating medication has been reported in nursing homes for several decades. The Reducing Use of Sedatives (RedUSe) project was designed to address this issue through a combination of audit, feedback, staff education, and medication review. The project significantly reduced sedative use in a controlled trial of 25 Tasmanian nursing homes. To expand the project to 150 nursing homes across Australia, an improved and scalable method of data collection was required. This paper describes and evaluates a method for remotely extracting, transforming, and validating electronic resident and medication data from community pharmacies supplying medications to nursing homes. OBJECTIVE: The aim of this study was to develop and evaluate an electronic method for extracting and enriching data on psychotropic medication use in nursing homes, on a national scale. METHODS: An application uploaded resident details and medication data from computerized medication packing systems in the pharmacies supplying participating nursing homes. The server converted medication codes used by the packing systems to Australian Medicines Terminology coding and subsequently to Anatomical Therapeutic Chemical (ATC) codes for grouping. Medications of interest, in this case antipsychotics and benzodiazepines, were automatically identified and quantified during the upload. This data was then validated on the Web by project staff and a ""champion nurse"" at the participating home. RESULTS: Of participating nursing homes, 94.6% (142/150) had resident and medication records uploaded. Facilitating an upload for one pharmacy took an average of 15 min. A total of 17,722 resident profiles were extracted, representing 95.6% (17,722/18,537) of the homes' residents. For these, 546,535 medication records were extracted, of which, 28,053 were identified as antipsychotics or benzodiazepines. Of these, 8.17% (2291/28,053) were modified during validation and verification stages, and 4.75% (1398/29,451) were added. The champion nurse required a mean of 33 min website interaction to verify data, compared with 60 min for manual data entry. CONCLUSIONS: The results show that the electronic data collection process is accurate: 95.25% (28,053/29,451) of sedative medications being taken by residents were identified and, of those, 91.83% (25,762/28,053) were correct without any manual intervention. The process worked effectively for nearly all homes. Although the pharmacy packing systems contain some invalid patient records, and data is sometimes incorrectly recorded, validation steps can overcome these problems and provide sufficiently accurate data for the purposes of reporting medication use in individual nursing homes.", Aged; Antipsychotic Agents/*therapeutic use; Humans; Hypnotics and Sedatives/*therapeutic use; Internet/*statistics & numerical data; Medical Records/*standards; Nursing Homes/*standards; *antipsychotic agents; *benzodiazepines; *electronic health records; *health information systems; *inappropriate prescribing; *information storage and retrieval; *nursing homes; *systematized nomenclature of medicine,-2,-2,
117,Lentferink,2017,Journal of medical Internet research,Key Components in eHealth Interventions Combining Self-Tracking and Persuasive eCoaching to Promote a Healthier Lifestyle: A Scoping Review,"BACKGROUND: The combination of self-tracking and persuasive eCoaching in automated interventions is a new and promising approach for healthy lifestyle management. OBJECTIVE: The aim of this study was to identify key components of self-tracking and persuasive eCoaching in automated healthy lifestyle interventions that contribute to their effectiveness on health outcomes, usability, and adherence. A secondary aim was to identify the way in which these key components should be designed to contribute to improved health outcomes, usability, and adherence. METHODS: The scoping review methodology proposed by Arskey and O'Malley was applied. Scopus, EMBASE, PsycINFO, and PubMed were searched for publications dated from January 1, 2013 to January 31, 2016 that included (1) self-tracking, (2) persuasive eCoaching, and (3) healthy lifestyle intervention. RESULTS: The search resulted in 32 publications, 17 of which provided results regarding the effect on health outcomes, 27 of which provided results regarding usability, and 13 of which provided results regarding adherence. Among the 32 publications, 27 described an intervention. The most commonly applied persuasive eCoaching components in the described interventions were personalization (n=24), suggestion (n=19), goal-setting (n=17), simulation (n=17), and reminders (n=15). As for self-tracking components, most interventions utilized an accelerometer to measure steps (n=11). Furthermore, the medium through which the user could access the intervention was usually a mobile phone (n=10). The following key components and their specific design seem to influence both health outcomes and usability in a positive way: reduction by setting short-term goals to eventually reach long-term goals, personalization of goals, praise messages, reminders to input self-tracking data into the technology, use of validity-tested devices, integration of self-tracking and persuasive eCoaching, and provision of face-to-face instructions during implementation. In addition, health outcomes or usability were not negatively affected when more effort was requested from participants to input data into the technology. The data extracted from the included publications provided limited ability to identify key components for adherence. However, one key component was identified for both usability and adherence, namely the provision of personalized content. CONCLUSIONS: This scoping review provides a first overview of the key components in automated healthy lifestyle interventions combining self-tracking and persuasive eCoaching that can be utilized during the development of such interventions. Future studies should focus on the identification of key components for effects on adherence, as adherence is a prerequisite for an intervention to be effective.", Health Promotion/*methods; Humans; Life Style; Persuasive Communication; *Telemedicine; *health promotion; *remote sensing technology; *review,-2,-2,
118,Page,2017,International journal of medical informatics,A systematic review of the effectiveness of interruptive medication prescribing alerts in hospital CPOE systems to change prescriber behavior and improve patient safety,"OBJECTIVES: To assess the evidence of the effectiveness of different categories of interruptive medication prescribing alerts to change prescriber behavior and/or improve patient outcomes in hospital computerized provider order entry (CPOE) systems. METHODS: PubMed, Embase, CINAHL and the Cochrane Library were searched for relevant articles published between January 2000 and February 2016. Studies were included if they compared the outcomes of automatic, interruptive medication prescribing alert/s to a control/comparison group to determine alert effectiveness. RESULTS: Twenty-three studies describing 32 alerts classified into 11 alert categories were identified. The most common alert categories studied were drug-condition interaction (n=6), drug-drug interaction alerts (n=6) and corollary order alerts (n=6). All 23 papers investigated the effect of the intervention alert on at least one outcome measure of prescriber behavior. Just over half of the studies (53%, n=17) reported a statistically significant beneficial effect from the intervention alert; 34% (n=11) reported no statistically significant effect, and 6% (n=2) reported a significant detrimental effect. Two studies also evaluated the effect of alerts on patient outcome measures; neither finding that patient outcomes significantly improved following alert implementation (6%, n=2). The greatest volume of evidence relates to three alert categories: drug-condition, drug-drug and corollary order alerts. Of these, drug-condition alerts had the greatest number of studies reporting positive effects (five out of six studies). Only two of six studies of drug-drug interaction and one of six of corollary alerts reported positive benefits. DISCUSSION AND CONCLUSION: The current evidence-base does not show a clear indication that particular categories of alerts are more effective than others. While the majority of alert categories were shown to improve outcomes in some studies, there were also many cases where outcomes did not improve. This lack of evidence hinders decisions about the amount and type of decision support that should be integrated into CPOE systems to increase safety while reducing the risk of alert fatigue. Virtually no studies have sought to investigate the impact on changes to prescriber behavior and outcomes overall when alerts from multiple categories are incorporated within the same system."," *Clinical Alarms; Decision Support Systems, Clinical/*standards; Drug Interactions; Humans; Medical Order Entry Systems/*standards; Medication Errors/*prevention & control; *Patient Safety; Physicians/*psychology; Reminder Systems; *Computerized provider order entry systems; *Decision support systems, clinical; *Electronic prescribing; *Medical order entry systems; *Medication; *Reminder systems",-2,-2,
119,Micheel,2017,Journal of medical Internet research,Internet-Based Assessment of Oncology Health Care Professional Learning Style and Optimization of Materials for Web-Based Learning: Controlled Trial With Concealed Allocation,"BACKGROUND: Precision medicine has resulted in increasing complexity in the treatment of cancer. Web-based educational materials can help address the needs of oncology health care professionals seeking to understand up-to-date treatment strategies. OBJECTIVE: This study aimed to assess learning styles of oncology health care professionals and to determine whether learning style-tailored educational materials lead to enhanced learning. METHODS: In all, 21,465 oncology health care professionals were invited by email to participate in the fully automated, parallel group study. Enrollment and follow-up occurred between July 13 and September 7, 2015. Self-enrolled participants took a learning style survey and were assigned to the intervention or control arm using concealed alternating allocation. Participants in the intervention group viewed educational materials consistent with their preferences for learning (reading, listening, and/or watching); participants in the control group viewed educational materials typical of the My Cancer Genome website. Educational materials covered the topic of treatment of metastatic estrogen receptor-positive (ER+) breast cancer using cyclin-dependent kinases 4/6 (CDK4/6) inhibitors. Participant knowledge was assessed immediately before (pretest), immediately after (posttest), and 2 weeks after (follow-up test) review of the educational materials. Study statisticians were blinded to group assignment. RESULTS: A total of 751 participants enrolled in the study. Of these, 367 (48.9%) were allocated to the intervention arm and 384 (51.1%) were allocated to the control arm. Of those allocated to the intervention arm, 256 (69.8%) completed all assessments. Of those allocated to the control arm, 296 (77.1%) completed all assessments. An additional 12 participants were deemed ineligible and one withdrew. Of the 552 participants, 438 (79.3%) self-identified as multimodal learners. The intervention arm showed greater improvement in posttest score compared to the control group (0.4 points or 4.0% more improvement on average; P=.004) and a higher follow-up test score than the control group (0.3 points or 3.3% more improvement on average; P=.02). CONCLUSIONS: Although the study demonstrated more learning with learning style-tailored educational materials, the magnitude of increased learning and the largely multimodal learning styles preferred by the study participants lead us to conclude that future content-creation efforts should focus on multimodal educational materials rather than learning style-tailored content."," Adult; Education, Distance/*standards; Female; Health Personnel/*standards; Humans; Information Dissemination/*methods; Internet/*statistics & numerical data; Learning; Male; Medical Oncology/*standards; Middle Aged; Precision Medicine/*methods; Surveys and Questionnaires; Telemedicine/*methods; *Web-based Instruction; *continuing education; *e-learning; *education, distance; *information dissemination; *learning; *medical oncology/education; *online systems; *teaching materials",-2,-2,
120,Khajehali,2017,Artificial intelligence in medicine,Extract critical factors affecting the length of hospital stay of pneumonia patient by data mining (case study: an Iranian hospital),"MOTIVATION: Pneumonia is a prevalent infection of lower respiratory tract caused by infected lungs. Length of stay (LOS) in hospital is one of the simplest and most important indicators in hospital activity that is used for different purposes. The aim of this study is to explore the important factors affecting the LOS of patients with pneumonia in hospitals. METHODS: The clinical data set for the study were collected from 387 patients in a specialized hospital in Iran between 2009 and 2015. Patients discharge summary includes their demographic details, reasons for admission, prescribed medications for the patient, the result of laboratory tests, and length of treatment. RESULTS AND CONCLUSIONS: The proposed model in the study demonstrates the way various scenarios of data processing impact on the scale efficiency model, which points to the significance of the pre-processing in data mining. In this article, some methods were utilized; it is noteworthy that Bayesian boosting method led to better results in identifying the factors affecting LOS (accuracy 95.17%). In addition, it was found that 58% of patients younger than 15 years old and 74% of the elderly within the age range of 74-88 were more vulnerable to pneumonia disease. Also, it was found that the Meropenem is a relatively more effective medicine compared to other antibiotics which are used to treat pneumonia in the majority of age groups. Regardless of the impact of various laboratory findings (including CRP, ESR, WBC, NA, K), the patients LOS decreased as a result of Meropenem."," Adolescent; Adult; Age Factors; Aged; Aged, 80 and over; *Algorithms; Anti-Bacterial Agents/*therapeutic use; Bayes Theorem; Data Mining/*methods; Databases, Factual; Decision Trees; Female; Humans; Iran/epidemiology; *Length of Stay; Male; Meropenem; Middle Aged; Neural Networks (Computer); *Patient Admission; Patient Discharge; Patient Discharge Summaries; Pneumonia/diagnosis/*drug therapy/epidemiology; Predictive Value of Tests; Risk Factors; Thienamycins/therapeutic use; Time Factors; Treatment Outcome; Young Adult; Ensemble methods; Length of stay (LOS); Medical data mining; Patients; Pneumonia",-2,-2,
121,Househ,2017,Studies in health technology and informatics,The Hazards of Data Mining in Healthcare,"From the mid-1990s, data mining methods have been used to explore and find patterns and relationships in healthcare data. During the 1990s and early 2000's, data mining was a topic of great interest to healthcare researchers, as data mining showed some promise in the use of its predictive techniques to help model the healthcare system and improve the delivery of healthcare services. However, it was soon discovered that mining healthcare data had many challenges relating to the veracity of healthcare data and limitations around predictive modelling leading to failures of data mining projects. As the Big Data movement has gained momentum over the past few years, there has been a reemergence of interest in the use of data mining techniques and methods to analyze healthcare generated Big Data. Much has been written on the positive impacts of data mining on healthcare practice relating to issues of best practice, fraud detection, chronic disease management, and general healthcare decision making. Little has been written about the limitations and challenges of data mining use in healthcare. In this review paper, we explore some of the limitations and challenges in the use of data mining techniques in healthcare. Our results show that the limitations of data mining in healthcare include reliability of medical data, data sharing between healthcare organizations, inappropriate modelling leading to inaccurate predictions. We conclude that there are many pitfalls in the use of data mining in healthcare and more work is needed to show evidence of its utility in facilitating healthcare decision-making for healthcare providers, managers, and policy makers and more evidence is needed on data mining's overall impact on healthcare services and patient care.", *Data Mining; Decision Making; *Delivery of Health Care; Humans; Information Storage and Retrieval; Reproducibility of Results; Artificial Intelligence; Data Mining; Healthcare; Knowledge Discovery,-2,-2,
122,Househ,2017,Studies in health technology and informatics,"Big Data, Big Problems: A Healthcare Perspective","Much has been written on the benefits of big data for healthcare such as improving patient outcomes, public health surveillance, and healthcare policy decisions. Over the past five years, Big Data, and the data sciences field in general, has been hyped as the ""Holy Grail"" for the healthcare industry promising a more efficient healthcare system with the promise of improved healthcare outcomes. However, more recently, healthcare researchers are exposing the potential and harmful effects Big Data can have on patient care associating it with increased medical costs, patient mortality, and misguided decision making by clinicians and healthcare policy makers. In this paper, we review the current Big Data trends with a specific focus on the inadvertent negative impacts that Big Data could have on healthcare, in general, and specifically, as it relates to patient and clinical care. Our study results show that although Big Data is built up to be as a the ""Holy Grail"" for healthcare, small data techniques using traditional statistical methods are, in many cases, more accurate and can lead to more improved healthcare outcomes than Big Data methods. In sum, Big Data for healthcare may cause more problems for the healthcare industry than solutions, and in short, when it comes to the use of data in healthcare, ""size isn't everything.""", *Data Collection; *Decision Making; *Delivery of Health Care; Health Policy; Humans; *Patient Care; Statistics as Topic; Big Data; Challenges; Costs; Healthcare; Opportunities,-2,-2,
123,Bashir,2017,Systematic reviews,A systematic review of the processes used to link clinical trial registrations to their published results,"BACKGROUND: Studies measuring the completeness and consistency of trial registration and reporting rely on linking registries with bibliographic databases. In this systematic review, we quantified the processes used to identify these links. METHODS: PubMed and Embase databases were searched from inception to May 2016 for studies linking trial registries with bibliographic databases. The processes used to establish these links were categorised as automatic when the registration identifier was available in the bibliographic database or publication, or manual when linkage required inference or contacting of trial investigators. The number of links identified by each process was extracted where available. Linear regression was used to determine whether the proportions of links available via automatic processes had increased over time. RESULTS: In 43 studies that examined cohorts of registry entries, 24 used automatic and manual processes to find articles; 3 only automatic; and 11 only manual (5 did not specify). Twelve studies reported results for both manual and automatic processes and showed that a median of 23% (range from 13 to 42%) included automatic links to articles, while 17% (range from 5 to 42%) of registry entries required manual processes to find articles. There was no evidence that the proportion of registry entries with automatic links had increased (R (2) = 0.02, p = 0.36). In 39 studies that examined cohorts of articles, 21 used automatic and manual processes; 9 only automatic; and 2 only manual (7 did not specify). Sixteen studies reported numbers for automatic and manual processes and indicated that a median of 49% (range from 8 to 97%) of articles had automatic links to registry entries, and 10% (range from 0 to 28%) required manual processes to find registry entries. There was no evidence that the proportion of articles with automatic links to registry entries had increased (R (2) = 0.01, p = 0.73). CONCLUSIONS: The linkage of trial registries to their corresponding publications continues to require extensive manual processes. We did not find that the use of automatic linkage has increased over time. Further investigation is needed to inform approaches that will ensure publications are properly linked to trial registrations, thus enabling efficient monitoring of trial reporting."," *Biomedical Research; *Clinical Trials as Topic; *Databases, Bibliographic; Humans; Publication Bias; *Publishing; *Registries; Research Design; *Research Report; *Publication bias; *Reporting bias; *Systematic reviews as topic; *Trial registration",-1,1,
124,Wongkoblap,2017,Journal of medical Internet research,Researching Mental Health Disorders in the Era of Social Media: Systematic Review,"BACKGROUND: Mental illness is quickly becoming one of the most prevalent public health problems worldwide. Social network platforms, where users can express their emotions, feelings, and thoughts, are a valuable source of data for researching mental health, and techniques based on machine learning are increasingly used for this purpose. OBJECTIVE: The objective of this review was to explore the scope and limits of cutting-edge techniques that researchers are using for predictive analytics in mental health and to review associated issues, such as ethical concerns, in this area of research. METHODS: We performed a systematic literature review in March 2017, using keywords to search articles on data mining of social network data in the context of common mental health disorders, published between 2010 and March 8, 2017 in medical and computer science journals. RESULTS: The initial search returned a total of 5386 articles. Following a careful analysis of the titles, abstracts, and main texts, we selected 48 articles for review. We coded the articles according to key characteristics, techniques used for data collection, data preprocessing, feature extraction, feature selection, model construction, and model verification. The most common analytical method was text analysis, with several studies using different flavors of image analysis and social interaction graph analysis. CONCLUSIONS: Despite an increasing number of studies investigating mental health issues using social network data, some common problems persist. Assembling large, high-quality datasets of social media users with mental disorder is problematic, not only due to biases associated with the collection methods, but also with regard to managing consent and selecting appropriate analytics techniques.", Humans; Mental Disorders/*diagnosis; Social Media/*statistics & numerical data; *Social Networking; *anxiety; *artificial intelligence; *depression; *infodemiology; *machine learning; *mental disorders; *mental health; *public health informatics,-2,-2,
125,Dallora,2017,PloS one,Machine learning and microsimulation techniques on the prognosis of dementia: A systematic literature review,"BACKGROUND: Dementia is a complex disorder characterized by poor outcomes for the patients and high costs of care. After decades of research little is known about its mechanisms. Having prognostic estimates about dementia can help researchers, patients and public entities in dealing with this disorder. Thus, health data, machine learning and microsimulation techniques could be employed in developing prognostic estimates for dementia. OBJECTIVE: The goal of this paper is to present evidence on the state of the art of studies investigating and the prognosis of dementia using machine learning and microsimulation techniques. METHOD: To achieve our goal we carried out a systematic literature review, in which three large databases-Pubmed, Socups and Web of Science were searched to select studies that employed machine learning or microsimulation techniques for the prognosis of dementia. A single backward snowballing was done to identify further studies. A quality checklist was also employed to assess the quality of the evidence presented by the selected studies, and low quality studies were removed. Finally, data from the final set of studies were extracted in summary tables. RESULTS: In total 37 papers were included. The data summary results showed that the current research is focused on the investigation of the patients with mild cognitive impairment that will evolve to Alzheimer's disease, using machine learning techniques. Microsimulation studies were concerned with cost estimation and had a populational focus. Neuroimaging was the most commonly used variable. CONCLUSIONS: Prediction of conversion from MCI to AD is the dominant theme in the selected studies. Most studies used ML techniques on Neuroimaging data. Only a few data sources have been recruited by most studies and the ADNI database is the one most commonly used. Only two studies have investigated the prediction of epidemiological aspects of Dementia using either ML or MS techniques. Finally, care should be taken when interpreting the reported accuracy of ML techniques, given studies' different contexts.", *Computer Simulation; Dementia/*physiopathology; Humans; *Machine Learning; Prognosis,-2,-2,
126,Stansfield,2017,Research synthesis methods,Text mining for search term development in systematic reviewing: A discussion of some methods and challenges,"Using text mining to aid the development of database search strings for topics described by diverse terminology has potential benefits for systematic reviews; however, methods and tools for accomplishing this are poorly covered in the research methods literature. We briefly review the literature on applications of text mining for search term development for systematic reviewing. We found that the tools can be used in 5 overarching ways: improving the precision of searches; identifying search terms to improve search sensitivity; aiding the translation of search strategies across databases; searching and screening within an integrated system; and developing objectively derived search strategies. Using a case study and selected examples, we then reflect on the utility of certain technologies (term frequency-inverse document frequency and Termine, term frequency, and clustering) in improving the precision and sensitivity of searches. Challenges in using these tools are discussed. The utility of these tools is influenced by the different capabilities of the tools, the way the tools are used, and the text that is analysed. Increased awareness of how the tools perform facilitates the further development of methods for their use in systematic reviews."," *Data Mining; Databases, Bibliographic; Databases, Factual; Humans; Information Storage and Retrieval; *Review Literature as Topic; Search Engine; clustering; information retrieval; systematic search; text mining",2,1,
127,Seo,2017,PloS one,Ultrasound-guided cable-free 13-gauge vacuum-assisted biopsy of non-mass breast lesions,"PURPOSE: To compare the outcomes of ultrasound-guided core biopsy for non-mass breast lesions by the novel 13-gauge cable-free vacuum-assisted biopsy (VAB) and by the conventional 14-gauge semi-automated core needle biopsy (CCNB). MATERIALS AND METHODS: Our institutional review board approved this prospective study, and all patients provided written informed consent. Among 1840 ultrasound-guided percutaneous biopsies performed from August 2013 to December 2014, 145 non-mass breast lesions with suspicious microcalcifications on mammography or corresponding magnetic resonance imaging finding were subjected to 13-gauge VAB or 14-gauge CCNB. We evaluated the technical success rates, average specimen numbers, and tissue sampling time. We also compared the results of percutaneous biopsy and final surgical pathologic diagnosis to analyze the rates of diagnostic upgrade or downgrade. RESULTS: Ultrasound-guided VAB successfully targeted and sampled all lesions, whereas CCNB failed to demonstrate calcification in four (10.3%) breast lesions with microcalcification on specimen mammography. The mean sampling time were 238.6 and 170.6 seconds for VAB and CCNB, respectively. No major complications were observed with either method. Ductal carcinoma in situ (DCIS) and atypical ductal hyperplasia (ADH) lesions were more frequently upgraded after CCNB (8/23 and 3/5, respectively) than after VAB (2/26 and 0/4, respectively P = 0.028). CONCLUSION: Non-mass breast lesions were successfully and accurately biopsied using cable-free VAB. The underestimation rate of ultrasound-detected non-mass lesion was significantly lower with VAB than with CCNB. TRIAL REGISTRATION: CRiS KCT0002267."," Adult; Aged; Aged, 80 and over; Biopsy, Needle; Breast/*pathology; Breast Diseases/*pathology; Breast Neoplasms/pathology; Calcification, Physiologic; Diagnosis, Differential; Female; Humans; Hyperplasia/pathology; Image-Guided Biopsy; Magnetic Resonance Imaging; Mammography; Middle Aged; Prospective Studies",-2,-2,
128,Sunwoo,2017,PloS one,Computer-aided detection of brain metastasis on 3D MR imaging: Observer performance study,"PURPOSE: To assess the effect of computer-aided detection (CAD) of brain metastasis (BM) on radiologists' diagnostic performance in interpreting three-dimensional brain magnetic resonance (MR) imaging using follow-up imaging and consensus as the reference standard. MATERIALS AND METHODS: The institutional review board approved this retrospective study. The study cohort consisted of 110 consecutive patients with BM and 30 patients without BM. The training data set included MR images of 80 patients with 450 BM nodules. The test set included MR images of 30 patients with 134 BM nodules and 30 patients without BM. We developed a CAD system for BM detection using template-matching and K-means clustering algorithms for candidate detection and an artificial neural network for false-positive reduction. Four reviewers (two neuroradiologists and two radiology residents) interpreted the test set images before and after the use of CAD in a sequential manner. The sensitivity, false positive (FP) per case, and reading time were analyzed. A jackknife free-response receiver operating characteristic (JAFROC) method was used to determine the improvement in the diagnostic accuracy. RESULTS: The sensitivity of CAD was 87.3% with an FP per case of 302.4. CAD significantly improved the diagnostic performance of the four reviewers with a figure-of-merit (FOM) of 0.874 (without CAD) vs. 0.898 (with CAD) according to JAFROC analysis (p < 0.01). Statistically significant improvement was noted only for less-experienced reviewers (FOM without vs. with CAD, 0.834 vs. 0.877, p < 0.01). The additional time required to review the CAD results was approximately 72 sec (40% of the total review time). CONCLUSION: CAD as a second reader helps radiologists improve their diagnostic performance in the detection of BM on MR imaging, particularly for less-experienced reviewers."," Aged; Algorithms; Brain Neoplasms/*diagnosis/*secondary; Female; Humans; Imaging, Three-Dimensional/*methods; Machine Learning; Magnetic Resonance Imaging/*methods; Male; Middle Aged; Models, Theoretical; Retrospective Studies; Sensitivity and Specificity; Software; Tomography, X-Ray Computed",-2,-2,
129,Pramono,2017,PloS one,Automatic adventitious respiratory sound analysis: A systematic review,"BACKGROUND: Automatic detection or classification of adventitious sounds is useful to assist physicians in diagnosing or monitoring diseases such as asthma, Chronic Obstructive Pulmonary Disease (COPD), and pneumonia. While computerised respiratory sound analysis, specifically for the detection or classification of adventitious sounds, has recently been the focus of an increasing number of studies, a standardised approach and comparison has not been well established. OBJECTIVE: To provide a review of existing algorithms for the detection or classification of adventitious respiratory sounds. This systematic review provides a complete summary of methods used in the literature to give a baseline for future works. DATA SOURCES: A systematic review of English articles published between 1938 and 2016, searched using the Scopus (1938-2016) and IEEExplore (1984-2016) databases. Additional articles were further obtained by references listed in the articles found. Search terms included adventitious sound detection, adventitious sound classification, abnormal respiratory sound detection, abnormal respiratory sound classification, wheeze detection, wheeze classification, crackle detection, crackle classification, rhonchi detection, rhonchi classification, stridor detection, stridor classification, pleural rub detection, pleural rub classification, squawk detection, and squawk classification. STUDY SELECTION: Only articles were included that focused on adventitious sound detection or classification, based on respiratory sounds, with performance reported and sufficient information provided to be approximately repeated. DATA EXTRACTION: Investigators extracted data about the adventitious sound type analysed, approach and level of analysis, instrumentation or data source, location of sensor, amount of data obtained, data management, features, methods, and performance achieved. DATA SYNTHESIS: A total of 77 reports from the literature were included in this review. 55 (71.43%) of the studies focused on wheeze, 40 (51.95%) on crackle, 9 (11.69%) on stridor, 9 (11.69%) on rhonchi, and 18 (23.38%) on other sounds such as pleural rub, squawk, as well as the pathology. Instrumentation used to collect data included microphones, stethoscopes, and accelerometers. Several references obtained data from online repositories or book audio CD companions. Detection or classification methods used varied from empirically determined thresholds to more complex machine learning techniques. Performance reported in the surveyed works were converted to accuracy measures for data synthesis. LIMITATIONS: Direct comparison of the performance of surveyed works cannot be performed as the input data used by each was different. A standard validation method has not been established, resulting in different works using different methods and performance measure definitions. CONCLUSION: A review of the literature was performed to summarise different analysis approaches, features, and methods used for the analysis. The performance of recent studies showed a high agreement with conventional non-automatic identification. This suggests that automated adventitious sound detection or classification is a promising solution to overcome the limitations of conventional auscultation and to assist in the monitoring of relevant diseases."," Asthma/diagnosis/physiopathology; *Automation; Humans; Pneumonia/diagnosis/physiopathology; Pulmonary Disease, Chronic Obstructive/diagnosis/physiopathology; *Respiratory Sounds",-2,-2,
130,Aakre,2017,International journal of medical informatics,Prospective validation of a near real-time EHR-integrated automated SOFA score calculator,"OBJECTIVES: We created an algorithm for automated Sequential Organ Failure Assessment (SOFA) score calculation within the Electronic Health Record (EHR) to facilitate detection of sepsis based on the Third International Consensus Definitions for Sepsis and Septic Shock (SEPSIS-3) clinical definition. We evaluated the accuracy of near real-time and daily automated SOFA score calculation compared with manual score calculation. METHODS: Automated SOFA scoring computer programs were developed using available EHR data sources and integrated into a critical care focused patient care dashboard at Mayo Clinic in Rochester, Minnesota. We prospectively compared the accuracy of automated versus manual calculation for a sample of patients admitted to the medical intensive care unit at Mayo Clinic Hospitals in Rochester, Minnesota and Jacksonville, Florida. Agreement was calculated with Cohen's kappa statistic. Reason for discrepancy was tabulated during manual review. RESULTS: Random spot check comparisons were performed 134 times on 27 unique patients, and daily SOFA score comparisons were performed for 215 patients over a total of 1206 patient days. Agreement between automatically scored and manually scored SOFA components for both random spot checks (696 pairs, kappa=0.89) and daily calculation (5972 pairs, kappa=0.89) was high. The most common discrepancies were in the respiratory component (inaccurate fraction of inspired oxygen retrieval; 200/1206) and creatinine (normal creatinine in patients with no urine output on dialysis; 128/1094). 147 patients were at risk of developing sepsis after intensive care unit admission, 10 later developed sepsis confirmed by chart review. All were identified before onset of sepsis with the DeltaSOFA>/=2 point criterion and 46 patients were false-positives. CONCLUSIONS: Near real-time automated SOFA scoring was found to have strong agreement with manual score calculation and may be useful for the detection of sepsis utilizing the new SEPSIS-3 definition.", Aged; *Algorithms; Consensus; Critical Care; *Electronic Health Records; Female; Humans; Intensive Care Units; Male; Middle Aged; Minnesota/epidemiology; *Organ Dysfunction Scores; Prospective Studies; Sepsis/*diagnosis/epidemiology; *Automation; *Clinical decision support; *Computer-assisted diagnosis; *Early diagnosis; *Sepsis,-2,-2,
131,Tsuchiya,2017,PloS one,Visual field changes after vitrectomy with internal limiting membrane peeling for epiretinal membrane or macular hole in glaucomatous eyes,"PURPOSE: To investigate visual field changes after vitrectomy for macular diseases in glaucomatous eyes. METHODS: A retrospective review of 54 eyes from 54 patients with glaucoma, who underwent vitrectomy for epiretinal membrane (ERM; 42 eyes) or macular hole (MH; 12 eyes). Standard automated perimetry (Humphrey visual field 24-2 program) was performed and analyzed preoperatively and twice postoperatively (1st and 2nd sessions; 4.7 +/- 2.5, 10.3 +/- 3.7 months after surgery, respectively). Postoperative visual field sensitivity at each test point was compared with the preoperative value. Longitudinal changes in mean visual field sensitivity (MVFS) of the 12 test points within 10 degrees eccentricity (center) and the remaining test points (periphery), best-corrected visual acuity (BCVA), intraocular pressure (IOP), and ganglion cell complex (GCC) thickness, and the association of factors with changes in central or peripheral MVFS over time were analyzed using linear mixed-effects models. In addition, 45 eyes from 45 patients without glaucoma who underwent vitrectomy for epiretinal membrane (ERM; 34 eyes) or macular hole (MH; 11 eyes) were similarly examined and statistically analyzed (control group). RESULTS: In glaucomatous eyes, visual field test points changed significantly and reproducibly; two points deteriorated only at the center and twelve points improved only at the periphery. Central MVFS decreased (p = 0.03), whereas peripheral MVFS increased postoperatively (p = 0.010). In the control group, no visual field test points showed deterioration, and central MVFS did not change significantly after vitrectomy. BCVA improved, GCC thickness decreased, and IOP did not change postoperatively in both groups. The linear mixed-effects models identified older age, systemic hypertension, longer axial length, and preoperative medication scores of >/=2 as risk factors for central MVFS deterioration in glaucomatous eyes. CONCLUSIONS: Visual field sensitivity within 10 degrees eccentricity may deteriorate after vitrectomy for ERM or MH in glaucomatous eyes.", Aged; Epiretinal Membrane/*complications; Female; Glaucoma/*complications/physiopathology/*surgery; Humans; Male; Postoperative Period; Retinal Perforations/*complications; Retrospective Studies; *Visual Fields; Vitrectomy/*adverse effects,-2,-2,
132,Raichand,2017,Systematic reviews,Conclusions in systematic reviews of mammography for breast cancer screening and associations with review design and author characteristics,"BACKGROUND: Debates about the benefits and harms of mammography continue despite the accumulation of evidence. We sought to quantify the disagreement across systematic reviews of mammography and determine whether author or design characteristics were associated with conclusions that were favourable to the use of mammography for routine breast cancer screening. METHODS: We identified systematic reviews of mammography published between January 2000 and November 2015, and extracted information about the selection of evidence, age groups, the use of meta-analysis, and authors' professions and financial competing interest disclosures. Conclusions about specific age groups were graded as favourable if they stated that there were meaningful benefits, that benefits of mammography outweighed harms, or that harms were inconsequential. The main outcome measures were the proportions of favourable conclusions relative to review design and author characteristics. RESULTS: From 59 conclusions identified in 50 reviews, 42% (25/59) were graded as favourable by two investigators. Among the conclusions produced by clinicians, 63% (12/19) were graded as favourable compared to 32% (13/40) from other authors. In the 50-69 age group where the largest proportion of systematic reviews were focused, conclusions drawn by authors without financial competing interests (odds ratio 0.06; 95% CI 0.07-0.56) and non-clinicians (odds ratio 0.11; 95% CI 0.01-0.84) were less likely to be graded as favourable. There was no trend in the proportion of favourable conclusions over the period, and we found no significant association between review design characteristics and favourable conclusions. CONCLUSIONS: Differences in the conclusions of systematic reviews of the evidence for mammography have persisted for 15 years. We found no strong evidence that design characteristics were associated with greater support for the benefits of mammography in routine breast cancer screening. Instead, the results suggested that the specific expertise and competing interests of the authors influenced the conclusions of systematic reviews.", Breast Neoplasms/*prevention & control; Early Detection of Cancer; Humans; *Mammography; *Mass Screening; Outcome Assessment (Health Care); *Research Design; *Bias; *Breast cancer; *Competing interests; *Mammography screening; *Systematic reviews as topic,-2,-2,
133,Fong,2017,International journal of medical informatics,Integrating natural language processing expertise with patient safety event review committees to improve the analysis of medication events,"OBJECTIVES: Many healthcare providers have implemented patient safety event reporting systems to better understand and improve patient safety. Reviewing and analyzing these reports is often time consuming and resource intensive because of both the quantity of reports and length of free-text descriptions in the reports. METHODS: Natural language processing (NLP) experts collaborated with clinical experts on a patient safety committee to assist in the identification and analysis of medication related patient safety events. Different NLP algorithmic approaches were developed to identify four types of medication related patient safety events and the models were compared. RESULTS: Well performing NLP models were generated to categorize medication related events into pharmacy delivery delays, dispensing errors, Pyxis discrepancies, and prescriber errors with receiver operating characteristic areas under the curve of 0.96, 0.87, 0.96, and 0.81 respectively. We also found that modeling the brief without the resolution text generally improved model performance. These models were integrated into a dashboard visualization to support the patient safety committee review process. CONCLUSIONS: We demonstrate the capabilities of various NLP models and the use of two text inclusion strategies at categorizing medication related patient safety events. The NLP models and visualization could be used to improve the efficiency of patient safety event data review and analysis."," Advisory Committees; Data Interpretation, Statistical; Drug-Related Side Effects and Adverse Reactions/*prevention & control; Humans; Medication Errors/*prevention & control; *Natural Language Processing; *Patient Safety; *Pharmaceutical Preparations; Risk Management; *Machine learning; *Medication; *Patient safety events; *Visualization",-2,-2,
134,Slovis,2017,Journal of the American Medical Informatics Association : JAMIA,Asynchronous automated electronic laboratory result notifications: a systematic review,"Objective: To systematically review the literature pertaining to asynchronous automated electronic notifications of laboratory results to clinicians. Methods: PubMed, Web of Science, and the Cochrane Collaboration were queried for studies pertaining to automated electronic notifications of laboratory results. A title review was performed on the primary results, with a further abstract review and full review to produce the final set of included articles. Results: The full review included 34 articles, representing 19 institutions. Of these, 19 reported implementation and design of systems, 11 reported quasi-experimental studies, 3 reported a randomized controlled trial, and 1 was a meta-analysis. Twenty-seven articles included alerts of critical results, while 5 focused on urgent notifications and 2 on elective notifications. There was considerable variability in clinical setting, system implementation, and results presented. Conclusion: Several asynchronous automated electronic notification systems for laboratory results have been evaluated, most from >10 years ago. Further research on the effect of notifications on clinicians as well as the use of modern electronic health records and new methods of notification is warranted to determine their effects on workflow and clinical outcomes.", *Clinical Alarms; *Clinical Laboratory Techniques; *Communication; Electronic Health Records; Humans; Medical Informatics; Time Factors; alerts; automated; critical; laboratory; notifications,-2,-2,
135,Krauss,2017,Studies in health technology and informatics,Challenges and Approaches to Make Multidisciplinary Team Meetings Interoperable - The KIMBo Project,"BACKGROUND: Multidisciplinary team meetings (MDTMs) are already in use for certain areas in healthcare (e.g. treatment of cancer). Due to the lack of common standards and accessibility for the applied IT systems, their potential is not yet completely exploited. OBJECTIVES: Common requirements for MDTMs shall be identified and aggregated into a process definition to be automated by an application architecture utilizing modern standards in electronic healthcare, e.g. HL7 FHIR. METHODS: To identify requirements, an extensive literature review as well as semi-structured expert interviews were conducted. RESULTS: Results showed, that interoperability and flexibility in terms of the process are key requirements to be addressed. An architecture blueprint as well as an aggregated process definition were derived from the insights gained. To evaluate the feasibility of identified requirements, methods of explorative prototyping in software engineering were used. CONCLUSION: MDTMs will become an important part of modern and future healthcare but the need for standardization in terms of interoperability is imminent.", *Delivery of Health Care; Health Information Systems; Humans; *Patient Care Team; *Software; Automation; Bpmn; Hl7 fhir; Interoperability; Multidisciplinary Team Meeting; Workflow,-2,-2,
136,Provoost,2017,Journal of medical Internet research,Embodied Conversational Agents in Clinical Psychology: A Scoping Review,"BACKGROUND: Embodied conversational agents (ECAs) are computer-generated characters that simulate key properties of human face-to-face conversation, such as verbal and nonverbal behavior. In Internet-based eHealth interventions, ECAs may be used for the delivery of automated human support factors. OBJECTIVE: We aim to provide an overview of the technological and clinical possibilities, as well as the evidence base for ECA applications in clinical psychology, to inform health professionals about the activity in this field of research. METHODS: Given the large variety of applied methodologies, types of applications, and scientific disciplines involved in ECA research, we conducted a systematic scoping review. Scoping reviews aim to map key concepts and types of evidence underlying an area of research, and answer less-specific questions than traditional systematic reviews. Systematic searches for ECA applications in the treatment of mood, anxiety, psychotic, autism spectrum, and substance use disorders were conducted in databases in the fields of psychology and computer science, as well as in interdisciplinary databases. Studies were included if they conveyed primary research findings on an ECA application that targeted one of the disorders. We mapped each study's background information, how the different disorders were addressed, how ECAs and users could interact with one another, methodological aspects, and the study's aims and outcomes. RESULTS: This study included N=54 publications (N=49 studies). More than half of the studies (n=26) focused on autism treatment, and ECAs were used most often for social skills training (n=23). Applications ranged from simple reinforcement of social behaviors through emotional expressions to sophisticated multimodal conversational systems. Most applications (n=43) were still in the development and piloting phase, that is, not yet ready for routine practice evaluation or application. Few studies conducted controlled research into clinical effects of ECAs, such as a reduction in symptom severity. CONCLUSIONS: ECAs for mental disorders are emerging. State-of-the-art techniques, involving, for example, communication through natural language or nonverbal behavior, are increasingly being considered and adopted for psychotherapeutic interventions in ECA research with promising results. However, evidence on their clinical application remains scarce. At present, their value to clinical practice lies mostly in the experimental determination of critical human support factors. In the context of using ECAs as an adjunct to existing interventions with the aim of supporting users, important questions remain with regard to the personalization of ECAs' interaction with users, and the optimal timing and manner of providing support. To increase the evidence base with regard to Internet interventions, we propose an additional focus on low-tech ECA solutions that can be rapidly developed, tested, and applied in routine practice."," *Communication; Humans; Psychology, Clinical/*methods; Telemedicine/*statistics & numerical data; *clinical psychology; *eHealth; *embodied conversational agent; *health behavior; *human computer interaction; *intelligent agent; *mental disorders; *review",-2,-2,
137,Aguilera,2017,Journal of medical Internet research,Automated Text Messaging as an Adjunct to Cognitive Behavioral Therapy for Depression: A Clinical Trial,"BACKGROUND: Cognitive Behavioral Therapy (CBT) for depression is efficacious, but effectiveness is limited when implemented in low-income settings due to engagement difficulties including nonadherence with skill-building homework and early discontinuation of treatment. Automated messaging can be used in clinical settings to increase dosage of depression treatment and encourage sustained engagement with psychotherapy. OBJECTIVES: The aim of this study was to test whether a text messaging adjunct (mood monitoring text messages, treatment-related text messages, and a clinician dashboard to display patient data) increases engagement and improves clinical outcomes in a group CBT treatment for depression. Specifically, we aim to assess whether the text messaging adjunct led to an increase in group therapy sessions attended, an increase in duration of therapy attended, and reductions in Patient Health Questionnaire-9 item (PHQ-9) symptoms compared with the control condition of standard group CBT in a sample of low-income Spanish speaking Latino patients. METHODS: Patients in an outpatient behavioral health clinic were assigned to standard group CBT for depression (control condition; n=40) or the same treatment with the addition of a text messaging adjunct (n=45). The adjunct consisted of a daily mood monitoring message, a daily message reiterating the theme of that week's content, and medication and appointment reminders. Mood data and qualitative responses were sent to a Web-based platform (HealthySMS) for review by the therapist and displayed in session as a tool for teaching CBT skills. RESULTS: Intent-to-treat analyses on therapy attendance during 16 sessions of weekly therapy found that patients assigned to the text messaging adjunct stayed in therapy significantly longer (median of 13.5 weeks before dropping out) than patients assigned to the control condition (median of 3 weeks before dropping out; Wilcoxon-Mann-Whitney z=-2.21, P=.03). Patients assigned to the text messaging adjunct also generally attended more sessions (median=6 sessions) during this period than patients assigned to the control condition (median =2.5 sessions), but the effect was not significant (Wilcoxon-Mann-Whitney z=-1.65, P=.10). Both patients assigned to the text messaging adjunct (B=-.29, 95% CI -0.38 to -0.19, z=-5.80, P<.001) and patients assigned to the control conditions (B=-.20, 95% CI -0.32 to -0.07, z=-3.12, P=.002) experienced significant decreases in depressive symptom severity over the course of treatment; however, the conditions did not significantly differ in their degree of symptom reduction. CONCLUSIONS: This study provides support for automated text messaging as a tool to sustain engagement in CBT for depression over time. There were no differences in depression outcomes between conditions, but this may be influenced by low follow-up rates of patients who dropped out of treatment.", Cognitive Behavioral Therapy/*methods; Depression/*therapy; Female; Humans; Male; Middle Aged; Telemedicine; Text Messaging/*statistics & numerical data; *Latinos; *cognitive behavioral therapy; *depression; *mental health; *mhealth; *text messaging,-2,-2,
138,Dudchenko,2017,Studies in health technology and informatics,Decision Support Systems in Cardiology: A Systematic Review,The aim of this work was to identify the most common approaches used in the intelligent decision support systems employed in the diagnosis of cardiovascular diseases and identify accuracy of these systems. Forty-one relevant publications were included in the review using Scopus and Web of Science. Knowledge base and fuzzy logic and ANN is the most commonly used approach to diagnosis and prediction. The accuracy of the considered systems reaches 98%.," Artificial Intelligence; *Cardiology; *Decision Support Systems, Clinical; *Expert Systems; Fuzzy Logic; Humans; Knowledge Bases; Software; Heart disease; prediction and diagnosis systems",-2,-2,
139,Hassan,2017,PloS one,The putative drug efflux systems of the Bacillus cereus group,"The Bacillus cereus group of bacteria includes seven closely related species, three of which, B. anthracis, B. cereus and B. thuringiensis, are pathogens of humans, animals and/or insects. Preliminary investigations into the transport capabilities of different bacterial lineages suggested that genes encoding putative efflux systems were unusually abundant in the B. cereus group compared to other bacteria. To explore the drug efflux potential of the B. cereus group all putative efflux systems were identified in the genomes of prototypical strains of B. cereus, B. anthracis and B. thuringiensis using our Transporter Automated Annotation Pipeline. More than 90 putative drug efflux systems were found within each of these strains, accounting for up to 2.7% of their protein coding potential. Comparative analyses demonstrated that the efflux systems are highly conserved between these species; 70-80% of the putative efflux pumps were shared between all three strains studied. Furthermore, 82% of the putative efflux system proteins encoded by the prototypical B. cereus strain ATCC 14579 (type strain) were found to be conserved in at least 80% of 169 B. cereus group strains that have high quality genome sequences available. However, only a handful of these efflux pumps have been functionally characterized. Deletion of individual efflux pump genes from B. cereus typically had little impact to drug resistance phenotypes or the general fitness of the strains, possibly because of the large numbers of alternative efflux systems that may have overlapping substrate specificities. Therefore, to gain insight into the possible transport functions of efflux systems in B. cereus, we undertook large-scale qRT-PCR analyses of efflux pump gene expression following drug shocks and other stress treatments. Clustering of gene expression changes identified several groups of similarly regulated systems that may have overlapping drug resistance functions. In this article we review current knowledge of the small molecule efflux pumps encoded by the B. cereus group and suggest the likely functions of numerous uncharacterised pumps."," Anti-Bacterial Agents/pharmacology; Bacillus cereus/drug effects/genetics/*metabolism; Biological Transport; Genes, Bacterial; Microbial Sensitivity Tests; Reverse Transcriptase Polymerase Chain Reaction",-2,-2,
140,Leclere,2017,PloS one,Automated detection of hospital outbreaks: A systematic review of methods,"OBJECTIVES: Several automated algorithms for epidemiological surveillance in hospitals have been proposed. However, the usefulness of these methods to detect nosocomial outbreaks remains unclear. The goal of this review was to describe outbreak detection algorithms that have been tested within hospitals, consider how they were evaluated, and synthesize their results. METHODS: We developed a search query using keywords associated with hospital outbreak detection and searched the MEDLINE database. To ensure the highest sensitivity, no limitations were initially imposed on publication languages and dates, although we subsequently excluded studies published before 2000. Every study that described a method to detect outbreaks within hospitals was included, without any exclusion based on study design. Additional studies were identified through citations in retrieved studies. RESULTS: Twenty-nine studies were included. The detection algorithms were grouped into 5 categories: simple thresholds (n = 6), statistical process control (n = 12), scan statistics (n = 6), traditional statistical models (n = 6), and data mining methods (n = 4). The evaluation of the algorithms was often solely descriptive (n = 15), but more complex epidemiological criteria were also investigated (n = 10). The performance measures varied widely between studies: e.g., the sensitivity of an algorithm in a real world setting could vary between 17 and 100%. CONCLUSION: Even if outbreak detection algorithms are useful complementary tools for traditional surveillance, the heterogeneity in results among published studies does not support quantitative synthesis of their performance. A standardized framework should be followed when evaluating outbreak detection methods to allow comparison of algorithms across studies and synthesis of results.", Algorithms; Cross Infection/*diagnosis; *Disease Outbreaks; *Hospitals; Humans; Population Surveillance/*methods,-2,,
141,Yergens,2017,Studies in health technology and informatics,Automated Identification of National Health Survey Research Topics in the Academic Literature,"National health surveys are routinely conducted to provide value data about a country's health status and the health services being consumed by the population. This information is used for surveillance, research, and the planning of healthcare services at local and national levels. Although these national health surveys are viewed as important resources for public and population health, there is limited information as to the type of research being conducted with these surveys. This study investigates, through the use of automated text data mining, an approach to identify and collate the type of academic literature being published using national health surveys.", Algorithms; Canada/epidemiology; Data Mining/*methods; *Health Care Surveys; *Health Surveys; Humans; Republic of Korea/epidemiology; United States/epidemiology; Text data mining; algorithm; epidemiology; literature review; national health survey,1,1,
142,Yoon,2017,PloS one,Mobile Genome Express (MGE): A comprehensive automatic genetic analyses pipeline with a mobile device,"The development of next-generation sequencing (NGS) technology allows to sequence whole exomes or genome. However, data analysis is still the biggest bottleneck for its wide implementation. Most laboratories still depend on manual procedures for data handling and analyses, which translates into a delay and decreased efficiency in the delivery of NGS results to doctors and patients. Thus, there is high demand for developing an automatic and an easy-to-use NGS data analyses system. We developed comprehensive, automatic genetic analyses controller named Mobile Genome Express (MGE) that works in smartphones or other mobile devices. MGE can handle all the steps for genetic analyses, such as: sample information submission, sequencing run quality check from the sequencer, secured data transfer and results review. We sequenced an Actrometrix control DNA containing multiple proven human mutations using a targeted sequencing panel, and the whole analysis was managed by MGE, and its data reviewing program called ELECTRO. All steps were processed automatically except for the final sequencing review procedure with ELECTRO to confirm mutations. The data analysis process was completed within several hours. We confirmed the mutations that we have identified were consistent with our previous results obtained by using multi-step, manual pipelines."," Computational Biology; Computers, Handheld; *DNA Mutational Analysis; High-Throughput Nucleotide Sequencing; Humans; Software",-2,-2,
143,Witzel,2017,Systematic reviews,Consolidating emerging evidence surrounding HIVST and HIVSS: a rapid systematic mapping protocol,"BACKGROUND: HIV self-testing (HIVST) is becoming popular with policy makers and commissioners globally, with a key aim of expanding access through reducing barriers to testing for individuals at risk of HIV infection. HIV self-sampling (HIVSS) was available previously to self-testing but was confined mainly to the USA and the UK. It remains to be seen whether the momentum behind HIVST will also energise efforts to expand HIVSS. Recent years have seen a rapid growth in the type of evidence related to these interventions as well as several systematic reviews. The vast majority of this evidence relates to acceptability as well as values and preferences, although new types of evidence are emerging. This systematic map aims to consolidate all emerging evidence related to HIVST and HIVSS to respond to this rapidly changing area. METHODS: We will systematically search databases and the abstracts of five conferences from 2006 to the present date, with monthly-automated database searches. Searches will combine key terms relating to HIV (e.g. HIV, AIDS, human immune-deficiency syndrome) with terms related to self-testing (e.g. home-test, self-test, mail-test, home dried blood spot test). Abstracts will be reviewed against inclusion criteria in duplicate. Data will be manually extracted through a standard form and then entered to an open access relational map (HIVST.org). When new and sufficient evidence emerges which addresses existing knowledge gaps, we will complete a review on a relevant topic. DISCUSSION: This innovative approach will allow rapid cataloguing, documenting and dissemination of new evidence and key findings as they emerge into the public domain. SYSTEMATIC REVIEW REGISTRATION: This protocol has not been registered with PROSPERO as they do not register systematic maps.", Administrative Personnel; HIV Infections/*diagnosis; Health Policy; Humans; Mass Screening/*methods; Patient Acceptance of Health Care; *Self Care; Systematic Reviews as Topic,1,1,
144,Heitkemper,2017,Journal of the American Medical Informatics Association : JAMIA,Do health information technology self-management interventions improve glycemic control in medically underserved adults with diabetes? A systematic review and meta-analysis,"Objective: The purpose of this systematic review and meta-analysis was to examine the effect of health information technology (HIT) diabetes self-management education (DSME) interventions on glycemic control in medically underserved patients. Materials and Methods: Following an a priori protocol, 5 databases were searched. Studies were appraised for quality using the Cochrane Risk of Bias assessment. Studies reporting either hemoglobin A1c pre- and post-intervention or its change at 6 or 12 months were eligible for inclusion in the meta-analysis using random effects models. Results: Thirteen studies met the criteria for the systematic review and 10 for the meta-analysis and represent data from 3257 adults with diabetes (mean age 55 years; 66% female; 74% racial/ethnic minorities). Most studies ( n = 10) reflected an unclear risk of bias. Interventions varied by HIT type: computer software without Internet ( n = 2), cellular/automated telephone ( n = 4), Internet-based ( n = 4), and telemedicine/telehealth ( n = 3). Pooled A1c decreases were found at 6 months (-0.36 (95% CI, -0.53 and -0.19]; I 2 = 35.1%, Q = 5.0), with diminishing effect at 12 months (-0.27 [95% CI, -0.49 and -0.04]; I 2 = 42.4%, Q = 10.4). Discussion: Findings suggest that medically underserved patients with diabetes achieve glycemic benefit following HIT DSME interventions, with dissipating but significant effects at 12 months. Telemedicine/telehealth interventions were the most successful HIT type because they incorporated interaction with educators similar to in-person DSME. Conclusion: These results are similar to in-person DSME in medically underserved patients, showing that well-designed HIT DSME has the potential to increase access and improve outcomes for this vulnerable group.", Diabetes Mellitus/blood/*therapy; Female; Glycated Hemoglobin A/analysis; Humans; Male; *Medical Informatics; *Patient Education as Topic; *Self-Management; health information technology; medically underserved/health disparities; meta-analysis; self-management; type 2 diabetes,-2,-2,
145,Olofsson,2017,Research synthesis methods,Can abstract screening workload be reduced using text mining? User experiences of the tool Rayyan,"BACKGROUND: One time-consuming aspect of conducting systematic reviews is the task of sifting through abstracts to identify relevant studies. One promising approach for reducing this burden uses text mining technology to identify those abstracts that are potentially most relevant for a project, allowing those abstracts to be screened first. OBJECTIVES: To examine the effectiveness of the text mining functionality of the abstract screening tool Rayyan. User experiences were collected. METHODS: Rayyan was used to screen abstracts for 6 reviews in 2015. After screening 25%, 50%, and 75% of the abstracts, the screeners logged the relevant references identified. A survey was sent to users. RESULTS: After screening half of the search result with Rayyan, 86% to 99% of the references deemed relevant to the study were identified. Of those studies included in the final reports, 96% to 100% were already identified in the first half of the screening process. Users rated Rayyan 4.5 out of 5. DISCUSSION: The text mining function in Rayyan successfully helped reviewers identify relevant studies early in the screening process.", *Data Mining/methods; Humans; Review Literature as Topic; Workload; abstract screening; review efficiency; systematic reviews; text mining,2,1,
146,Han,2017,Journal of the American Medical Informatics Association : JAMIA,Development of an automated assessment tool for MedWatch reports in the FDA adverse event reporting system,"Objective: As the US Food and Drug Administration (FDA) receives over a million adverse event reports associated with medication use every year, a system is needed to aid FDA safety evaluators in identifying reports most likely to demonstrate causal relationships to the suspect medications. We combined text mining with machine learning to construct and evaluate such a system to identify medication-related adverse event reports. Methods: FDA safety evaluators assessed 326 reports for medication-related causality. We engineered features from these reports and constructed random forest, L1 regularized logistic regression, and support vector machine models. We evaluated model accuracy and further assessed utility by generating report rankings that represented a prioritized report review process. Results: Our random forest model showed the best performance in report ranking and accuracy, with an area under the receiver operating characteristic curve of 0.66. The generated report ordering assigns reports with a higher probability of medication-related causality a higher rank and is significantly correlated to a perfect report ordering, with a Kendall's tau of 0.24 ( P = .002). Conclusion: Our models produced prioritized report orderings that enable FDA safety evaluators to focus on reports that are more likely to contain valuable medication-related adverse event information. Applying our models to all FDA adverse event reports has the potential to streamline the manual review process and greatly reduce reviewer workload."," *Adverse Drug Reaction Reporting Systems; Data Mining; Drug-Related Side Effects and Adverse Reactions; Logistic Models; Machine Learning; Models, Theoretical; Natural Language Processing; ROC Curve; *Support Vector Machine; United States; *United States Food and Drug Administration; supervised machine learning",-2,-1,
147,Lienemann,2017,Journal of medical Internet research,Methods for Coding Tobacco-Related Twitter Data: A Systematic Review,"BACKGROUND: As Twitter has grown in popularity to 313 million monthly active users, researchers have increasingly been using it as a data source for tobacco-related research. OBJECTIVE: The objective of this systematic review was to assess the methodological approaches of categorically coded tobacco Twitter data and make recommendations for future studies. METHODS: Data sources included PsycINFO, Web of Science, PubMed, ABI/INFORM, Communication Source, and Tobacco Regulatory Science. Searches were limited to peer-reviewed journals and conference proceedings in English from January 2006 to July 2016. The initial search identified 274 articles using a Twitter keyword and a tobacco keyword. One coder reviewed all abstracts and identified 27 articles that met the following inclusion criteria: (1) original research, (2) focused on tobacco or a tobacco product, (3) analyzed Twitter data, and (4) coded Twitter data categorically. One coder extracted data collection and coding methods. RESULTS: E-cigarettes were the most common type of Twitter data analyzed, followed by specific tobacco campaigns. The most prevalent data sources were Gnip and Twitter's Streaming application programming interface (API). The primary methods of coding were hand-coding and machine learning. The studies predominantly coded for relevance, sentiment, theme, user or account, and location of user. CONCLUSIONS: Standards for data collection and coding should be developed to be able to more easily compare and replicate tobacco-related Twitter results. Additional recommendations include the following: sample Twitter's databases multiple times, make a distinction between message attitude and emotional tone for sentiment, code images and URLs, and analyze user profiles. Being relatively novel and widely used among adolescents and black and Hispanic individuals, Twitter could provide a rich source of tobacco surveillance data among vulnerable populations.", Data Collection/*methods; Electronic Nicotine Delivery Systems/*statistics & numerical data; Humans; Smoking/*epidemiology; Social Marketing; *Social Media; Tobacco Products/*statistics & numerical data; *Internet; *review; *social marketing; *tobacco,-2,-2,
148,Malycha,2017,Systematic reviews,Variables associated with unplanned general adult ICU admission in hospitalised patients: protocol for a systematic review,"BACKGROUND: Failure to promptly identify deterioration in hospitalised patients is associated with delayed admission to intensive care units (ICUs) and poor outcomes. Existing vital sign-based Early Warning Score (EWS) algorithms do not have a sufficiently high positive predictive value to be used for automated activation of an ICU outreach team. Incorporating additional patient data might improve the predictive power of EWS algorithms; however, it is currently not known which patient data (or variables) are most predictive of ICU admission. We describe the protocol for a systematic review of variables associated with ICU admission. METHODS/DESIGN: MEDLINE, EMBASE, CINAHL and the Cochrane Library, including Cochrane Database of Systematic Reviews and the Cochrane Central Register of Controlled Trials (CENTRAL) will be searched for studies that assess the association of routinely recorded variables associated with subsequent unplanned ICU admission. Only studies involving adult patients admitted to general ICUs will be included. We will extract data relating to the statistical association between ICU admission and predictor variables, the quality of the studies and the generalisability of the findings. DISCUSSION: The results of this review will aid the development of future models which predict the risk of unplanned ICU admission. SYSTEMATIC REVIEW REGISTRATION: PROSPERO: CRD42015029617.", Adult; Algorithms; Humans; *Intensive Care Units; *Patient Admission; Research Design; Risk Assessment/*methods; Risk Factors; *Systematic Reviews as Topic,-2,-2,
149,Swartz,2017,International journal of medical informatics,Creation of a simple natural language processing tool to support an imaging utilization quality dashboard,"BACKGROUND: Testing for venous thromboembolism (VTE) is associated with cost and risk to patients (e.g. radiation). To assess the appropriateness of imaging utilization at the provider level, it is important to know that provider's diagnostic yield (percentage of tests positive for the diagnostic entity of interest). However, determining diagnostic yield typically requires either time-consuming, manual review of radiology reports or the use of complex and/or proprietary natural language processing software. OBJECTIVES: The objectives of this study were twofold: 1) to develop and implement a simple, user-configurable, and open-source natural language processing tool to classify radiology reports with high accuracy and 2) to use the results of the tool to design a provider-specific VTE imaging dashboard, consisting of both utilization rate and diagnostic yield. METHODS: Two physicians reviewed a training set of 400 lower extremity ultrasound (UTZ) and computed tomography pulmonary angiogram (CTPA) reports to understand the language used in VTE-positive and VTE-negative reports. The insights from this review informed the arguments to the five modifiable parameters of the NLP tool. A validation set of 2,000 studies was then independently classified by the reviewers and by the tool; the classifications were compared and the performance of the tool was calculated. RESULTS: The tool was highly accurate in classifying the presence and absence of VTE for both the UTZ (sensitivity 95.7%; 95% CI 91.5-99.8, specificity 100%; 95% CI 100-100) and CTPA reports (sensitivity 97.1%; 95% CI 94.3-99.9, specificity 98.6%; 95% CI 97.8-99.4). The diagnostic yield was then calculated at the individual provider level and the imaging dashboard was created. CONCLUSIONS: We have created a novel NLP tool designed for users without a background in computer programming, which has been used to classify venous thromboembolism reports with a high degree of accuracy. The tool is open-source and available for download at http://iturrate.com/simpleNLP. Results obtained using this tool can be applied to enhance quality by presenting information about utilization and yield to providers via an imaging dashboard."," Humans; Image Processing, Computer-Assisted/*standards; *Natural Language Processing; Tomography, X-Ray Computed/*methods; Venous Thromboembolism/*diagnostic imaging; *Automated text classification; *Benchmarking; *Deep venous thrombosis; *Practice variation; *Pulmonary embolism",-2,-2,
150,O'Shea,2017,International journal of medical informatics,Digital disease detection: A systematic review of event-based internet biosurveillance systems,"BACKGROUND: Internet access and usage has changed how people seek and report health information. Meanwhile,infectious diseases continue to threaten humanity. The analysis of Big Data, or vast digital data, presents an opportunity to improve disease surveillance and epidemic intelligence. Epidemic intelligence contains two components: indicator based and event-based. A relatively new surveillance type has emerged called event-based Internet biosurveillance systems. These systems use information on events impacting health from Internet sources, such as social media or news aggregates. These systems circumvent the limitations of traditional reporting systems by being inexpensive, transparent, and flexible. Yet, innovations and the functionality of these systems can change rapidly. AIM: To update the current state of knowledge on event-based Internet biosurveillance systems by identifying all systems, including current functionality, with hopes to aid decision makers with whether to incorporate new methods into comprehensive programmes of surveillance. METHODS: A systematic review was performed through PubMed, Scopus, and Google Scholar databases, while also including grey literature and other publication types. RESULTS: 50 event-based Internet systems were identified, including an extraction of 15 attributes for each system, described in 99 articles. Each system uses different innovative technology and data sources to gather data, process, and disseminate data to detect infectious disease outbreaks. CONCLUSIONS: The review emphasises the importance of using both formal and informal sources for timely and accurate infectious disease outbreak surveillance, cataloguing all event-based Internet biosurveillance systems. By doing so, future researchers will be able to use this review as a library for referencing systems, with hopes of learning, building, and expanding Internet-based surveillance systems. Event-based Internet biosurveillance should act as an extension of traditional systems, to be utilised as an additional, supplemental data source to have a more comprehensive estimate of disease burden."," *Biosurveillance; Communicable Diseases/*diagnosis; Databases, Factual; Disease Outbreaks/*statistics & numerical data; Humans; *Internet; *Disease surveillance; *Public health",-2,-2,
151,Cocos,2017,Journal of the American Medical Informatics Association : JAMIA,Deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in Twitter posts,"Objective: Social media is an important pharmacovigilance data source for adverse drug reaction (ADR) identification. Human review of social media data is infeasible due to data quantity, thus natural language processing techniques are necessary. Social media includes informal vocabulary and irregular grammar, which challenge natural language processing methods. Our objective is to develop a scalable, deep-learning approach that exceeds state-of-the-art ADR detection performance in social media. Materials and Methods: We developed a recurrent neural network (RNN) model that labels words in an input sequence with ADR membership tags. The only input features are word-embedding vectors, which can be formed through task-independent pretraining or during ADR detection training. Results: Our best-performing RNN model used pretrained word embeddings created from a large, non-domain-specific Twitter dataset. It achieved an approximate match F-measure of 0.755 for ADR identification on the dataset, compared to 0.631 for a baseline lexicon system and 0.65 for the state-of-the-art conditional random field model. Feature analysis indicated that semantic information in pretrained word embeddings boosted sensitivity and, combined with contextual awareness captured in the RNN, precision. Discussion: Our model required no task-specific feature engineering, suggesting generalizability to additional sequence-labeling tasks. Learning curve analysis showed that our model reached optimal performance with fewer training examples than the other models. Conclusion: ADR detection performance in social media is significantly improved by using a contextually aware model and word embeddings formed from large, unlabeled datasets. The approach reduces manual data-labeling requirements and is scalable to large social media datasets.", Drug-Related Side Effects and Adverse Reactions/*diagnosis; Humans; Machine Learning; *Natural Language Processing; *Neural Networks (Computer); *Pharmacovigilance; *Social Media; Twitter messaging; adverse drug reaction; natural language processing; neural networks (computer); social media,-1,-2,
152,Cahan,2017,Journal of medical Internet research,A Learning Health Care System Using Computer-Aided Diagnosis,"Physicians intuitively apply pattern recognition when evaluating a patient. Rational diagnosis making requires that clinical patterns be put in the context of disease prior probability, yet physicians often exhibit flawed probabilistic reasoning. Difficulties in making a diagnosis are reflected in the high rates of deadly and costly diagnostic errors. Introduced 6 decades ago, computerized diagnosis support systems are still not widely used by internists. These systems cannot efficiently recognize patterns and are unable to consider the base rate of potential diagnoses. We review the limitations of current computer-aided diagnosis support systems. We then portray future diagnosis support systems and provide a conceptual framework for their development. We argue for capturing physician knowledge using a novel knowledge representation model of the clinical picture. This model (based on structured patient presentation patterns) holds not only symptoms and signs but also their temporal and semantic interrelations. We call for the collection of crowdsourced, automatically deidentified, structured patient patterns as means to support distributed knowledge accumulation and maintenance. In this approach, each structured patient pattern adds to a self-growing and -maintaining knowledge base, sharing the experience of physicians worldwide. Besides supporting diagnosis by relating the symptoms and signs with the final diagnosis recorded, the collective pattern map can also provide disease base-rate estimates and real-time surveillance for early detection of outbreaks. We explain how health care in resource-limited settings can benefit from using this approach and how it can be applied to provide feedback-rich medical education for both students and practitioners."," Delivery of Health Care/*methods; Diagnosis, Computer-Assisted/*methods; Humans; *crowdsourcing; *decision support systems, clinical; *diagnosis support systems; *diagnosis, computer-assisted; *diagnostic errors; *knowledge bases; *knowledge management; *pattern recognition, automated; *structured knowledge representation",-2,-2,
153,Cotton,2017,BMC bioinformatics,flippant-An R package for the automated analysis of fluorescence-based scramblase assays,"BACKGROUND: The lipid scrambling activity of protein extracts and purified scramblases is typically measured using a fluorescence-based assay. While the assay has yielded insight into the scramblase activity in crude membrane preparations, functional validation of candidate scramblases, stoichiometry of scramblase complexes as well as ATP-dependence of flippases, data analysis in its context has remained a task involving many manual steps. RESULTS: With the extension package ""flippant"" to R, a free software environment for statistical computing and graphics, we introduce an integrated solution for the analysis and publication-grade graphical presentation of dithionite scramblase assays and demonstrate its utility in revisiting an originally manual analysis from the publication record, closely reproducing the reported results. CONCLUSIONS: ""flippant"" allows for quick, reproducible data analysis of scramblase activity assays and provides a platform for review, dissemination and extension of the strategies it employs.", Biochemistry/*methods; Fluorescence; Humans; *Lipids; Phospholipid Transfer Proteins/analysis/*metabolism; *Software; Dithionite scramblase assay; R; Scramblase,-2,-2,
154,Souri,2017,Systematic reviews,Identification of validated case definitions for chronic disease using electronic medical records: a systematic review protocol,"BACKGROUND: Primary care electronic medical record (EMR) data are being used for research, surveillance, and clinical monitoring. To broaden the reach and usability of EMR data, case definitions must be specified to identify and characterize important chronic conditions. The purpose of this study is to identify all case definitions for a set of chronic conditions that have been tested and validated in primary care EMR and EMR-linked data. This work will provide a reference list of case definitions, together with their performance metrics, and will identify gaps where new case definitions are needed. METHODS: We will consider a set of 40 chronic conditions, previously identified as potentially important for surveillance in a review of multimorbidity measures. We will perform a systematic search of the published literature to identify studies that describe case definitions for clinical conditions in EMR data and report the performance of these definitions. We will stratify our search by studies that use EMR data alone and those that use EMR-linked data. We will compare the performance of different definitions for the same conditions and explore the influence of data source, jurisdiction, and patient population. DISCUSSION: EMR data from primary care providers can be compiled and used for benefit by the healthcare system. Not only does this work have the potential to further develop disease surveillance and health knowledge, EMR surveillance systems can provide rapid feedback to participating physicians regarding their patients. Existing case definitions will serve as a starting point for the development and validation of new case definitions and will enable better surveillance, research, and practice feedback based on detailed clinical EMR data. SYSTEMATIC REVIEW REGISTRATION: PROSPERO CRD42016040020.", Chronic Disease/*epidemiology; *Electronic Health Records/statistics & numerical data; Humans; Reproducibility of Results; *Systematic Reviews as Topic; *Big data; *Case definitions; *Chronic disease; *Electronic medical record; *Systematic review,-2,-2,
155,Wang,2017,PloS one,Clinical evaluation of new automatic coronary-specific best cardiac phase selection algorithm for single-beat coronary CT angiography,"The aim of this study was to evaluate the workflow efficiency of a new automatic coronary-specific reconstruction technique (Smart Phase, GE Healthcare-SP) for selection of the best cardiac phase with least coronary motion when compared with expert manual selection (MS) of best phase in patients with high heart rate. A total of 46 patients with heart rates above 75 bpm who underwent single beat coronary computed tomography angiography (CCTA) were enrolled in this study. CCTA of all subjects were performed on a 256-detector row CT scanner (Revolution CT, GE Healthcare, Waukesha, Wisconsin, US). With the SP technique, the acquired phase range was automatically searched in 2% phase intervals during the reconstruction process to determine the optimal phase for coronary assessment, while for routine expert MS, reconstructions were performed at 5% intervals and a best phase was manually determined. The reconstruction and review times were recorded to measure the workflow efficiency for each method. Two reviewers subjectively assessed image quality for each coronary artery in the MS and SP reconstruction volumes using a 4-point grading scale. The average HR of the enrolled patients was 91.1+/-19.0bpm. A total of 204 vessels were assessed. The subjective image quality using SP was comparable to that of the MS, 1.45+/-0.85 vs 1.43+/-0.81 respectively (p = 0.88). The average time was 246 seconds for the manual best phase selection, and 98 seconds for the SP selection, resulting in average time saving of 148 seconds (60%) with use of the SP algorithm. The coronary specific automatic cardiac best phase selection technique (Smart Phase) improves clinical workflow in high heart rate patients and provides image quality comparable with manual cardiac best phase selection. Reconstruction of single-beat CCTA exams with SP can benefit the users with less experienced in CCTA image interpretation."," Adult; Aged; Algorithms; Computed Tomography Angiography/*methods; Coronary Angiography/*methods; Coronary Artery Disease/*diagnostic imaging; Coronary Vessels/*diagnostic imaging; Female; Heart Rate; Humans; Male; Middle Aged; Radiographic Image Enhancement/*methods; Radiographic Image Interpretation, Computer-Assisted/*methods",-2,-2,
156,Tusting,2017,PLoS medicine,Housing Improvements and Malaria Risk in Sub-Saharan Africa: A Multi-Country Analysis of Survey Data,"BACKGROUND: Improvements to housing may contribute to malaria control and elimination by reducing house entry by malaria vectors and thus exposure to biting. We tested the hypothesis that the odds of malaria infection are lower in modern, improved housing compared to traditional housing in sub-Saharan Africa (SSA). METHODS AND FINDINGS: We analysed 15 Demographic and Health Surveys (DHS) and 14 Malaria Indicator Surveys (MIS) conducted in 21 countries in SSA between 2008 and 2015 that measured malaria infection by microscopy or rapid diagnostic test (RDT). DHS/MIS surveys record whether houses are built with finished materials (e.g., metal) or rudimentary materials (e.g., thatch). This information was used to develop a binary housing quality variable where houses built using finished wall, roof, and floor materials were classified as ""modern"", and all other houses were classified as ""traditional"". Conditional logistic regression was used to determine the association between housing quality and prevalence of malaria infection in children aged 0-5 y, adjusting for age, gender, insecticide-treated net (ITN) use, indoor residual spraying, household wealth, and geographic cluster. Individual survey odds ratios (ORs) were combined to determine a summary OR using a random effects meta-analysis. Of 284,532 total children surveyed, 139,318 were tested for malaria infection using microscopy (n = 131,652) or RDT (n = 138,540). Within individual surveys, malaria prevalence measured by microscopy ranged from 0.4% (Madagascar 2011) to 45.5% (Burkina Faso 2010) among children living in modern houses and from 0.4% (The Gambia 2013) to 70.6% (Burkina Faso 2010) in traditional houses, and malaria prevalence measured by RDT ranged from 0.3% (Senegal 2013-2014) to 61.2% (Burkina Faso 2010) in modern houses and from 1.5% (The Gambia 2013) to 79.8% (Burkina Faso 2010) in traditional houses. Across all surveys, modern housing was associated with a 9% to 14% reduction in the odds of malaria infection (microscopy: adjusted OR 0.91, 95% CI 0.85-0.97, p = 0.003; RDT: adjusted OR 0.86, 95% CI 0.80-0.92, p < 0.001). This association was consistent regardless of ITN usage. As a comparison, the odds of malaria infection were 15% to 16% lower among ITN users versus non-users (microscopy: adjusted OR 0.84, 95% CI 0.79-0.90, p < 0.001; RDT: adjusted OR 0.85, 95% CI 0.80-0.90, p < 0.001). The main limitation of this study is that residual confounding by household wealth of the observed association between housing quality and malaria prevalence is possible, since the wealth index may not have fully captured differences in socioeconomic position; however, the use of multiple national surveys offers the advantage of a large sample size and the elimination of many biases typically associated with pooling observational data. CONCLUSIONS: Housing quality is an important risk factor for malaria infection across the spectrum of malaria endemicity in SSA, with a strength of association between housing quality and malaria similar to that observed between ITN use and malaria. Improved housing should be considered a promising intervention for malaria control and elimination and long-term prevention of reintroduction."," Africa South of the Sahara/epidemiology; Child, Preschool; Cross-Sectional Studies; Female; *Housing/standards; Humans; Infant; Infant, Newborn; Malaria/*epidemiology; Male; Risk Factors",-2,-2,
157,Marshall,2017,Systematic reviews,Documenting research with transgender and gender diverse people: protocol for an evidence map and thematic analysis,"BACKGROUND: There is limited information about how transgender, gender diverse, and Two-Spirit (trans) people have been represented and studied by researchers. The objectives of this study are to (1) map and describe trans research in the social sciences, sciences, humanities, health, education, and business, (2) identify evidence gaps and opportunities for more responsible research with trans people, (3) assess the use of text mining for study identification, and (4) increase access to trans research for key stakeholders through the creation of a web-based evidence map. METHODS: Study design was informed by community consultations and pilot searches. Eligibility criteria were established to include all original research of any design, including trans people or their health information, and published in English in peer-reviewed journals. A complex electronic search strategy based on relevant concepts in 15 databases was developed to obtain a broad range of results linked to transgender, gender diverse, and Two-Spirit individuals and communities. Searches conducted in early 2015 resulted in 25,242 references after removal of duplicates. Based on the number of references, resources, and an objective to capture upwards of 90% of the existing literature, this study is a good candidate for text mining using Latent Dirichlet Allocation to improve efficiency of the screening process. The following information will be collected for evidence mapping: study topic, study design, methods and data sources, recruitment strategies, sample size, sample demographics, researcher name and affiliation, country where research was conducted, funding source, and year of publication. DISCUSSION: The proposed research incorporates an extensive search strategy, text mining, and evidence map; it therefore has the potential to build on knowledge in several fields. Review results will increase awareness of existing trans research, identify evidence gaps, and inform strategic research prioritization. Publishing the map online will improve access to research for key stakeholders including community members, policy makers, and healthcare providers. This study will also contribute to knowledge in the area of text mining for study identification by providing an example of how semi-automation performs for screening on title and abstract and on full text.", Documentation/*methods; Humans; *Research Design; *Sexual and Gender Minorities; Transgender Persons; *Evidence map; *Gender diverse; *Research ethics; *Research prioritization; *Responsible research; *Text mining; *Transgender,2,1,
158,Rochefort,2017,BMC health services research,Accuracy and generalizability of using automated methods for identifying adverse events from electronic health record data: a validation study protocol,"BACKGROUND: Adverse events (AEs) in acute care hospitals are frequent and associated with significant morbidity, mortality, and costs. Measuring AEs is necessary for quality improvement and benchmarking purposes, but current detection methods lack in accuracy, efficiency, and generalizability. The growing availability of electronic health records (EHR) and the development of natural language processing techniques for encoding narrative data offer an opportunity to develop potentially better methods. The purpose of this study is to determine the accuracy and generalizability of using automated methods for detecting three high-incidence and high-impact AEs from EHR data: a) hospital-acquired pneumonia, b) ventilator-associated event and, c) central line-associated bloodstream infection. METHODS: This validation study will be conducted among medical, surgical and ICU patients admitted between 2013 and 2016 to the Centre hospitalier universitaire de Sherbrooke (CHUS) and the McGill University Health Centre (MUHC), which has both French and English sites. A random 60% sample of CHUS patients will be used for model development purposes (cohort 1, development set). Using a random sample of these patients, a reference standard assessment of their medical chart will be performed. Multivariate logistic regression and the area under the curve (AUC) will be employed to iteratively develop and optimize three automated AE detection models (i.e., one per AE of interest) using EHR data from the CHUS. These models will then be validated on a random sample of the remaining 40% of CHUS patients (cohort 1, internal validation set) using chart review to assess accuracy. The most accurate models developed and validated at the CHUS will then be applied to EHR data from a random sample of patients admitted to the MUHC French site (cohort 2) and English site (cohort 3)-a critical requirement given the use of narrative data -, and accuracy will be assessed using chart review. Generalizability will be determined by comparing AUCs from cohorts 2 and 3 to those from cohort 1. DISCUSSION: This study will likely produce more accurate and efficient measures of AEs. These measures could be used to assess the incidence rates of AEs, evaluate the success of preventive interventions, or benchmark performance across hospitals."," Catheterization, Central Venous/*adverse effects; Cross Infection/*epidemiology; Electronic Health Records/statistics & numerical data; Female; Hospitalization/statistics & numerical data; Hospitals; Humans; Incidence; Male; Natural Language Processing; Pneumonia/epidemiology; Quality Improvement; Respiration, Artificial/*adverse effects; *Acute care hospital; *Adverse events; *Automated detection; *Data warehouse; *Electronic health record; *Natural language processing; *Patient safety",-1,-2,
159,Liu,2017,PloS one,The association between body mass index and mortality among Asian peritoneal dialysis patients: A meta-analysis,"BACKGROUND: Previous studies have revealed that increased body mass index (BMI) is associated with decreased mortality among hemodialysis patients. However, few studies have dealt with the association between BMI and mortality among patients undergoing peritoneal dialysis (PD) and even fewer studies have focused on the Asian PD patients. The reported studies were often non-conclusive and some even yielded contradictory results. This paper, to our best knowledge, registers the first attempt to systematically review the current literature and summarize new results on the association between BMI and mortality among the Asian PD population. METHOD: A systematic literature review was performed in Medline and EMBASE to identify relevant cohort studies on all-cause and cardiovascular disease (CVD) mortality stratified by BMI categories tailored to Asians among the Asian PD population. We meta-analyzed individual results based on a random effect model, strictly complying with Preferred Reporting Items for Systematic Reviews and Meta-analysis. RESULTS: The paper reviews seven cohort studies with a total of 3,610 Asian PD patients. Obese group (BMI = 25-29.9 kg/m2) was associated with higher risk of all-cause mortality (HR = 1.46, 95%CI [1.07-1.98]; p = 0.02) and CVD mortality (HR = 2.01, 95%CI [1.14-3.54]; p = 0.02), compared to the normal group (BMI = 18.5-22.9 kg/m2). The underweight group (BMI<18.5kg/m2) was also associated with an elevated risk of all-cause mortality (HR = 2.11, 95%CI [1.46-3.07]; p<0.001). No significant associations between BMI with all-cause mortality were found among the overweight group (23-24.9 kg/m2) (HR = 1.00, 95%CI [0.76-1.32]; p = 0.9). The association between BMI and CVD mortality risk among the underweight and overweight groups was found nonsignificant (p = 0.5 and 0.6 respectively). CONCLUSION: Obesity is associated with increased mortality in Asian PD patients. The study indicates a ""V-shaped"" trend in the association between BMI and mortality in these patients.", *Body Mass Index; Humans; Obesity/*physiopathology; Peritoneal Dialysis/*mortality; Risk Factors; Survival Rate,-2,-2,
160,Cahan,2017,International journal of medical informatics,Computer-aided assessment of the generalizability of clinical trial results,"BACKGROUND: The effects of an intervention on patients from populations other than that included in a trial may vary as a result of differences in population features, treatment administration, or general setting. Determining the generalizability of a trial to a target population is important in clinical decision making at both the individual practitioner and policy-making levels. However, awareness to the challenges associated with the assessment of generalizability of trials is low and tools to facilitate such assessment are lacking. METHODS: We review the main factors affecting the generalizability of a clinical trial results beyond the trial population. We then propose a framework for a standardized evaluation of parameters relevant to determining the external validity of clinical trials to produce a ""generalizability score"". We then apply this framework to populations of patients with heart failure included in trials, cohorts and registries to demonstrate the use of the generalizability score and its graphic representation along three dimensions: participants' demographics, their clinical profile and intervention setting. We use the generalizability score to compare a single trial to multiple ""target"" clinical scenarios. Additionally, we present the generalizability score of several studies with regard to a single ""target"" population. RESULTS: Similarity indices vary considerably between trials and target population, but inconsistent reporting of participant characteristics limit head-to-head comparisons. CONCLUSION: We discuss the challenges involved in performing automatic assessment of trial generalizability at scale and propose the adoption of a standard format for reporting the characteristics of trial participants to enable better interpretation of their results.", Clinical Trials as Topic/*standards/*statistics & numerical data; *Computer-Aided Design; Humans; *Patient Selection; *Research Design; *Clinical trials; *Decision support; *External validity; *Generalizability; *Similarity assessment,-1,1,
161,Abbott,2017,Journal of the American Medical Informatics Association : JAMIA,Automatic health record review to help prioritize gravely ill Social Security disability applicants,"Objective: Every year, thousands of patients die waiting for disability benefits from the Social Security Administration. Some qualify for expedited service under the Compassionate Allowance (CAL) initiative, but CAL software focuses exclusively on information from a single form field. This paper describes the development of a supplemental process for identifying some overlooked but gravely ill applicants, through automatic annotation of health records accompanying new claims. We explore improved prioritization instead of fully autonomous claims approval. Materials and Methods: We developed a sample of claims containing medical records at the moment of arrival in a single office. A series of tools annotated both patient records and public Web page descriptions of CAL medical conditions. We trained random forests to identify CAL patients and validated each model with 10-fold cross validation. Results: Our main model, a general CAL classifier, had an area under the receiver operating characteristic curve of 0.915. Combining this classifier with existing software improved sensitivity from 0.960 to 0.994, detecting every deceased patient, but reducing positive predictive value to 0.216. Discussion: True positive CAL identification is a priority, given CAL patient mortality. Mere prioritization of the false positives would not create a meaningful burden in terms of manual review. Death certificate data suggest the presence of truly ill patients among putative false positives. Conclusion: To a limited extent, it is possible to identify gravely ill Social Security disability applicants by analyzing annotations of unstructured electronic health records, and the level of identification is sufficient to be useful in prioritizing case reviews."," *Critical Illness; Decision Trees; *Disability Evaluation; *Disabled Persons; *Electronic Health Records; Eligibility Determination/*methods; Humans; Information Storage and Retrieval/methods; Insurance, Disability; *Natural Language Processing; ROC Curve; *Social Security; United States; Social Security; disability; government; health records; natural language processing",-2,-2,
162,Tsai,2017,PloS one,Serum Uric Acid and Progression of Kidney Disease: A Longitudinal Analysis and Mini-Review,"BACKGROUND: Increasing evidence supports the association between hyperuricemia and incident chronic kidney disease (CKD); however, there are conflicting data regarding the role of hyperuricemia in the progression of CKD. This study retrospectively assessed the longitudinal association between uric acid (UA) level and CKD progression in a Chinese population lived in Taiwan. METHODS: Patients with physician diagnosis of hyperuricemia or receiving urate-lowering therapy between 2003 and 2005 were identified in the electronic medical records (EMR) of a tertiary medical center and were followed up until December 31, 2011. Patients were divided into four UA categories at the cut-off 6, 8, and 10 mg/dL. CKD progression was estimated by the change of estimated glomerular filtration rate (eGFR) in the linear mixed models. Kidney failure was defined as an eGFR less than 15 mL/min/1.73 m2 or requiring renal replacement therapy. RESULTS: A total of 739 patients were analyzed. In the full-adjusted model, patients with a baseline UA level >/=6 mg/dL had greater decline in eGFR ((beta = -9.6, 95% CI -16.1, -3.1), comparing to those with a UA level less than 6 mg/dL. When stratifying patients into four UA categories, all three hyperuricemia categories (UA6-8, 8-10, >/=10 mg/dL) associated with a greater decline in eGFR over the follow-up period with an increasing dose-response, comparing to the lowest UA category. The risk of progression to renal failure increased 7% (hazard ratio 1.07, 95% CI 1.00, 1.14) for each 1mg/dL increase in baseline UA level. The influences of hyperuricemia on eGFR decline and the risk of kidney failure were more prominent in patients without proteinuria than those with proteinuria. CONCLUSION: Our study showed a higher uric acid level is associated with a significant rapid decline in eGFR and a higher risk of kidney failure, particularly in patients without proteinuria. Our findings suggest hyperuricemia is a potential modifiable factor of CKD progression.", Cohort Studies; Disease Progression; Humans; Kidney Diseases/blood/*pathology; Longitudinal Studies; Retrospective Studies; Uric Acid/*blood,-2,-2,
163,Schiff,2017,Journal of the American Medical Informatics Association : JAMIA,Screening for medication errors using an outlier detection system,"Objective: The study objective was to evaluate the accuracy, validity, and clinical usefulness of medication error alerts generated by an alerting system using outlier detection screening. Materials and Methods: Five years of clinical data were extracted from an electronic health record system for 747 985 patients who had at least one visit during 2012-2013 at practices affiliated with 2 academic medical centers. Data were screened using the system to detect outliers suggestive of potential medication errors. A sample of 300 charts was selected for review from the 15 693 alerts generated. A coding system was developed and codes assigned based on chart review to reflect the accuracy, validity, and clinical value of the alerts. Results: Three-quarters of the chart-reviewed alerts generated by the screening system were found to be valid in which potential medication errors were identified. Of these valid alerts, the majority (75.0%) were found to be clinically useful in flagging potential medication errors or issues. Discussion: A clinical decision support (CDS) system that used a probabilistic, machine-learning approach based on statistically derived outliers to detect medication errors generated potentially useful alerts with a modest rate of false positives. The performance of such a surveillance and alerting system is critically dependent on the quality and completeness of the underlying data. Conclusion: The screening system was able to generate alerts that might otherwise be missed with existing CDS systems and did so with a reasonably high degree of alert usefulness when subjected to review of patients' clinical contexts and details."," *Decision Support Systems, Clinical; Humans; Machine Learning; *Medical Order Entry Systems; Medication Errors/*prevention & control; Outpatient Clinics, Hospital; *clinical decision support; *electronic health records; *machine learning; *medication alert systems; *patient safety",-2,-2,
164,Gu,2017,International journal of medical informatics,Visualizing the knowledge structure and evolution of big data research in healthcare informatics,"BACKGROUND: In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field. METHODS: To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers' production trends in the field and the trend of each paper's co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future. RESULTS: By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People's Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing). CONCLUSION: This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.", Bibliometrics; *Biomedical Research; Data Mining/*methods; *Databases as Topic; Humans; *Medical Informatics; *Bibliometrics; *Big data; *Healthcare informatics; *Knowledge management; *Knowledge structure,-1,2,
165,Maenner,2016,PloS one,Development of a Machine Learning Algorithm for the Surveillance of Autism Spectrum Disorder,"The Autism and Developmental Disabilities Monitoring (ADDM) Network conducts population-based surveillance of autism spectrum disorder (ASD) among 8-year old children in multiple US sites. To classify ASD, trained clinicians review developmental evaluations collected from multiple health and education sources to determine whether the child meets the ASD surveillance case criteria. The number of evaluations collected has dramatically increased since the year 2000, challenging the resources and timeliness of the surveillance system. We developed and evaluated a machine learning approach to classify case status in ADDM using words and phrases contained in children's developmental evaluations. We trained a random forest classifier using data from the 2008 Georgia ADDM site which included 1,162 children with 5,396 evaluations (601 children met ADDM ASD criteria using standard ADDM methods). The classifier used the words and phrases from the evaluations to predict ASD case status. We evaluated its performance on the 2010 Georgia ADDM surveillance data (1,450 children with 9,811 evaluations; 754 children met ADDM ASD criteria). We also estimated ASD prevalence using predictions from the classification algorithm. Overall, the machine learning approach predicted ASD case statuses that were 86.5% concordant with the clinician-determined case statuses (84.0% sensitivity, 89.4% predictive value positive). The area under the resulting receiver-operating characteristic curve was 0.932. Algorithm-derived ASD ""prevalence"" was 1.46% compared to the published (clinician-determined) estimate of 1.55%. Using only the text contained in developmental evaluations, a machine learning algorithm was able to discriminate between children that do and do not meet ASD surveillance criteria at one surveillance site.", *Algorithms; Area Under Curve; Autism Spectrum Disorder/classification/*diagnosis; Child; Female; Georgia/epidemiology; Humans; *Machine Learning; Male; Prevalence; ROC Curve; Sensitivity and Specificity,-1,-2,
166,Kal,2016,PloS one,Is Implicit Motor Learning Preserved after Stroke? A Systematic Review with Meta-Analysis,"Many stroke patients experience difficulty with performing dual-tasks. A promising intervention to target this issue is implicit motor learning, as it should enhance patients' automaticity of movement. Yet, although it is often thought that implicit motor learning is preserved post-stroke, evidence for this claim has not been systematically analysed yet. Therefore, we systematically reviewed whether implicit motor learning is preserved post-stroke, and whether patients benefit more from implicit than from explicit motor learning. We comprehensively searched conventional (MEDLINE, Cochrane, Embase, PEDro, PsycINFO) and grey literature databases (BIOSIS, Web of Science, OpenGrey, British Library, trial registries) for relevant reports. Two independent reviewers screened reports, extracted data, and performed a risk of bias assessment. Overall, we included 20 out of the 2177 identified reports that allow for a succinct evaluation of implicit motor learning. Of these, only 1 study investigated learning on a relatively complex, whole-body (balance board) task. All 19 other studies concerned variants of the serial-reaction time paradigm, with most of these focusing on learning with the unaffected hand (N = 13) rather than the affected hand or both hands (both: N = 4). Four of the 20 studies compared explicit and implicit motor learning post-stroke. Meta-analyses suggest that patients with stroke can learn implicitly with their unaffected side (mean difference (MD) = 69 ms, 95% CI[45.1, 92.9], p < .00001), but not with their affected side (standardized MD = -.11, 95% CI[-.45, .25], p = .56). Finally, implicit motor learning seemed equally effective as explicit motor learning post-stroke (SMD = -.54, 95% CI[-1.37, .29], p = .20). However, overall, the high risk of bias, small samples, and limited clinical relevance of most studies make it impossible to draw reliable conclusions regarding the effect of implicit motor learning strategies post-stroke. High quality studies with larger samples are warranted to test implicit motor learning in clinically relevant contexts.", Aged; Humans; Learning/*physiology; Middle Aged; Motor Skills/*physiology; Reaction Time; Recovery of Function; Stroke/*physiopathology/psychology; Stroke Rehabilitation,-2,-2,
167,Canela-Xandri,2016,PloS one,Improved Genetic Profiling of Anthropometric Traits Using a Big Data Approach,"Genome-wide association studies (GWAS) promised to translate their findings into clinically beneficial improvements of patient management by tailoring disease management to the individual through the prediction of disease risk. However, the ability to translate genetic findings from GWAS into predictive tools that are of clinical utility and which may inform clinical practice has, so far, been encouraging but limited. Here we propose to use a more powerful statistical approach, the use of which has traditionally been limited due to computational requirements and lack of sufficiently large individual level genotyped cohorts, but which improve the prediction of multiple medically relevant phenotypes using the same panel of SNPs. As a proof of principle, we used a shared panel of 319,038 common SNPs with MAF > 0.05 to train the prediction models in 114,264 unrelated White-British individuals for height and four obesity related traits (body mass index, basal metabolic rate, body fat percentage, and waist-to-hip ratio). We obtained prediction accuracies that ranged between 46% and 75% of the maximum achievable given the captured heritable component. For height, this represents an improvement in prediction accuracy of up to 68% (184% more phenotypic variance explained) over SNPs reported to be robustly associated with height in a previous GWAS meta-analysis of similar size. Across-population predictions in White non-British individuals were similar to those in White-British whilst those in Asian and Black individuals were informative but less accurate. We estimate that the genotyping of circa 500,000 unrelated individuals will yield predictions between 66% and 82% of the SNP-heritability captured by common variants in our array. Prediction accuracies did not improve when including rarer SNPs or when fitting multiple traits jointly in multivariate models."," Adiposity/*genetics; Anthropometry; Basal Metabolism/*genetics; *Body Mass Index; Female; Genetic Association Studies; Genetic Variation; Genotype; Humans; Male; Models, Genetic; Obesity/*genetics; Phenotype; *Polymorphism, Single Nucleotide; *Waist-Hip Ratio",-2,-2,
168,Gonzalez-Villa,2016,Artificial intelligence in medicine,A review on brain structures segmentation in magnetic resonance imaging,"BACKGROUND AND OBJECTIVES: Automatic brain structures segmentation in magnetic resonance images has been widely investigated in recent years with the goal of helping diagnosis and patient follow-up in different brain diseases. Here, we present a review of the state-of-the-art of automatic methods available in the literature ranging from structure specific segmentation methods to whole brain parcellation approaches. METHODS: We divide first the algorithms according to their target structures and then we propose a general classification based on their segmentation strategy, which includes atlas-based, learning-based, deformable, region-based and hybrid methods. We further discuss each category's strengths and weaknesses and analyze its performance in segmenting different brain structures providing a qualitative and quantitative comparison. RESULTS: We compare the results of the analyzed works for the following brain structures: hippocampus, thalamus, caudate nucleus, putamen, pallidum, amygdala, accumbens, lateral ventricles, and brainstem. The structures on which more works have focused on are the hippocampus and the caudate nucleus. In general, the accumbens (0.69 mean DSC) is the most difficult structure to segment whereas the structures that seem to get the best results are the brainstem, closely followed by the thalamus and the putamen with 0.88, 0.87 and 0.86 mean DSC, respectively. Atlas-based approaches achieve good results when segmenting the hippocampus (DSC between 0.75 and 0.90), thalamus (0.88-0.92) and lateral ventricles (0.83-0.93), while deformable methods perform good for caudate nucleus (0.84-0.91) and putamen segmentation (0.86-0.89). CONCLUSIONS: There is not yet a single automatic segmentation approach that can emerge as a standard for the clinical practice, providing accurate brain structures segmentation. Future trends need to focus on combining multi-atlas methods with learning-based or deformable approaches. Employing atlases to provide spatial robustness and modeling the structures appearance with supervised classifiers or Active Appearance Models could lead to improved segmentation results.", *Algorithms; Atlases as Topic; Brain/*diagnostic imaging; Hippocampus/diagnostic imaging; Humans; *Magnetic Resonance Imaging; *Automated segmentation methods; *Brain structures; *Review,-2,-2,
169,Vuokko,2017,International journal of medical informatics,Impacts of structuring the electronic health record: Results of a systematic literature review from the perspective of secondary use of patient data,"PURPOSE: To explore the impacts that structuring of electronic health records (EHRs) has had from the perspective of secondary use of patient data as reflected in currently published literature. This paper presents the results of a systematic literature review aimed at answering the following questions; (1) what are the common methods of structuring patient data to serve secondary use purposes; (2) what are the common methods of evaluating patient data structuring in the secondary use context, and (3) what impacts or outcomes of EHR structuring have been reported from the secondary use perspective. METHODS: The reported study forms part of a wider systematic literature review on the impacts of EHR structuring methods and evaluations of their impact. The review was based on a 12-step systematic review protocol adapted from the Cochrane methodology. Original articles included in the study were divided into three groups for analysis and reporting based on their use focus: nursing documentation, medical use and secondary use (presented in this paper). The analysis from the perspective of secondary use of data includes 85 original articles from 1975 to 2010 retrieved from 15 bibliographic databases. RESULTS: The implementation of structured EHRs can be roughly divided into applications for documenting patient data at the point of care and application for retrieval of patient data (post hoc structuring). Two thirds of the secondary use articles concern EHR structuring methods which were still under development or in the testing phase. METHODS: of structuring patient data such as codes, terminologies, reference information models, forms or templates and documentation standards were usually applied in combination. Most of the identified benefits of utilizing structured EHR data for secondary use purposes concentrated on information content and quality or on technical quality and reliability, particularly in the case of Natural Language Processing (NLP) studies. A few individual articles evaluated impacts on care processes, productivity and costs, patient safety, care quality or other health impacts. In most articles these endpoints were usually discussed as goals of secondary use and less as evidence-supported impacts, resulting from the use of structured EHR data for secondary purposes. CONCLUSIONS: Further studies and more sound evaluation methods are needed for evidence on how EHRs are utilized for secondary purposes, and how structured documentation methods can serve different users' needs, e.g. administration, statistics and research and development, in parallel to medical use purposes.", *Documentation; Electronic Health Records/*organization & administration/standards/*statistics &; numerical data; Humans; Information Storage and Retrieval/*standards; Meaningful Use; Quality of Health Care; *Electronic health records; *Free text; *Secondary use of patient data; *Structured data; *Systematic literature review,-2,-2,
170,Kadi,2017,International journal of medical informatics,Knowledge discovery in cardiology: A systematic literature review,"CONTEXT: Data mining (DM) provides the methodology and technology needed to transform huge amounts of data into useful information for decision making. It is a powerful process employed to extract knowledge and discover new patterns embedded in large data sets. Data mining has been increasingly used in medicine, particularly in cardiology. In fact, DM applications can greatly benefit all those involved in cardiology, such as patients, cardiologists and nurses. OBJECTIVE: The purpose of this paper is to review papers concerning the application of DM techniques in cardiology so as to summarize and analyze evidence regarding: (1) the DM techniques most frequently used in cardiology; (2) the performance of DM models in cardiology; (3) comparisons of the performance of different DM models in cardiology. METHOD: We performed a systematic literature review of empirical studies on the application of DM techniques in cardiology published in the period between 1 January 2000 and 31 December 2015. RESULTS: A total of 149 articles published between 2000 and 2015 were selected, studied and analyzed according to the following criteria: DM techniques and performance of the approaches developed. The results obtained showed that a significant number of the studies selected used classification and prediction techniques when developing DM models. Neural networks, decision trees and support vector machines were identified as being the techniques most frequently employed when developing DM models in cardiology. Moreover, neural networks and support vector machines achieved the highest accuracy rates and were proved to be more efficient than other techniques."," Cardiology/*education; Data Mining/*methods; Decision Trees; Models, Theoretical; Neural Networks (Computer); Support Vector Machine; *Cardiology; *Data mining; *Knowledge extraction; *Medical tasks",-1,2,
171,Gogovor,2017,International journal of medical informatics,Informing the development of an Internet-based chronic pain self-management program,"BACKGROUND: Self-management can optimize health outcomes for individuals with chronic pain (CP), an increasing fiscal and social burden in Canada. However, self-management is rarely integrated into the regular care (team activities and medical treatment) patients receive. Health information technology offers an opportunity to provide regular monitoring and exchange of information between patient and care team. OBJECTIVE: To identify information needs and gaps in chronic pain management as well as technology features to inform the development of an Internet-based self-management program. METHODS: Two methods were used. First was a structured literature review: electronic databases were searched up to 2015 with combinations of MeSH terms and text-words such as chronic pain, self-management, self-efficacy, technology, Internet-based, patient portal, and e-health. A narrative synthesis of the characteristics and content of Internet-based pain management programs emerging from the literature review and how they relate to gaps in chronic pain management were completed. Second, four audiotaped focus group sessions were conducted with individuals with chronic pain and caregivers (n=9) and health professionals (n=7) recruited from three multidisciplinary tertiary and rehabilitation centres. A thematic analysis of the focus group transcripts was conducted. RESULTS: Thirty-nine primary articles related to 20 patient-oriented Internet-based programs were selected. Gaps in CP management included lack of knowledge, limited access to health care, suboptimal care, and lack of self-management support. Overall, 14 themes related to information needs and gaps in care were identified by both health professionals and patients, three were exclusive to patients and five to health professionals. Common themes from the focus groups included patient education on chronic pain care, attitude-belief-culture, financial and legal issues, end-of-program crash, and motivational content. CONCLUSIONS: Internet-based programs contain automated, communication and decision support features that can address information and care gaps reported by patients and clinicians. However, focus groups identified functionalities not reported in the literature, non-medical and condition- and context-specific information, integration of personal health records, and the role of the different health professionals in chronic pain management were not identified. These gaps need to be considered in the future development of Internet-based programs. While the association between the mechanisms of Internet-based programs' features and outcomes is not clearly established, the results of this study indicate that interactivity, personalization and tailored messages, combined with therapist contact will maximize the effectiveness of an Internet-based chronic pain program in enhancing self-management.", Adult; Aged; Attitude; Canada; Caregivers; Chronic Pain/*therapy; Female; Focus Groups; Humans; *Internet; Male; Middle Aged; *Pain Management; Patient Education as Topic/methods; *Self Care; *Chronic pain; *Internet-based program; *Self-management; *e-health,-2,-2,
172,Ouzzani,2016,Systematic reviews,Rayyan-a web and mobile app for systematic reviews,"BACKGROUND: Synthesis of multiple randomized controlled trials (RCTs) in a systematic review can summarize the effects of individual outcomes and provide numerical answers about the effectiveness of interventions. Filtering of searches is time consuming, and no single method fulfills the principal requirements of speed with accuracy. Automation of systematic reviews is driven by a necessity to expedite the availability of current best evidence for policy and clinical decision-making. We developed Rayyan ( http://rayyan.qcri.org ), a free web and mobile app, that helps expedite the initial screening of abstracts and titles using a process of semi-automation while incorporating a high level of usability. For the beta testing phase, we used two published Cochrane reviews in which included studies had been selected manually. Their searches, with 1030 records and 273 records, were uploaded to Rayyan. Different features of Rayyan were tested using these two reviews. We also conducted a survey of Rayyan's users and collected feedback through a built-in feature. RESULTS: Pilot testing of Rayyan focused on usability, accuracy against manual methods, and the added value of the prediction feature. The ""taster"" review (273 records) allowed a quick overview of Rayyan for early comments on usability. The second review (1030 records) required several iterations to identify the previously identified 11 trials. The ""suggestions"" and ""hints,"" based on the ""prediction model,"" appeared as testing progressed beyond five included studies. Post rollout user experiences and a reflexive response by the developers enabled real-time modifications and improvements. The survey respondents reported 40% average time savings when using Rayyan compared to others tools, with 34% of the respondents reporting more than 50% time savings. In addition, around 75% of the respondents mentioned that screening and labeling studies as well as collaborating on reviews to be the two most important features of Rayyan. As of November 2016, Rayyan users exceed 2000 from over 60 countries conducting hundreds of reviews totaling more than 1.6M citations. Feedback from users, obtained mostly through the app web site and a recent survey, has highlighted the ease in exploration of searches, the time saved, and simplicity in sharing and comparing include-exclude decisions. The strongest features of the app, identified and reported in user feedback, were its ability to help in screening and collaboration as well as the time savings it affords to users. CONCLUSIONS: Rayyan is responsive and intuitive in use with significant potential to lighten the load of reviewers.", Feedback; Humans; *Internet; *Mobile Applications/standards; Randomized Controlled Trials as Topic; *Research Design; *Review Literature as Topic; Time Factors; *Automation; *Evidence-based medicine; *Systematic reviews,2,2,
173,Salamon,2016,PloS one,Towards the Automatic Classification of Avian Flight Calls for Bioacoustic Monitoring,"Automatic classification of animal vocalizations has great potential to enhance the monitoring of species movements and behaviors. This is particularly true for monitoring nocturnal bird migration, where automated classification of migrants' flight calls could yield new biological insights and conservation applications for birds that vocalize during migration. In this paper we investigate the automatic classification of bird species from flight calls, and in particular the relationship between two different problem formulations commonly found in the literature: classifying a short clip containing one of a fixed set of known species (N-class problem) and the continuous monitoring problem, the latter of which is relevant to migration monitoring. We implemented a state-of-the-art audio classification model based on unsupervised feature learning and evaluated it on three novel datasets, one for studying the N-class problem including over 5000 flight calls from 43 different species, and two realistic datasets for studying the monitoring scenario comprising hundreds of thousands of audio clips that were compiled by means of remote acoustic sensors deployed in the field during two migration seasons. We show that the model achieves high accuracy when classifying a clip to one of N known species, even for a large number of species. In contrast, the model does not perform as well in the continuous monitoring case. Through a detailed error analysis (that included full expert review of false positives and negatives) we show the model is confounded by varying background noise conditions and previously unseen vocalizations. We also show that the model needs to be parameterized and benchmarked differently for the continuous monitoring scenario. Finally, we show that despite the reduced performance, given the right conditions the model can still characterize the migration pattern of a specific species. The paper concludes with directions for future research."," Animal Migration; Animals; Area Under Curve; Automation; Birds/*classification/physiology; Flight, Animal/*physiology; ROC Curve; Seasons; Tape Recording; Vocalization, Animal",-2,-2,
174,Ivliev,2016,PloS one,Drug Repositioning through Systematic Mining of Gene Coexpression Networks in Cancer,"Gene coexpression network analysis is a powerful ""data-driven"" approach essential for understanding cancer biology and mechanisms of tumor development. Yet, despite the completion of thousands of studies on cancer gene expression, there have been few attempts to normalize and integrate co-expression data from scattered sources in a concise ""meta-analysis"" framework. We generated such a resource by exploring gene coexpression networks in 82 microarray datasets from 9 major human cancer types. The analysis was conducted using an elaborate weighted gene coexpression network (WGCNA) methodology and identified over 3,000 robust gene coexpression modules. The modules covered a range of known tumor features, such as proliferation, extracellular matrix remodeling, hypoxia, inflammation, angiogenesis, tumor differentiation programs, specific signaling pathways, genomic alterations, and biomarkers of individual tumor subtypes. To prioritize genes with respect to those tumor features, we ranked genes within each module by connectivity, leading to identification of module-specific functionally prominent hub genes. To showcase the utility of this network information, we positioned known cancer drug targets within the coexpression networks and predicted that Anakinra, an anti-rheumatoid therapeutic agent, may be promising for development in colorectal cancer. We offer a comprehensive, normalized and well documented collection of >3000 gene coexpression modules in a variety of cancers as a rich data resource to facilitate further progress in cancer research."," Biomarkers, Tumor/genetics; Cell Differentiation/genetics; Cell Proliferation/genetics; Data Mining/methods; Drug Repositioning/methods; Gene Expression/*genetics; Gene Expression Profiling/methods; Gene Regulatory Networks/*genetics; Genomics/methods; Humans; Hypoxia/genetics; Inflammation/genetics; Neoplasms/*genetics; Signal Transduction/genetics",-2,-2,
175,,,,,,,,-2,
176,Wallace,2016,Journal of machine learning research : JMLR,Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision,"Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method - supervised distant supervision (SDS) - that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.", Evidence-based medicine; data extraction; distant supervision; natural language processing; text mining,2,2,
177,Yli-Huumo,2016,PloS one,Where Is Current Research on Blockchain Technology?-A Systematic Review,"Blockchain is a decentralized transaction and data management technology developed first for Bitcoin cryptocurrency. The interest in Blockchain technology has been increasing since the idea was coined in 2008. The reason for the interest in Blockchain is its central attributes that provide security, anonymity and data integrity without any third party organization in control of the transactions, and therefore it creates interesting research areas, especially from the perspective of technical challenges and limitations. In this research, we have conducted a systematic mapping study with the goal of collecting all relevant research on Blockchain technology. Our objective is to understand the current research topics, challenges and future directions regarding Blockchain technology from the technical perspective. We have extracted 41 primary papers from scientific databases. The results show that focus in over 80% of the papers is on Bitcoin system and less than 20% deals with other Blockchain applications including e.g. smart contracts and licensing. The majority of research is focusing on revealing and improving limitations of Blockchain from privacy and security perspectives, but many of the proposed solutions lack concrete evaluation on their effectiveness. Many other Blockchain scalability related challenges including throughput and latency have been left unstudied. On the basis of this study, recommendations on future research directions are provided for researchers.", Cost-Benefit Analysis; Data Mining/economics/*trends; Humans; Research/*trends; Technology/*economics/trends,-2,-2,
178,Nieto,2016,PloS one,An Empirical Biomarker-Based Calculator for Cystic Index in a Model of Autosomal Recessive Polycystic Kidney Disease-The Nieto-Narayan Formula,"Autosomal recessive polycystic kidney disease (ARPKD) is associated with progressive enlargement of the kidneys fuelled by the formation and expansion of fluid-filled cysts. The disease is congenital and children that do not succumb to it during the neonatal period will, by age 10 years, more often than not, require nephrectomy+renal replacement therapy for management of both pain and renal insufficiency. Since increasing cystic index (CI; percent of kidney occupied by cysts) drives both renal expansion and organ dysfunction, management of these patients, including decisions such as elective nephrectomy and prioritization on the transplant waitlist, could clearly benefit from serial determination of CI. So also, clinical trials in ARPKD evaluating the efficacy of novel drug candidates could benefit from serial determination of CI. Although ultrasound is currently the imaging modality of choice for diagnosis of ARPKD, its utilization for assessing disease progression is highly limited. Magnetic resonance imaging or computed tomography, although more reliable for determination of CI, are expensive, time-consuming and somewhat impractical in the pediatric population. Using a well-established mammalian model of ARPKD, we undertook a big data-like analysis of minimally- or non-invasive blood and urine biomarkers of renal injury/dysfunction to derive a family of equations for estimating CI. We then applied a signal averaging protocol to distill these equations to a single empirical formula for calculation of CI. Such a formula will eventually find use in identifying and monitoring patients at high risk for progressing to end-stage renal disease and aid in the conduct of clinical trials."," Animals; *Biomarkers/blood/urine; Blood Urea Nitrogen; Child; Creatinine/blood; Cystatin C/blood; Cysts/pathology; Hepatitis A Virus Cellular Receptor 1/blood; Humans; Interleukin-18/blood; Kidney/diagnostic imaging/metabolism/physiopathology; Kidney Transplantation; Lipocalin-2/blood; Magnetic Resonance Imaging; Mice; Polycystic Kidney, Autosomal Recessive/*blood/diagnostic imaging/pathology/*urine; Rats; Renal Insufficiency/*blood/pathology/*urine; Severity of Illness Index; Ultrasonography",-2,-2,
179,Pawloski,2017,Journal of the American Medical Informatics Association : JAMIA,Predicting neutropenia risk in patients with cancer using electronic data,"Objectives: Clinical guidelines recommending the use of myeloid growth factors are largely based on the prescribed chemotherapy regimen. The guidelines suggest that oncologists consider patient-specific characteristics when prescribing granulocyte-colony stimulating factor (G-CSF) prophylaxis; however, a mechanism to quantify individual patient risk is lacking. Readily available electronic health record (EHR) data can provide patient-specific information needed for individualized neutropenia risk estimation. An evidence-based, individualized neutropenia risk estimation algorithm has been developed. This study evaluated the automated extraction of EHR chemotherapy treatment data and externally validated the neutropenia risk prediction model. Materials and Methods: A retrospective cohort of adult patients with newly diagnosed breast, colorectal, lung, lymphoid, or ovarian cancer who received the first cycle of a cytotoxic chemotherapy regimen from 2008 to 2013 were recruited from a single cancer clinic. Electronically extracted EHR chemotherapy treatment data were validated by chart review. Neutropenia risk stratification was conducted and risk model performance was assessed using calibration and discrimination. Results: Chemotherapy treatment data electronically extracted from the EHR were verified by chart review. The neutropenia risk prediction tool classified 126 patients (57%) as being low risk for febrile neutropenia, 44 (20%) as intermediate risk, and 51 (23%) as high risk. The model was well calibrated (Hosmer-Lemeshow goodness-of-fit test = 0.24). Discrimination was adequate and slightly less than in the original internal validation (c-statistic 0.75 vs 0.81). Conclusion: Chemotherapy treatment data were electronically extracted from the EHR successfully. The individualized neutropenia risk prediction model performed well in our retrospective external cohort.", Aged; *Algorithms; Antineoplastic Combined Chemotherapy Protocols/*adverse effects; *Electronic Health Records; Female; Granulocyte Colony-Stimulating Factor/therapeutic use; Humans; Information Storage and Retrieval; Logistic Models; Male; Middle Aged; Neoplasms/*complications/drug therapy; Neutropenia/*chemically induced; ROC Curve; Retrospective Studies; Risk Assessment/*methods; chemotherapy; clinical decision support systems; computer-based decision support; febrile neutropenia; granulocyte-colony stimulating factor; risk model,-2,-2,
180,Cristea,2016,PloS one,The Effectiveness of Cognitive Bias Modification Interventions for Substance Addictions: A Meta-Analysis,"BACKGROUND AND AIMS: Cognitive bias modification (CBM) interventions, presumably targeting automatic processes, are considered particularly promising for addictions. We conducted a meta-analysis examining randomized controlled trials (RCTs) of CBM for substance addiction outcomes. METHODS: Studies were identified through systematic searches in bibliographical databases. We included RCTs of CBM interventions, alone or in combination with other treatments, for any type of addiction. We examined trial risk of bias, publication bias and possible moderators. Effects sizes were computed for post-test and follow-up, using a random-effects model. We grouped outcome measures and reported results for addiction (all related measures), craving and cognitive bias. RESULTS: We identified 25 trials, 18 for alcohol problems, and 7 for smoking. At post-test, there was no significant effect of CBM for addiction, g = 0.08 (95% CI -0.02 to 0.18) or craving, g = 0.05 (95% CI -0.06 to 0.16), but there was a significant, moderate effect on cognitive bias, g = 0.60 (95% CI 0.39 to 0.79). Results were similar for alcohol and smoking outcomes taken separately. Follow-up addiction outcomes were reported in 7 trials, resulting in a small but significant effect of CBM, g = 0.18 (95% CI 0.03 to 0.32). Results for addiction and craving did not differ by substance type, sample type, delivery setting, bias targeted or number of sessions. Risk of bias was high or uncertain in most trials, for most criteria considered. Meta-regression analyses revealed significant inverse relationships between risk of bias and effect sizes for addiction outcomes and craving. The relationship between cognitive bias and respectively addiction ESs was not significant. There was consistent evidence of publication bias in the form of funnel plot asymmetry. CONCLUSIONS: Our results cast serious doubts on the clinical utility of CBM interventions for addiction problems, but sounder methodological trials are necessary before this issue can be settled. We found no indication that positive effects on biases translate into effects on addiction outcomes.", Bias; *Cognitive Behavioral Therapy; Female; Humans; Male; Outcome Assessment (Health Care); Substance-Related Disorders/*therapy,-2,-2,
181,Brown,2017,Journal of the American Medical Informatics Association : JAMIA,A systematic review of the types and causes of prescribing errors generated from using computerized provider order entry systems in primary and secondary care,"Objective: To understand the different types and causes of prescribing errors associated with computerized provider order entry (CPOE) systems, and recommend improvements in these systems. Materials and Methods: We conducted a systematic review of the literature published between January 2004 and June 2015 using three large databases: the Cumulative Index to Nursing and Allied Health Literature, Embase, and Medline. Studies that reported qualitative data about the types and causes of these errors were included. A narrative synthesis of all eligible studies was undertaken. Results: A total of 1185 publications were identified, of which 34 were included in the review. We identified 8 key themes associated with CPOE-related prescribing errors: computer screen display, drop-down menus and auto-population, wording, default settings, nonintuitive or inflexible ordering, repeat prescriptions and automated processes, users' work processes, and clinical decision support systems. Displaying an incomplete list of a patient's medications on the computer screen often contributed to prescribing errors. Lack of system flexibility resulted in users employing error-prone workarounds, such as the addition of contradictory free-text comments. Users' misinterpretations of how text was presented in CPOE systems were also linked with the occurrence of prescribing errors. Discussion and Conclusions: Human factors design is important to reduce error rates. Drop-down menus should be designed with safeguards to decrease the likelihood of selection errors. Development of more sophisticated clinical decision support, which can perform checks on free-text, may also prevent errors. Further research is needed to ensure that systems minimize error likelihood and meet users' workflow expectations.", Drug Prescriptions; Ergonomics; Humans; *Medical Order Entry Systems; *Medication Errors/prevention & control; alerts; clinical decision support; computerized provider order entry; decision-making; medication errors; patient safety,-2,-2,
182,Varghese,2016,Studies in health technology and informatics,Key Data Elements in Myeloid Leukemia,"Data standards consisting of key data elements for clinical routine and trial documentation harmonize documentation within and across different health care institutions making documentation more efficient and improving scientific data analysis. This work focusses on the field of myeloid leukemia (ML), where a semantic core of common data elements (CDEs) in routine and trial documentation is established by automatic UMLS-based form analysis of existing documentation models. These CDEs (n = 227) were initially reviewed and commented by leukemia experts before they were systematically surveyed by an international voting process through seven hematologists of four countries. The total agreement score was 86%. 116 elements (51%) of these share an agreement score of 100%. This work generated CDEs with language-independent semantic codes and international clinical expert review to build a first approach towards an international data standard for ML. A first version of the CDE list is implemented in the data standard Operational Data Model and additional other data formats for reuse in different medical information systems."," Clinical Trials as Topic; Data Collection; Documentation/*standards; Humans; *Leukemia, Myeloid; Semantics",1,-2,
183,Chen,2017,Journal of the American Medical Informatics Association : JAMIA,Identifying collaborative care teams through electronic medical record utilization patterns,"Objective: The goal of this investigation was to determine whether automated approaches can learn patient-oriented care teams via utilization of an electronic medical record (EMR) system. Materials and Methods: To perform this investigation, we designed a data-mining framework that relies on a combination of latent topic modeling and network analysis to infer patterns of collaborative teams. We applied the framework to the EMR utilization records of over 10 000 employees and 17 000 inpatients at a large academic medical center during a 4-month window in 2010. Next, we conducted an extrinsic evaluation of the patterns to determine the plausibility of the inferred care teams via surveys with knowledgeable experts. Finally, we conducted an intrinsic evaluation to contextualize each team in terms of collaboration strength (via a cluster coefficient) and clinical credibility (via associations between teams and patient comorbidities). Results: The framework discovered 34 collaborative care teams, 27 (79.4%) of which were confirmed as administratively plausible. Of those, 26 teams depicted strong collaborations, with a cluster coefficient > 0.5. There were 119 diagnostic conditions associated with 34 care teams. Additionally, to provide clarity on how the survey respondents arrived at their determinations, we worked with several oncologists to develop an illustrative example of how a certain team functions in cancer care. Discussion: Inferred collaborative teams are plausible; translating such patterns into optimized collaborative care will require administrative review and integration with management practices. Conclusions: EMR utilization records can be mined for collaborative care patterns in large complex medical centers.", Cooperative Behavior; *Data Mining; Electronic Health Records/*statistics & numerical data; Humans; Interprofessional Relations; *Patient Care Team; Patient-Centered Care; collaborative networks; data mining; electronic medical records; health care organization modeling,-2,-2,
184,Shemilt,2016,Systematic reviews,Use of cost-effectiveness analysis to compare the efficiency of study identification methods in systematic reviews,"BACKGROUND: Meta-research studies investigating methods, systems, and processes designed to improve the efficiency of systematic review workflows can contribute to building an evidence base that can help to increase value and reduce waste in research. This study demonstrates the use of an economic evaluation framework to compare the costs and effects of four variant approaches to identifying eligible studies for consideration in systematic reviews. METHODS: A cost-effectiveness analysis was conducted using a basic decision-analytic model, to compare the relative efficiency of 'safety first', 'double screening', 'single screening' and 'single screening with text mining' approaches in the title-abstract screening stage of a 'case study' systematic review about undergraduate medical education in UK general practice settings. Incremental cost-effectiveness ratios (ICERs) were calculated as the 'incremental cost per citation 'saved' from inappropriate exclusion' from the review. Resource use and effect parameters were estimated based on retrospective analysis of 'review process' meta-data curated alongside the 'case study' review, in conjunction with retrospective simulation studies to model the integrated use of text mining. Unit cost parameters were estimated based on the 'case study' review's project budget. A base case analysis was conducted, with deterministic sensitivity analyses to investigate the impact of variations in values of key parameters. RESULTS: Use of 'single screening with text mining' would have resulted in title-abstract screening workload reductions (base case analysis) of >60 % compared with other approaches. Across modelled scenarios, the 'safety first' approach was, consistently, equally effective and less costly than conventional 'double screening'. Compared with 'single screening with text mining', estimated ICERs for the two non-dominated approaches (base case analyses) ranged from pound1975 ('single screening' without a 'provisionally included' code) to pound4427 ('safety first' with a 'provisionally included' code) per citation 'saved'. Patterns of results were consistent between base case and sensitivity analyses. CONCLUSIONS: Alternatives to the conventional 'double screening' approach, integrating text mining, warrant further consideration as potentially more efficient approaches to identifying eligible studies for systematic reviews. Comparable economic evaluations conducted using other systematic review datasets are needed to determine the generalisability of these findings and to build an evidence base to inform guidance for review authors.", *Cost-Benefit Analysis; Data Mining/*methods; Humans; Patient Safety; *Research Design,1,-2,
185,Meghji,2016,PloS one,A Systematic Review of the Prevalence and Pattern of Imaging Defined Post-TB Lung Disease,"BACKGROUND: Tuberculosis is an important risk factor for chronic respiratory disease in resource poor settings. The persistence of abnormal spirometry and symptoms after treatment are well described, but the structural abnormalities underlying these changes remain poorly defined, limiting our ability to phenotype post-TB lung disease in to meaningful categories for clinical management, prognostication, and ongoing research. The relationship between post-TB lung damage and patient-centred outcomes including functional impairment, respiratory symptoms, and health related quality of life also remains unclear. METHODS: We performed a systematic literature review to determine the prevalence and pattern of imaging-defined lung pathology in adults after medical treatment for pleural, miliary, or pulmonary TB disease. Data were collected on study characteristics, and the modality, timing, and findings of thoracic imaging. The proportion of studies relating imaging findings to spirometry results and patient morbidity was recorded. Study quality was assessed using a modified Newcastle-Ottowa score. (Prospero Registration number CRD42015027958). RESULTS: We identified 37 eligible studies. The principle features seen on CXR were cavitation (8.3-83.7%), bronchiectasis (4.3-11.2%), and fibrosis (25.0-70.4%), but prevalence was highly variable. CT imaging identified a wider range of residual abnormalities than CXR, including nodules (25.0-55.8%), consolidation (3.7-19.2%), and emphysema (15.0-45.0%). The prevalence of cavitation was generally lower (7.4-34.6%) and bronchiectasis higher (35.0-86.0%) on CT vs. CXR imaging. A paucity of prospective data, and data from HIV-infected adults and sub-Saharan Africa (sSA) was noted. Few studies related structural damage to physiological impairment, respiratory symptoms, or patient morbidity. CONCLUSIONS: Post-TB structural lung pathology is common. Prospective data are required to determine the evolution of this lung damage and its associated morbidity over time. Further data are required from HIV-infected groups and those living in sSA."," Humans; Image Processing, Computer-Assisted/*methods; Pattern Recognition, Automated/*methods; Prevalence; Tomography, X-Ray Computed/*methods; Tuberculosis, Pulmonary/*diagnostic imaging/*epidemiology/pathology; United Kingdom/epidemiology",-2,-2,
186,Lyell,2017,Journal of the American Medical Informatics Association : JAMIA,Automation bias and verification complexity: a systematic review,"Introduction: While potentially reducing decision errors, decision support systems can introduce new types of errors. Automation bias (AB) happens when users become overreliant on decision support, which reduces vigilance in information seeking and processing. Most research originates from the human factors literature, where the prevailing view is that AB occurs only in multitasking environments. Objectives: This review seeks to compare the human factors and health care literature, focusing on the apparent association of AB with multitasking and task complexity. Data sources: EMBASE, Medline, Compendex, Inspec, IEEE Xplore, Scopus, Web of Science, PsycINFO, and Business Source Premiere from 1983 to 2015. Study selection: Evaluation studies where task execution was assisted by automation and resulted in errors were included. Participants needed to be able to verify automation correctness and perform the task manually. Methods: Tasks were identified and grouped. Task and automation type and presence of multitasking were noted. Each task was rated for its verification complexity. Results: Of 890 papers identified, 40 met the inclusion criteria; 6 were in health care. Contrary to the prevailing human factors view, AB was found in single tasks, typically involving diagnosis rather than monitoring, and with high verification complexity. Limitations: The literature is fragmented, with large discrepancies in how AB is reported. Few studies reported the statistical significance of AB compared to a control condition. Conclusion: AB appears to be associated with the degree of cognitive load experienced in decision tasks, and appears to not be uniquely associated with multitasking. Strategies to minimize AB might focus on cognitive load reduction."," *Attitude to Computers; Automation; Bias; *Decision Support Systems, Clinical; Humans; clinical cognitive biases; complexity; decision support systems",-2,-2,
187,Richesson,2016,Artificial intelligence in medicine,"Clinical phenotyping in selected national networks: demonstrating the need for high-throughput, portable, and computational methods","OBJECTIVE: The combination of phenomic data from electronic health records (EHR) and clinical data repositories with dense biological data has enabled genomic and pharmacogenomic discovery, a first step toward precision medicine. Computational methods for the identification of clinical phenotypes from EHR data will advance our understanding of disease risk and drug response, and support the practice of precision medicine on a national scale. METHODS: Based on our experience within three national research networks, we summarize the broad approaches to clinical phenotyping and highlight the important role of these networks in the progression of high-throughput phenotyping and precision medicine. We provide supporting literature in the form of a non-systematic review. RESULTS: The practice of clinical phenotyping is evolving to meet the growing demand for scalable, portable, and data driven methods and tools. The resources required for traditional phenotyping algorithms from expert defined rules are significant. In contrast, machine learning approaches that rely on data patterns will require fewer clinical domain experts and resources. CONCLUSIONS: Machine learning approaches that generate phenotype definitions from patient features and clinical profiles will result in truly computational phenotypes, derived from data rather than experts. Research networks and phenotype developers should cooperate to develop methods, collaboration platforms, and data standards that will enable computational phenotyping and truly modernize biomedical research and precision medicine.", *Algorithms; *Electronic Health Records; Genomics; Humans; *Machine Learning; *Phenotype; *Precision Medicine; *Clinical phenotyping; *Networked research,-2,-2,
188,Alberdi,2016,Artificial intelligence in medicine,On the early diagnosis of Alzheimer's Disease from multimodal signals: A survey,"INTRODUCTION: The number of Alzheimer's Disease (AD) patients is increasing with increased life expectancy and 115.4 million people are expected to be affected in 2050. Unfortunately, AD is commonly diagnosed too late, when irreversible damages have been caused in the patient. OBJECTIVE: An automatic, continuous and unobtrusive early AD detection method would be required to improve patients' life quality and avoid big healthcare costs. Thus, the objective of this survey is to review the multimodal signals that could be used in the development of such a system, emphasizing on the accuracy that they have shown up to date for AD detection. Some useful tools and specific issues towards this goal will also have to be reviewed. METHODS: An extensive literature review was performed following a specific search strategy, inclusion criteria, data extraction and quality assessment in the Inspec, Compendex and PubMed databases. RESULTS: This work reviews the extensive list of psychological, physiological, behavioural and cognitive measurements that could be used for AD detection. The most promising measurements seem to be magnetic resonance imaging (MRI) for AD vs control (CTL) discrimination with an 98.95% accuracy, while electroencephalogram (EEG) shows the best results for mild cognitive impairment (MCI) vs CTL (97.88%) and MCI vs AD distinction (94.05%). Available physiological and behavioural AD datasets are listed, as well as medical imaging analysis steps and neuroimaging processing toolboxes. Some issues such as ""label noise"" and multi-site data are discussed. CONCLUSIONS: The development of an unobtrusive and transparent AD detection system should be based on a multimodal system in order to take full advantage of all kinds of symptoms, detect even the smallest changes and combine them, so as to detect AD as early as possible. Such a multimodal system might probably be based on physiological monitoring of MRI or EEG, as well as behavioural measurements like the ones proposed along the article. The mentioned AD datasets and image processing toolboxes are available for their use towards this goal. Issues like ""label noise"" and multi-site neuroimaging incompatibilities may also have to be overcome, but methods for this purpose are already available."," Alzheimer Disease/*diagnosis; *Early Diagnosis; Humans; *Image Processing, Computer-Assisted; Magnetic Resonance Imaging; *Alzheimer's Disease; *Behaviour; *Early detection; *Multimodality; *Physiology",-2,-2,
189,Roberts,2017,Journal of the American Medical Informatics Association : JAMIA,Biomedical informatics advancing the national health agenda: the AMIA 2015 year-in-review in clinical and consumer informatics,"The field of biomedical informatics experienced a productive 2015 in terms of research. In order to highlight the accomplishments of that research, elicit trends, and identify shortcomings at a macro level, a 19-person team conducted an extensive review of the literature in clinical and consumer informatics. The result of this process included a year-in-review presentation at the American Medical Informatics Association Annual Symposium and a written report (see supplemental data). Key findings are detailed in the report and summarized here. This article organizes the clinical and consumer health informatics research from 2015 under 3 themes: the electronic health record (EHR), the learning health system (LHS), and consumer engagement. Key findings include the following: (1) There are significant advances in establishing policies for EHR feature implementation, but increased interoperability is necessary for these to gain traction. (2) Decision support systems improve practice behaviors, but evidence of their impact on clinical outcomes is still lacking. (3) Progress in natural language processing (NLP) suggests that we are approaching but have not yet achieved truly interactive NLP systems. (4) Prediction models are becoming more robust but remain hampered by the lack of interoperable clinical data records. (5) Consumers can and will use mobile applications for improved engagement, yet EHR integration remains elusive."," *Consumer Health Informatics; Humans; Meaningful Use; *Medical Informatics; Patient Participation; Public Health Informatics; Societies, Medical; United States; biomedical informatics; consumer engagement; electronic health records; learning health system; year in review",-2,-2,
190,David,2016,PloS one,Comorbid Analysis of Genes Associated with Autism Spectrum Disorders Reveals Differential Evolutionary Constraints,"The burden of comorbidity in Autism Spectrum Disorder (ASD) is substantial. The symptoms of autism overlap with many other human conditions, reflecting common molecular pathologies suggesting that cross-disorder analysis will help prioritize autism gene candidates. Genes in the intersection between autism and related conditions may represent nonspecific indicators of dysregulation while genes unique to autism may play a more causal role. Thorough literature review allowed us to extract 125 ICD-9 codes comorbid to ASD that we mapped to 30 specific human disorders. In the present work, we performed an automated extraction of genes associated with ASD and its comorbid disorders, and found 1031 genes involved in ASD, among which 262 are involved in ASD only, with the remaining 779 involved in ASD and at least one comorbid disorder. A pathway analysis revealed 13 pathways not involved in any other comorbid disorders and therefore unique to ASD, all associated with basal cellular functions. These pathways differ from the pathways associated with both ASD and its comorbid conditions, with the latter being more specific to neural function. To determine whether the sequence of these genes have been subjected to differential evolutionary constraints, we studied long term constraints by looking into Genomic Evolutionary Rate Profiling, and showed that genes involved in several comorbid disorders seem to have undergone more purifying selection than the genes involved in ASD only. This result was corroborated by a higher dN/dS ratio for genes unique to ASD as compare to those that are shared between ASD and its comorbid disorders. Short-term evolutionary constraints showed the same trend as the pN/pS ratio indicates that genes unique to ASD were under significantly less evolutionary constraint than the genes associated with all other disorders."," Autism Spectrum Disorder/*complications/*genetics; Cluster Analysis; Databases, Genetic; Humans; International Classification of Diseases",-2,-2,
191,Reed,2016,Systematic reviews,The HCV care continuum among people who use drugs: protocol for a systematic review and meta-analysis,"INTRODUCTION: The diagnosis, management, and treatment for hepatitis C virus (HCV) infection (the ""HCV care continuum"") have improved in recent years. People who use drugs (PWUD) have a prevalence of HCV infection from 30 to 70 %, yet rates of testing, engagement in care, and treatment for HCV are disproportionately low compared to other populations. Delineating the progression of PWUD through the steps in the HCV care continuum in the USA is important in informing efforts to improve HCV outcomes among PWUD. METHODS/DESIGN: Scientific databases will be searched using a comprehensive automated search strategy; gray literature and reference lists will be manually searched. Eligible reports will provide original research data related to the HCV care continuum in the USA including proportions of PWUD engaging in the following discrete steps: screening/testing, engagement in care (including receiving an HCV clinical assessment), treatment initiation and completion, and rates of those with successful HCV treatment. A quality-rating tool will be developed to ascertain the level of bias (including selection bias) in each report, and a quality score will be assigned to each eligible report. A tool adapted from the Pragmatic Explanatory Continuum Indicator Summary-2 instrument will be developed to assess the extent to which an included report reflects an effectiveness or efficacy study design. Pooled estimates and measures of association will be calculated using random effects models, and heterogeneity will be assessed at each stage of data synthesis. DISCUSSION: Through this review, we hope to quantify the proportion of PWUD at each progressive step and to help identify key individual, social, and structural points of leakage in the HCV care continuum for PWUD. In meeting these objectives, we will identify predictors to progress along the HCV care continuum, which can be used to inform policy to directly improve HCV care for PWUD. SYSTEMATIC REVIEW REGISTRATION: PROSPERO CRD42016034113.", *Continuity of Patient Care; *Drug Users; Health Services Accessibility; *Hepacivirus; Hepatitis C/etiology/*therapy/virology; Humans; Research Design; Substance-Related Disorders/*complications; Systematic Reviews as Topic; *Healthcare access; *Hepatitis c; *Hepatitis c care continuum; *Meta-analysis; *People who use drugs; *Systematic review,-2,-2,
192,Wang,2017,Journal of the American Medical Informatics Association : JAMIA,Use of electronic healthcare records to identify complex patients with atrial fibrillation for targeted intervention,"Background: Practice guidelines recommend anticoagulation therapy for patients with atrial fibrillation (AF) who have other risk factors putting them at an elevated risk of stroke. These patients remain undertreated, but, with increasing use of electronic healthcare records (EHRs), it may be possible to identify candidates for treatment. Objective: To test algorithms for identifying AF patients who also have known risk factors for stroke and major bleeding using EHR data. Materials and Methods: We evaluated the performance of algorithms using EHR data from the Partners Healthcare System at identifying AF patients and 16 additional conditions that are risk factors in the CHA 2 DS 2 -VASc and HAS-BLED risk scores for stroke and major bleeding. Algorithms were based on information contained in problem lists, billing codes, laboratory data, prescription data, vital status, and clinical notes. The performance of candidate algorithms in 1000 bootstrap resamples was compared to a gold standard of manual chart review by experienced resident physicians. Results: : Physicians reviewed 480 patient charts. For 11 conditions, the median positive predictive value (PPV) of the EHR-derived algorithms was greater than 0.90. Although the PPV for some risk factors was poor, the median PPV for identifying patients with a CHA 2 DS 2 -VASc score >/=2 or a HAS-BLED score >/=3 was 1.00 and 0.92, respectively. Discussion: We developed and tested a set of algorithms to identify AF patients and known risk factors for stroke and major bleeding using EHR data. Algorithms such as these can be built into EHR systems to facilitate informed decision making and help shift population health management efforts towards patients with the greatest need.", *Algorithms; Anticoagulants/adverse effects/therapeutic use; Atrial Fibrillation/complications/*diagnosis/drug therapy; *Electronic Health Records; Hemorrhage/etiology; Humans; Natural Language Processing; Risk Assessment/methods; Risk Factors; Stroke/etiology; algorithms; anticoagulation; chronic disease; outcomes; quality improvement; stroke,-2,-2,
193,Tsoromokos,2016,Studies in health technology and informatics,Design of an Innovative Information System for the Intensive Care Unit in a Public Hospital,"The health sector is increasingly focused on the use of Communication Technology (ICT) Information and Communication. New technologies which introduced in health, should lead to lower cost of procedures, saving employees' working time and immediate and secure data storages for easy future search or meta-analysis. The DPP4ICU application which presented in this document, allows at the Intensive Care Unit's nurses (ICU) to enter directly the handwritten accountability, in the Organization Information System. Through this application is accelerated the proper completion of a document and is improved data quality. The application provides the ability to authorized users to exchange information with an automated manner."," Communication; Documentation/*methods; *Handwriting; Hospitals, Public/organization & administration; Humans; Information Systems/*organization & administration; Intensive Care Units/*organization & administration; *Nursing Staff, Hospital; Software Design",-2,-2,
194,Carter,2016,Studies in health technology and informatics,Finding 'Evidence of Absence' in Medical Notes: Using NLP for Clinical Inferencing,"Extracting evidence of the absence of a target of interest from medical text can be useful in clinical inferencing. The purpose of our study was to develop a natural language processing (NLP) pipelineto identify the presence of indwelling urinary catheters from electronic medical notes to aid in detection of catheter-associated urinary tract infections (CAUTI). Finding clear evidence that a patient does not have an indwelling urinary catheter is useful in making a determination regarding CAUTI. We developed a lexicon of seven core concepts to infer the absence of a urinary catheter. Of the 990,391 concepts extractedby NLP from a large corpus of 744,285 electronic medical notes from 5589 hospitalized patients, 63,516 were labeled as evidence of absence.Human review revealed three primary causes for false negatives. The lexicon and NLP pipeline were refined using this information, resulting in outputs with an acceptable false positive rate of 11%.", Catheter-Related Infections/*diagnosis; Diagnostic Errors; Documentation/*statistics & numerical data; Electronic Health Records/*statistics & numerical data; Humans; Inpatients; *Natural Language Processing; Urinary Catheters/*adverse effects,-2,-2,
195,Topaz,2016,Studies in health technology and informatics,Mining Clinicians' Electronic Documentation to Identify Heart Failure Patients with Ineffective Self-Management: A Pilot Text-Mining Study,"Effective self-management can decrease up to 50% of heart failure hospitalizations. Unfortunately, self-management by patients with heart failure remains poor. This pilot study aimed to explore the use of text-mining to identify heart failure patients with ineffective self-management. We first built a comprehensive self-management vocabulary based on the literature and clinical notes review. We then randomly selected 545 heart failure patients treated within Partners Healthcare hospitals (Boston, MA, USA) and conducted a regular expression search with the compiled vocabulary within 43,107 interdisciplinary clinical notes of these patients. We found that 38.2% (n = 208) patients had documentation of ineffective heart failure self-management in the domains of poor diet adherence (28.4%), missed medical encounters (26.4%) poor medication adherence (20.2%) and non-specified self-management issues (e.g., ""compliance issues"", 34.6%). We showed the feasibility of using text-mining to identify patients with ineffective self-management. More natural language processing algorithms are needed to help busy clinicians identify these patients."," Boston/epidemiology; *Data Mining; Electronic Health Records/classification/*statistics & numerical data; Heart Failure/*diagnosis/epidemiology/*therapy; Humans; Natural Language Processing; Outcome Assessment (Health Care)/*methods; Patient Compliance/*statistics & numerical data; Pilot Projects; Prevalence; Reproducibility of Results; Risk Assessment; Self Care; Sensitivity and Specificity; Terminology as Topic; Treatment Failure; Treatment Outcome; Vocabulary, Controlled",-2,-2,
196,Yu,2016,Studies in health technology and informatics,Development and Appraisal of Multiple Accounting Record System (Mars),"UNLABELLED: The aim of the system is to achieve simplification of workflow, reduction of recording time, and increase the income for the study hospital. METHODS: The project team decided to develop a multiple accounting record system that generates the account records based on the nursing records automatically, reduces the time and effort for nurses to review the procedure and provide another note of material consumption. Three configuration files were identified to demonstrate the relationship of treatments and reimbursement items. RESULTS: The workflow was simplified. The nurses averagely reduced 10 minutes of daily recording time, and the reimbursement points have been increased by 7.49%. CONCLUSION: The project streamlined the workflow and provides the institute a better way in finical management."," *Accounts Payable and Receivable; Economics, Nursing/*organization & administration; Electronic Health Records/*economics; Financial Management, Hospital/*organization & administration; Health Care Costs/*statistics & numerical data; Hospital Information Systems/organization & administration; Management Information Systems/*economics; Taiwan",-2,-2,
197,Millar,2016,Studies in health technology and informatics,The Need for a Global Language - SNOMED CT Introduction,"SNOMED CT is the most comprehensive, multilingual clinical healthcare terminology in the world. It is a resource with comprehensive, scientifically validated clinical content. SNOMED CT enables consistent, processable representation of clinical content in electronic health records. When implemented in software applicationsSNOMED CT can be used to represent clinically relevant information consistently, reliabl comprehensively as an integral part of producing electronic health information. SNOMED CT supports the development of comprehensive high-quality clinical content in health records. It provides a standardized way to represent clinical phrases captured by the healthcare professional and enables automatic interpretation of these. SNOMED CT is a clinically validated, semantically rich, controlled vocabulary that facilitates evolutionary growth in expressivity to meet emerging requirements. SNOMED CT based clinical information benefits individual patients and clinicians as well as populations and it supports evidence based care. The use of an Electronic Health Record (EHR) improves communication and increases the availability of relevant information. IHTSDO works with other standards oganisations to ensure interoperability and a key area has been the work with ICN to enable the use of ICNP and SNOMED CT by the nursing profession internationally.", Electronic Health Records/*standards; Guideline Adherence/standards; Information Storage and Retrieval/*standards; Meaningful Use/standards; Medical Record Linkage/standards; Nursing Records/*standards; *Practice Guidelines as Topic; *Systematized Nomenclature of Medicine; *Terminology as Topic,1,-2,
198,Choi,2016,Studies in health technology and informatics,Exploration of Risk Factors for Falls Using Electronic Nursing Records,"INTRODUCTION: The purpose was to identify fall risk factors between admission day and fall occurred day using electronic nursing records and the Morse Fall Scale (MFS). METHODS: The MFS and fall related data were obtained through retrospective chart review from June 1, 2014 to May 31, 2015. Descriptive statistics and McNemar test were used for statistical tests. RESULTS: Fall was evaluated in 447 events, 16 patients experienced recurrent fall. Pain, emotional distress, urinary problems and fever were significant differences between admission day and fall occurred day. There were explored significant MFS risk factors in risk group, history of falling, second diagnosis, IV catheter status, medication concerning fall risk, mental status, general weakness and gait in MFS subscales. DISCUSSION: Routine fall screening is important for early detection of fall. Identification of high-risk group and using fall prevention guidelines could improve prevention of fall.", Accidental Falls/*prevention & control/*statistics & numerical data; Data Mining/*methods; Electronic Health Records/*statistics & numerical data; Female; Humans; Male; Nursing Care/statistics & numerical data; Nursing Records/*statistics & numerical data; Population Surveillance/*methods; Prevalence; Republic of Korea/epidemiology; Risk Factors,-2,-2,
199,Hausner,2016,Journal of clinical epidemiology,Prospective comparison of search strategies for systematic reviews: an objective approach yielded higher sensitivity than a conceptual one,"BACKGROUND: In the development of search strategies for systematic reviews, ""conceptual approaches"" are generally recommended to identify appropriate search terms for those parts of the strategies for which no validated search filters exist. However, ""objective approaches"" based on search terms identified by text analysis are increasingly being applied. OBJECTIVES: To prospectively compare an objective with a conceptual approach for the development of search strategies. METHODS: Two different MEDLINE search strategies were developed in parallel for five systematic reviews covering a range of topics and study designs. The Institute for Quality and Efficiency in Health Care (IQWiG) applied an objective approach, and external experts applied a conceptual approach for the same research questions. For each systematic review, the citations retrieved were combined and the overall pool of citations screened to determine sensitivity and precision. RESULTS: The objective approach yielded a weighted mean sensitivity and precision of 97% and 5%. The corresponding values for the conceptual approach were 75% and 4%. CONCLUSION: Our findings indicate that the objective approach applied by IQWiG for search strategy development yields higher sensitivity than and similar precision to a conceptual approach. The main advantage of the objective approach is that it produces consistent results across searches.", Humans; Information Storage and Retrieval/*methods/statistics & numerical data; MEDLINE/*statistics & numerical data; Prospective Studies; Reproducibility of Results; Research Design; *Review Literature as Topic; Sensitivity and Specificity; *Data mining; *Information storage and retrieval; *Medline; *Prospective studies; *Reproducibility of results; *Sensitivity and specificity,1,2,
200,Kwak,2016,BMC bioinformatics,Automated prostate tissue referencing for cancer detection and diagnosis,"BACKGROUND: The current practice of histopathology review is limited in speed and accuracy. The current diagnostic paradigm does not fully describe the complex and complicated patterns of cancer. To address these needs, we develop an automated and objective system that facilitates a comprehensive and easy information management and decision-making. We also develop a tissue similarity measure scheme to broaden our understanding of tissue characteristics. RESULTS: The system includes a database of previously evaluated prostate tissue images, clinical information and a tissue retrieval process. In the system, a tissue is characterized by its morphology. The retrieval process seeks to find the closest matching cases with the tissue of interest. Moreover, we define 9 morphologic criteria by which a pathologist arrives at a histomorphologic diagnosis. Based on the 9 criteria, true tissue similarity is determined and serves as the gold standard of tissue retrieval. Here, we found a minimum of 4 and 3 matching cases, out of 5, for ~80 % and ~60 % of the queries when a match was defined as the tissue similarity score >/=5 and >/=6, respectively. We were also able to examine the relationship between tissues beyond the Gleason grading system due to the tissue similarity scoring system. CONCLUSIONS: Providing the closest matching cases and their clinical information with pathologists will help to conduct consistent and reliable diagnoses. Thus, we expect the system to facilitate quality maintenance and quality improvement of cancer pathology."," Automation; Databases, Factual; Humans; Male; Neoplasm Grading; Prostate/pathology; Prostatic Neoplasms/*diagnosis/*pathology; Reproducibility of Results; Database; Decision support; Infrared imaging; Prostate cancer; Tissue morphology; Tissue retrieval",-2,-2,
201,Howard,2016,Systematic reviews,SWIFT-Review: a text-mining workbench for systematic review,"BACKGROUND: There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus. METHODS: Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95 % of known relevant studies and (2) the ""Work Saved over Sampling"" (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics. RESULTS: Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50 % of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation. CONCLUSIONS: Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation."," *Algorithms; *Data Mining; Databases, Factual; Humans; Information Storage and Retrieval; Linear Models; *Machine Learning; *Review Literature as Topic; *Software; Literature prioritization; SWIFT-Review; Scoping reports; Software; Systematic review",2,,
202,Hao,2016,Journal of medical Internet research,The Voice of Chinese Health Consumers: A Text Mining Approach to Web-Based Physician Reviews,"BACKGROUND: Many Web-based health care platforms allow patients to evaluate physicians by posting open-end textual reviews based on their experiences. These reviews are helpful resources for other patients to choose high-quality doctors, especially in countries like China where no doctor referral systems exist. Analyzing such a large amount of user-generated content to understand the voice of health consumers has attracted much attention from health care providers and health care researchers. OBJECTIVE: The aim of this paper is to automatically extract hidden topics from Web-based physician reviews using text-mining techniques to examine what Chinese patients have said about their doctors and whether these topics differ across various specialties. This knowledge will help health care consumers, providers, and researchers better understand this information. METHODS: We conducted two-fold analyses on the data collected from the ""Good Doctor Online"" platform, the largest online health community in China. First, we explored all reviews from 2006-2014 using descriptive statistics. Second, we applied the well-known topic extraction algorithm Latent Dirichlet Allocation to more than 500,000 textual reviews from over 75,000 Chinese doctors across four major specialty areas to understand what Chinese health consumers said online about their doctor visits. RESULTS: On the ""Good Doctor Online"" platform, 112,873 out of 314,624 doctors had been reviewed at least once by April 11, 2014. Among the 772,979 textual reviews, we chose to focus on four major specialty areas that received the most reviews: Internal Medicine, Surgery, Obstetrics/Gynecology and Pediatrics, and Chinese Traditional Medicine. Among the doctors who received reviews from those four medical specialties, two-thirds of them received more than two reviews and in a few extreme cases, some doctors received more than 500 reviews. Across the four major areas, the most popular topics reviewers found were the experience of finding doctors, doctors' technical skills and bedside manner, general appreciation from patients, and description of various symptoms. CONCLUSIONS: To the best of our knowledge, our work is the first study using an automated text-mining approach to analyze a large amount of unstructured textual data of Web-based physician reviews in China. Based on our analysis, we found that Chinese reviewers mainly concentrate on a few popular topics. This is consistent with the goal of Chinese online health platforms and demonstrates the health care focus in China's health care system. Our text-mining approach reveals a new research area on how to use big data to help health care providers, health care administrators, and policy makers hear patient voices, target patient concerns, and improve the quality of care in this age of patient-centered care. Also, on the health care consumer side, our text mining technique helps patients make more informed decisions about which specialists to see without reading thousands of reviews, which is simply not feasible. In addition, our comparison analysis of Web-based physician reviews in China and the United States also indicates some cultural differences.", China; Data Mining/*methods; Female; Humans; Internet/*statistics & numerical data; Male; Physicians; Telemedicine/*statistics & numerical data; Text Messaging/*statistics & numerical data; China health consumers; online doctor review; physician ratings; text mining,,,
203,Nath,2016,PloS one,A Natural Language Processing Tool for Large-Scale Data Extraction from Echocardiography Reports,"Large volumes of data are continuously generated from clinical notes and diagnostic studies catalogued in electronic health records (EHRs). Echocardiography is one of the most commonly ordered diagnostic tests in cardiology. This study sought to explore the feasibility and reliability of using natural language processing (NLP) for large-scale and targeted extraction of multiple data elements from echocardiography reports. An NLP tool, EchoInfer, was developed to automatically extract data pertaining to cardiovascular structure and function from heterogeneously formatted echocardiographic data sources. EchoInfer was applied to echocardiography reports (2004 to 2013) available from 3 different on-going clinical research projects. EchoInfer analyzed 15,116 echocardiography reports from 1684 patients, and extracted 59 quantitative and 21 qualitative data elements per report. EchoInfer achieved a precision of 94.06%, a recall of 92.21%, and an F1-score of 93.12% across all 80 data elements in 50 reports. Physician review of 400 reports demonstrated that EchoInfer achieved a recall of 92-99.9% and a precision of >97% in four data elements, including three quantitative and one qualitative data element. Failure of EchoInfer to correctly identify or reject reported parameters was primarily related to non-standardized reporting of echocardiography data. EchoInfer provides a powerful and reliable NLP-based approach for the large-scale, targeted extraction of information from heterogeneous data sources. The use of EchoInfer may have implications for the clinical management and research analysis of patients undergoing echocardiographic evaluation.", Aged; Echocardiography/*methods; Electronic Health Records; Female; Humans; Information Storage and Retrieval; Male; *Natural Language Processing; Reproducibility of Results,,,
204,Zhong,2016,Journal of the American Medical Informatics Association : JAMIA,An efficient approach for surveillance of childhood diabetes by type derived from electronic health record data: the SEARCH for Diabetes in Youth Study,"OBJECTIVE: To develop an efficient surveillance approach for childhood diabetes by type across 2 large US health care systems, using phenotyping algorithms derived from electronic health record (EHR) data. MATERIALS AND METHODS: Presumptive diabetes cases <20 years of age from 2 large independent health care systems were identified as those having >/=1 of the 5 indicators in the past 3.5 years, including elevated HbA1c, elevated blood glucose, diabetes-related billing codes, patient problem list, and outpatient anti-diabetic medications. EHRs of all the presumptive cases were manually reviewed, and true diabetes status and diabetes type were determined. Algorithms for identifying diabetes cases overall and classifying diabetes type were either prespecified or derived from classification and regression tree analysis. Surveillance approach was developed based on the best algorithms identified. RESULTS: We developed a stepwise surveillance approach using billing code-based prespecified algorithms and targeted manual EHR review, which efficiently and accurately ascertained and classified diabetes cases by type, in both health care systems. The sensitivity and positive predictive values in both systems were approximately >/=90% for ascertaining diabetes cases overall and classifying cases with type 1 or type 2 diabetes. About 80% of the cases with ""other"" type were also correctly classified. This stepwise surveillance approach resulted in a >70% reduction in the number of cases requiring manual validation compared to traditional surveillance methods. CONCLUSION: EHR data may be used to establish an efficient approach for large-scale surveillance for childhood diabetes by type, although some manual effort is still needed."," Adolescent; *Algorithms; Child; Child, Preschool; Clinical Coding; *Diabetes Mellitus, Type 1/classification; *Diabetes Mellitus, Type 2/classification; *Electronic Health Records; Female; Humans; Infant; Male; Population Surveillance/*methods; Sensitivity and Specificity; Young Adult; *ascertainment and classification; *automated algorithm; *childhood diabetes; *surveillance",-2,,
205,Fathiamini,2016,Journal of the American Medical Informatics Association : JAMIA,Automated identification of molecular effects of drugs (AIMED),"INTRODUCTION: Genomic profiling information is frequently available to oncologists, enabling targeted cancer therapy. Because clinically relevant information is rapidly emerging in the literature and elsewhere, there is a need for informatics technologies to support targeted therapies. To this end, we have developed a system for Automated Identification of Molecular Effects of Drugs, to help biomedical scientists curate this literature to facilitate decision support. OBJECTIVES: To create an automated system to identify assertions in the literature concerning drugs targeting genes with therapeutic implications and characterize the challenges inherent in automating this process in rapidly evolving domains. METHODS: We used subject-predicate-object triples (semantic predications) and co-occurrence relations generated by applying the SemRep Natural Language Processing system to MEDLINE abstracts and ClinicalTrials.gov descriptions. We applied customized semantic queries to find drugs targeting genes of interest. The results were manually reviewed by a team of experts. RESULTS: Compared to a manually curated set of relationships, recall, precision, and F2 were 0.39, 0.21, and 0.33, respectively, which represents a 3- to 4-fold improvement over a publically available set of predications (SemMedDB) alone. Upon review of ostensibly false positive results, 26% were considered relevant additions to the reference set, and an additional 61% were considered to be relevant for review. Adding co-occurrence data improved results for drugs in early development, but not their better-established counterparts. CONCLUSIONS: Precision medicine poses unique challenges for biomedical informatics systems that help domain experts find answers to their research questions. Further research is required to improve the performance of such systems, particularly for drugs in development.", Antineoplastic Agents/*pharmacology/therapeutic use; Clinical Trials as Topic; Humans; Information Storage and Retrieval/*methods; Medline; *Natural Language Processing; Neoplasms/*drug therapy/*genetics; *Precision Medicine; Semantics; Unified Medical Language System; *SemRep; *biomedical question answering; *molecular; *pharmacogenomics; *precision oncology; *targeted therapy,-2,,
206,Arambepola,2016,Journal of medical Internet research,The Impact of Automated Brief Messages Promoting Lifestyle Changes Delivered Via Mobile Devices to People with Type 2 Diabetes: A Systematic Literature Review and Meta-Analysis of Controlled Trials,"BACKGROUND: Brief automated messages have the potential to support self-management in people with type 2 diabetes, but their effect compared with usual care is unclear. OBJECTIVE: To examine the effectiveness of interventions to change lifestyle behavior delivered via automated brief messaging in patients with type 2 diabetes. METHODS: A systematic literature review of controlled trials examined the impact of interventions, delivered by brief messaging, and intended to promote lifestyle change in people with type 2 diabetes, on behavioral and clinical outcomes. Bibliographic databases searched included Medline, Embase, CINAHL, PsycINFO, and ISI WoK. Two reviewers independently screened citations. We extracted information on study risk of bias, setting (high versus low- and middle-income countries) and intervention characteristics (including use of theory and behavior-change techniques). Outcome measures included acceptability of the interventions and their impact on 1) determinants of lifestyle behavior (knowledge about diabetes, self-efficacy, attitudes towards self-management), 2) lifestyle behavior (diet, physical activity), and 3) clinical and patient-reported outcomes. Where possible, we pooled data using random-effects meta-analyses to obtain estimates of effect size of intervention compared to usual care. RESULTS: We identified 15 trials (15 interventions) meeting our inclusion criteria. Most interventions were delivered via short message service text messaging (n=12) and simultaneously targeted diet and physical activity (n=11). Nine interventions consisted of unidirectional messages, whereas six consisted of bidirectional messages, with patients receiving automated tailored feedback based on self-reported data. The acceptability of the interventions, and their impact on lifestyle behavior and its determinants, were examined in a low proportion of trials, with heterogeneous results being observed. In 13 trials (1155 patients) where data were available, there was a difference in glycated hemoglobin of -0.53% (95% CI -0.59% to -0.47%) between intervention groups compared to usual care. In five trials (406 patients) there was a non-significant difference in body mass index of -0.25 kg/m2 (95% CI -1.02 to 0.52). Interventions based on unidirectional messages produced similar effects in the outcomes examined, compared to those based on bidirectional messages. Interventions conducted in low- and middle-income countries showed a greater impact than those conducted in high-income countries. In general, trials were not free of bias and did not use explicit theory. CONCLUSIONS: Automated brief messages strategies can improve health outcomes in people with type 2 diabetes. Larger, methodologically robust trials are needed to confirm these positive results."," Diabetes Mellitus, Type 2/*therapy; Humans; Life Style; Middle Aged; *Self Care; *Text Messaging; Diabetes mellitus, type 2; diet; mobile health; physical activity; self-care; systematic review; text messaging",-2,,
207,Lamy,2016,Studies in health technology and informatics,Ontology-Oriented Programming for Biomedical Informatics,"Ontologies are now widely used in the biomedical domain. However, it is difficult to manipulate ontologies in a computer program and, consequently, it is not easy to integrate ontologies with databases or websites. Two main approaches have been proposed for accessing ontologies in a computer program: traditional API (Application Programming Interface) and ontology-oriented programming, either static or dynamic. In this paper, we will review these approaches and discuss their appropriateness for biomedical ontologies. We will also present an experience feedback about the integration of an ontology in a computer software during the VIIIP research project. Finally, we will present OwlReady, the solution we developed.", *Artificial Intelligence; Biological Ontologies/*organization & administration; Medical Informatics/*methods; Natural Language Processing; *Programming Languages; *Software,-1,,
208,Osborne,2016,Journal of the American Medical Informatics Association : JAMIA,Efficient identification of nationally mandated reportable cancer cases using natural language processing and machine learning,"OBJECTIVE: To help cancer registrars efficiently and accurately identify reportable cancer cases. MATERIAL AND METHODS: The Cancer Registry Control Panel (CRCP) was developed to detect mentions of reportable cancer cases using a pipeline built on the Unstructured Information Management Architecture - Asynchronous Scaleout (UIMA-AS) architecture containing the National Library of Medicine's UIMA MetaMap annotator as well as a variety of rule-based UIMA annotators that primarily act to filter out concepts referring to nonreportable cancers. CRCP inspects pathology reports nightly to identify pathology records containing relevant cancer concepts and combines this with diagnosis codes from the Clinical Electronic Data Warehouse to identify candidate cancer patients using supervised machine learning. Cancer mentions are highlighted in all candidate clinical notes and then sorted in CRCP's web interface for faster validation by cancer registrars. RESULTS: CRCP achieved an accuracy of 0.872 and detected reportable cancer cases with a precision of 0.843 and a recall of 0.848. CRCP increases throughput by 22.6% over a baseline (manual review) pathology report inspection system while achieving a higher precision and recall. Depending on registrar time constraints, CRCP can increase recall to 0.939 at the expense of precision by incorporating a data source information feature. CONCLUSION: CRCP demonstrates accurate results when applying natural language processing features to the problem of detecting patients with cases of reportable cancer from clinical notes. We show that implementing only a portion of cancer reporting rules in the form of regular expressions is sufficient to increase the precision, recall, and speed of the detection of reportable cancer cases when combined with off-the-shelf information extraction software and machine learning."," Data Mining/*methods; Electronic Health Records; Humans; International Classification of Diseases; *Machine Learning; Mandatory Reporting; *Natural Language Processing; *Neoplasms/pathology; Pathology, Clinical; United States; *electronic health records; *information extraction; *neoplasms; *user-computer interface",-2,,
209,Xu,2016,Journal of the American Medical Informatics Association : JAMIA,Extracting genetic alteration information for personalized cancer therapy from ClinicalTrials.gov,"OBJECTIVE: Clinical trials investigating drugs that target specific genetic alterations in tumors are important for promoting personalized cancer therapy. The goal of this project is to create a knowledge base of cancer treatment trials with annotations about genetic alterations from ClinicalTrials.gov. METHODS: We developed a semi-automatic framework that combines advanced text-processing techniques with manual review to curate genetic alteration information in cancer trials. The framework consists of a document classification system to identify cancer treatment trials from ClinicalTrials.gov and an information extraction system to extract gene and alteration pairs from the Title and Eligibility Criteria sections of clinical trials. By applying the framework to trials at ClinicalTrials.gov, we created a knowledge base of cancer treatment trials with genetic alteration annotations. We then evaluated each component of the framework against manually reviewed sets of clinical trials and generated descriptive statistics of the knowledge base. RESULTS AND DISCUSSION: The automated cancer treatment trial identification system achieved a high precision of 0.9944. Together with the manual review process, it identified 20 193 cancer treatment trials from ClinicalTrials.gov. The automated gene-alteration extraction system achieved a precision of 0.8300 and a recall of 0.6803. After validation by manual review, we generated a knowledge base of 2024 cancer trials that are labeled with specific genetic alteration information. Analysis of the knowledge base revealed the trend of increased use of targeted therapy for cancer, as well as top frequent gene-alteration pairs of interest. We expect this knowledge base to be a valuable resource for physicians and patients who are seeking information about personalized cancer therapy."," *Clinical Trials as Topic; DNA, Neoplasm; *Data Mining; Databases, Factual; Humans; *Knowledge Bases; Mutation; Natural Language Processing; Neoplasms/*genetics/therapy; Precision Medicine; *clinical trial; *natural language processing; *personalized cancer therapy",-2,,
210,Her,2016,Journal of the American Medical Informatics Association : JAMIA,The frequency of inappropriate nonformulary medication alert overrides in the inpatient setting,"BACKGROUND: Experts suggest that formulary alerts at the time of medication order entry are the most effective form of clinical decision support to automate formulary management. OBJECTIVE: Our objectives were to quantify the frequency of inappropriate nonformulary medication (NFM) alert overrides in the inpatient setting and provide insight on how the design of formulary alerts could be improved. METHODS: Alert overrides of the top 11 (n = 206) most-utilized and highest-costing NFMs, from January 1 to December 31, 2012, were randomly selected for appropriateness evaluation. Using an empirically developed appropriateness algorithm, appropriateness of NFM alert overrides was assessed by 2 pharmacists via chart review. Appropriateness agreement of overrides was assessed with a Cohen's kappa. We also assessed which types of NFMs were most likely to be inappropriately overridden, the override reasons that were disproportionately provided in the inappropriate overrides, and the specific reasons the overrides were considered inappropriate. RESULTS: Approximately 17.2% (n = 35.4/206) of NFM alerts were inappropriately overridden. Non-oral NFM alerts were more likely to be inappropriately overridden compared to orals. Alerts overridden with ""blank"" reasons were more likely to be inappropriate. The failure to first try a formulary alternative was the most common reason for alerts being overridden inappropriately. CONCLUSION: Approximately 1 in 5 NFM alert overrides are overridden inappropriately. Future research should evaluate the impact of mandating a valid override reason and adding a list of formulary alternatives to each NFM alert; we speculate these NFM alert features may decrease the frequency of inappropriate overrides."," Academic Medical Centers; Algorithms; Boston; Decision Support Systems, Clinical; Drug Therapy, Computer-Assisted; Formularies, Hospital as Topic; Hospitalization; Humans; *Medical Order Entry Systems; *alerts; *appropriateness; *clinical decision support; *computerized provider order entry; *hospital formulary",-2,,
211,Risson,2016,Journal of medical Internet research,Patterns of Treatment Switching in Multiple Sclerosis Therapies in US Patients Active on Social Media: Application of Social Media Content Analysis to Health Outcomes Research,"BACKGROUND: Social media analysis has rarely been applied to the study of specific questions in outcomes research. OBJECTIVE: The aim was to test the applicability of social media analysis to outcomes research using automated listening combined with filtering and analysis of data by specialists. After validation, the process was applied to the study of patterns of treatment switching in multiple sclerosis (MS). METHODS: A comprehensive listening and analysis process was developed that blended automated listening with filtering and analysis of data by life sciences-qualified analysts and physicians. The population was patients with MS from the United States. Data sources were Facebook, Twitter, blogs, and online forums. Sources were searched for mention of specific oral, injectable, and intravenous (IV) infusion treatments. The representativeness of the social media population was validated by comparison with community survey data and with data from three large US administrative claims databases: MarketScan, PharMetrics Plus, and Department of Defense. RESULTS: A total of 10,260 data points were sampled for manual review: 3025 from Twitter, 3771 from Facebook, 2773 from Internet forums, and 691 from blogs. The demographics of the social media population were similar to those reported from community surveys and claims databases. Mean age was 39 (SD 11) years and 14.56% (326/2239) of the population was older than 50 years. Women, patients aged 30 to 49 years, and those diagnosed for more than 10 years were represented by more data points than other patients were. Women also accounted for a large majority (82.6%, 819/991) of reported switches. Two-fifths of switching patients had lived with their disease for more than 10 years since diagnosis. Most reported switches (55.05%, 927/1684) were from injectable to oral drugs with switches from IV therapies to orals the second largest switch (15.38%, 259/1684). Switches to oral drugs accounted for more than 80% (927/1114) of the switches away from injectable therapies. Four reasons accounted for more than 90% of all switches: severe side effects, lack of efficacy, physicians' advice, and greater ease of use. Side effects were the main reason for switches to oral or to injectable therapies and search for greater efficacy was the most important factor in switches to IV therapies. Cost of medication was the reason for switching in less than 0.5% of patients. CONCLUSIONS: Social intelligence can be applied to outcomes research with power to analyze MS patients' personal experiences of treatments and to chart the most common reasons for switching between therapies."," Adult; Aged; *Blogging; Data Collection; Databases, Factual; *Drug Substitution; Female; Humans; *Internet; Male; Middle Aged; Multiple Sclerosis/*drug therapy; Outcome Assessment (Health Care); *Social Media; Surveys and Questionnaires; United States; Young Adult; Internet; drug switching; multiple sclerosis; outcomes assessment",-2,,
212,Sadasivam,2016,Journal of medical Internet research,Collective-Intelligence Recommender Systems: Advancing Computer Tailoring for Health Behavior Change Into the 21st Century,"BACKGROUND: What is the next frontier for computer-tailored health communication (CTHC) research? In current CTHC systems, study designers who have expertise in behavioral theory and mapping theory into CTHC systems select the variables and develop the rules that specify how the content should be tailored, based on their knowledge of the targeted population, the literature, and health behavior theories. In collective-intelligence recommender systems (hereafter recommender systems) used by Web 2.0 companies (eg, Netflix and Amazon), machine learning algorithms combine user profiles and continuous feedback ratings of content (from themselves and other users) to empirically tailor content. Augmenting current theory-based CTHC with empirical recommender systems could be evaluated as the next frontier for CTHC. OBJECTIVE: The objective of our study was to uncover barriers and challenges to using recommender systems in health promotion. METHODS: We conducted a focused literature review, interviewed subject experts (n=8), and synthesized the results. RESULTS: We describe (1) limitations of current CTHC systems, (2) advantages of incorporating recommender systems to move CTHC forward, and (3) challenges to incorporating recommender systems into CTHC. Based on the evidence presented, we propose a future research agenda for CTHC systems. CONCLUSIONS: We promote discussion of ways to move CTHC into the 21st century by incorporation of recommender systems.", Algorithms; Computers/trends; Feedback; *Health Behavior; Health Communication/*methods/trends; Humans; *Internet; Machine Learning; computer-tailored health communication; recommender systems,-2,,
213,Wallis,2016,PloS one,A Smartphone App and Cloud-Based Consultation System for Burn Injury Emergency Care,"BACKGROUND: Each year more than 10 million people worldwide are burned severely enough to require medical attention, with clinical outcomes noticeably worse in resource poor settings. Expert clinical advice on acute injuries can play a determinant role and there is a need for novel approaches that allow for timely access to advice. We developed an interactive mobile phone application that enables transfer of both patient data and pictures of a wound from the point-of-care to a remote burns expert who, in turn, provides advice back. METHODS AND RESULTS: The application is an integrated clinical decision support system that includes a mobile phone application and server software running in a cloud environment. The client application is installed on a smartphone and structured patient data and photographs can be captured in a protocol driven manner. The user can indicate the specific injured body surface(s) through a touchscreen interface and an integrated calculator estimates the total body surface area that the burn injury affects. Predefined standardised care advice including total fluid requirement is provided immediately by the software and the case data are relayed to a cloud server. A text message is automatically sent to a burn expert on call who then can access the cloud server with the smartphone app or a web browser, review the case and pictures, and respond with both structured and personalized advice to the health care professional at the point-of-care. CONCLUSIONS: In this article, we present the design of the smartphone and the server application alongside the type of structured patient data collected together with the pictures taken at point-of-care. We report on how the application will be introduced at point-of-care and how its clinical impact will be evaluated prior to roll out. Challenges, strengths and limitations of the system are identified that may help materialising or hinder the expected outcome to provide a solution for remote consultation on burns that can be integrated into routine acute clinical care and thereby promote equity in injury emergency care, a growing public health burden.", Body Surface Area; Burns/diagnosis/*therapy; Cloud Computing; *Emergency Medical Services; Humans; *Mobile Applications; Point-of-Care Systems; *Remote Consultation; *Smartphone; Text Messaging,-2,,
214,Evans,2016,Journal of the American Medical Informatics Association : JAMIA,Automated identification and predictive tools to help identify high-risk heart failure patients: pilot evaluation,"OBJECTIVE: Develop and evaluate an automated identification and predictive risk report for hospitalized heart failure (HF) patients. METHODS: Dictated free-text reports from the previous 24 h were analyzed each day with natural language processing (NLP), to help improve the early identification of hospitalized patients with HF. A second application that uses an Intermountain Healthcare-developed predictive score to determine each HF patient's risk for 30-day hospital readmission and 30-day mortality was also developed. That information was included in an identification and predictive risk report, which was evaluated at a 354-bed hospital that treats high-risk HF patients. RESULTS: The addition of NLP-identified HF patients increased the identification score's sensitivity from 82.6% to 95.3% and its specificity from 82.7% to 97.5%, and the model's positive predictive value is 97.45%. Daily multidisciplinary discharge planning meetings are now based on the information provided by the HF identification and predictive report, and clinician's review of potential HF admissions takes less time compared to the previously used manual methodology (10 vs 40 min). An evaluation of the use of the HF predictive report identified a significant reduction in 30-day mortality and a significant increase in patient discharges to home care instead of to a specialized nursing facility. CONCLUSIONS: Using clinical decision support to help identify HF patients and automatically calculating their 30-day all-cause readmission and 30-day mortality risks, coupled with a multidisciplinary care process pathway, was found to be an effective process to improve HF patient identification, significantly reduce 30-day mortality, and significantly increase patient discharges to home care."," Analysis of Variance; *Decision Making, Computer-Assisted; *Electronic Health Records; Female; Heart Failure/*diagnosis/mortality/therapy; Hospital Information Systems; Hospitalization; Humans; Male; *Natural Language Processing; Patient Readmission; Pilot Projects; *Risk Assessment; Sensitivity and Specificity; Severity of Illness Index; *clinical decision support; *heart failure; *risk stratification",-2,,
215,Ford,2016,Journal of the American Medical Informatics Association : JAMIA,Extracting information from the text of electronic medical records to improve case detection: a systematic review,"BACKGROUND: Electronic medical records (EMRs) are revolutionizing health-related research. One key issue for study quality is the accurate identification of patients with the condition of interest. Information in EMRs can be entered as structured codes or unstructured free text. The majority of research studies have used only coded parts of EMRs for case-detection, which may bias findings, miss cases, and reduce study quality. This review examines whether incorporating information from text into case-detection algorithms can improve research quality. METHODS: A systematic search returned 9659 papers, 67 of which reported on the extraction of information from free text of EMRs with the stated purpose of detecting cases of a named clinical condition. Methods for extracting information from text and the technical accuracy of case-detection algorithms were reviewed. RESULTS: Studies mainly used US hospital-based EMRs, and extracted information from text for 41 conditions using keyword searches, rule-based algorithms, and machine learning methods. There was no clear difference in case-detection algorithm accuracy between rule-based and machine learning methods of extraction. Inclusion of information from text resulted in a significant improvement in algorithm sensitivity and area under the receiver operating characteristic in comparison to codes alone (median sensitivity 78% (codes + text) vs 62% (codes), P = .03; median area under the receiver operating characteristic 95% (codes + text) vs 88% (codes), P = .025). CONCLUSIONS: Text in EMRs is accessible, especially with open source information extraction algorithms, and significantly improves case detection when combined with codes. More harmonization of reporting within EMR studies is needed, particularly standardized reporting of algorithm accuracy metrics like positive predictive value (precision) and sensitivity (recall).", *Algorithms; Data Mining; Diagnosis; *Electronic Health Records; Humans; *Information Storage and Retrieval; Machine Learning; Natural Language Processing; Sensitivity and Specificity; *case detection; *data quality; *review; *text mining,-2,,
216,Han,2016,PloS one,Reassessment of Species Diversity of the Subfamily Denticollinae (Coleoptera: Elateridae) through DNA Barcoding,"The subfamily Denticollinae is a taxonomically diverse group in the family Elateridae. Denticollinae includes many morphologically similar species and crop pests, as well as many undescribed species at each local fauna. To construct a rapid and reliable identification system for this subfamily, the effectiveness of molecular species identification was assessed based on 421 cytochrome c oxidase subunit I (COI) sequences of 84 morphologically identified species. Among the 84 morphospecies, molecular species identification of 60 species (71.4%) was consistent with their morphological identifications. Six cryptic and/or pseudocryptic species with large genetic divergence (>5%) were confirmed by their sympatric or allopatric distributions. However, 18 species, including a subspecies, had ambiguous genetic distances and shared overlapping intra- and interspecific genetic distances (range: 2.12%-3.67%) suggesting incomplete lineage sorting, introgression of mitochondrial genome, or affection by endosymbionts, such as Wolbachia infection, between species and simple genetic variation within species. In this study, we propose a conservative threshold of 3.6% for convenient molecular operational taxonomic unit (MOTU) identification in the subfamily Denticollinae based on the results of pairwise genetic distances analyses using neighbor-joining, mothur, Automatic Barcode Gap Discovery analysis, and tree-based species delimitation by Poisson Tree Processes analysis. Using the 3.6% threshold, we identified 87 MOTUs and found 8 MOTUs in the interval between 2.5% to 3.5%. Evaluation of MOTUs identified in this range requires integrative species delimitation, including review of morphological and ecological differences as well as sensitive genetic markers. From this study, we confirmed that COI sequence is useful for reassessing species diversity for polymorphic and polytypic species occurring in sympatric and allopatric distributions, and for a single species having an extensively large habitat."," Animals; Coleoptera/anatomy & histology/*genetics; *DNA Barcoding, Taxonomic; Electron Transport Complex IV/chemistry/genetics; Gene Library; *Genetic Variation; Likelihood Functions; Species Specificity",-2,,
217,Abdi,2016,PloS one,An Automated Summarization Assessment Algorithm for Identifying Summarizing Strategies,"BACKGROUND: Summarization is a process to select important information from a source text. Summarizing strategies are the core cognitive processes in summarization activity. Since summarization can be important as a tool to improve comprehension, it has attracted interest of teachers for teaching summary writing through direct instruction. To do this, they need to review and assess the students' summaries and these tasks are very time-consuming. Thus, a computer-assisted assessment can be used to help teachers to conduct this task more effectively. DESIGN/RESULTS: This paper aims to propose an algorithm based on the combination of semantic relations between words and their syntactic composition to identify summarizing strategies employed by students in summary writing. An innovative aspect of our algorithm lies in its ability to identify summarizing strategies at the syntactic and semantic levels. The efficiency of the algorithm is measured in terms of Precision, Recall and F-measure. We then implemented the algorithm for the automated summarization assessment system that can be used to identify the summarizing strategies used by students in summary writing.", *Algorithms; Automation; Humans; Students/psychology; Writing,1,,
218,Van Poucke,2016,PloS one,Scalable Predictive Analysis in Critically Ill Patients Using a Visual Open Data Analysis Platform,"With the accumulation of large amounts of health related data, predictive analytics could stimulate the transformation of reactive medicine towards Predictive, Preventive and Personalized (PPPM) Medicine, ultimately affecting both cost and quality of care. However, high-dimensionality and high-complexity of the data involved, prevents data-driven methods from easy translation into clinically relevant models. Additionally, the application of cutting edge predictive methods and data manipulation require substantial programming skills, limiting its direct exploitation by medical domain experts. This leaves a gap between potential and actual data usage. In this study, the authors address this problem by focusing on open, visual environments, suited to be applied by the medical community. Moreover, we review code free applications of big data technologies. As a showcase, a framework was developed for the meaningful use of data from critical care patients by integrating the MIMIC-II database in a data mining environment (RapidMiner) supporting scalable predictive analytics using visual tools (RapidMiner's Radoop extension). Guided by the CRoss-Industry Standard Process for Data Mining (CRISP-DM), the ETL process (Extract, Transform, Load) was initiated by retrieving data from the MIMIC-II tables of interest. As use case, correlation of platelet count and ICU survival was quantitatively assessed. Using visual tools for ETL on Hadoop and predictive modeling in RapidMiner, we developed robust processes for automatic building, parameter optimization and evaluation of various predictive models, under different feature selection schemes. Because these processes can be easily adopted in other projects, this environment is attractive for scalable predictive analytics in health research."," Algorithms; Critical Care/*statistics & numerical data; Critical Illness/*therapy; Data Mining/methods/*statistics & numerical data; Databases, Factual/statistics & numerical data; Humans; Information Storage and Retrieval/methods/*statistics & numerical data; Intensive Care Units/statistics & numerical data; Models, Theoretical; Programming Languages; Reproducibility of Results",-1,,
219,Walker,2016,International journal of medical informatics,Computer-assisted expert case definition in electronic health records,"PURPOSE: To describe how computer-assisted presentation of case data can lead experts to infer machine-implementable rules for case definition in electronic health records. As an illustration the technique has been applied to obtain a definition of acute liver dysfunction (ALD) in persons with inflammatory bowel disease (IBD). METHODS: The technique consists of repeatedly sampling new batches of case candidates from an enriched pool of persons meeting presumed minimal inclusion criteria, classifying the candidates by a machine-implementable candidate rule and by a human expert, and then updating the rule so that it captures new distinctions introduced by the expert. Iteration continues until an update results in an acceptably small number of changes to form a final case definition. RESULTS: The technique was applied to structured data and terms derived by natural language processing from text records in 29,336 adults with IBD. Over three rounds the technique led to rules with increasing predictive value, as the experts identified exceptions, and increasing sensitivity, as the experts identified missing inclusion criteria. In the final rule inclusion and exclusion terms were often keyed to an ALD onset date. When compared against clinical review in an independent test round, the derived final case definition had a sensitivity of 92% and a positive predictive value of 79%. CONCLUSION: An iterative technique of machine-supported expert review can yield a case definition that accommodates available data, incorporates pre-existing medical knowledge, is transparent and is open to continuous improvement. The expert updates to rules may be informative in themselves. In this limited setting, the final case definition for ALD performed better than previous, published attempts using expert definitions."," Adolescent; Adult; Aged; *Diagnosis, Computer-Assisted; Electronic Health Records/*statistics & numerical data; Female; Humans; Liver Diseases/*diagnosis; Male; *Medical Informatics; Middle Aged; Natural Language Processing; Young Adult; Case definition; Electronic health records; Health outcomes; Insurance claims data; Safety surveillance",-2,,
220,Puttkammer,2016,International journal of medical informatics,An assessment of data quality in a multi-site electronic medical record system in Haiti,"OBJECTIVES: Strong data quality (DQ) is a precursor to strong data use. In resource limited settings, routine DQ assessment (DQA) within electronic medical record (EMR) systems can be resource-intensive using manual methods such as audit and chart review; automated queries offer an efficient alternative. This DQA focused on Haiti's national EMR - iSante - and included longitudinal data for over 100,000 persons living with HIV (PLHIV) enrolled in HIV care and treatment services at 95 health care facilities (HCF). METHODS: This mixed-methods evaluation used a qualitative Delphi process to identify DQ priorities among local stakeholders, followed by a quantitative DQA on these priority areas. The quantitative DQA examined 13 indicators of completeness, accuracy, and timeliness of retrospective data collected from 2005 to 2013. We described levels of DQ for each indicator over time, and examined the consistency of within-HCF performance and associations between DQ and HCF and EMR system characteristics. RESULTS: Over all iSante data, age was incomplete in <1% of cases, while height, pregnancy status, TB status, and ART eligibility were more incomplete (approximately 20-40%). Suspicious data flags were present for <3% of cases of male sex, ART dispenses, CD4 values, and visit dates, but for 26% of cases of age. Discontinuation forms were available for about half of all patients without visits for 180 or more days, and >60% of encounter forms were entered late. For most indicators, DQ tended to improve over time. DQ was highly variable across HCF, and within HCFs DQ was variable across indicators. In adjusted analyses, HCF and system factors with generally favorable and statistically significant associations with DQ were University hospital category, private sector governance, presence of local iSante server, greater HCF experience with the EMR, greater maturity of the EMR itself, and having more system users but fewer new users. In qualitative feedback, local stakeholders emphasized lack of stable power supply as a key challenge to data quality and use of the iSante EMR. CONCLUSIONS: Variable performance on key DQ indicators across HCF suggests that excellent DQ is achievable in Haiti, but further effort is needed to systematize and routinize DQ approaches within HCFs. A dynamic, interactive ""DQ dashboard"" within iSante could bring transparency and motivate improvement. While the results of the study are specific to Haiti's iSante data system, the study's methods and thematic lessons learned holdgeneralized relevance for other large-scale EMR systems in resource-limited countries.", *Data Accuracy; Electronic Health Records/*organization & administration/*statistics & numerical; data; Evaluation Studies as Topic; Female; HIV/pathogenicity; HIV Infections/diagnosis/*drug therapy; Haiti; Health Services; Humans; Male; Pregnancy; Retrospective Studies; Data quality assessment; Electronic medical record; Health information system,-2,,
221,Mo,2015,Systematic reviews,Supporting systematic reviews using LDA-based document representations,"BACKGROUND: Identifying relevant studies for inclusion in a systematic review (i.e. screening) is a complex, laborious and expensive task. Recently, a number of studies has shown that the use of machine learning and text mining methods to automatically identify relevant studies has the potential to drastically decrease the workload involved in the screening phase. The vast majority of these machine learning methods exploit the same underlying principle, i.e. a study is modelled as a bag-of-words (BOW). METHODS: We explore the use of topic modelling methods to derive a more informative representation of studies. We apply Latent Dirichlet allocation (LDA), an unsupervised topic modelling approach, to automatically identify topics in a collection of studies. We then represent each study as a distribution of LDA topics. Additionally, we enrich topics derived using LDA with multi-word terms identified by using an automatic term recognition (ATR) tool. For evaluation purposes, we carry out automatic identification of relevant studies using support vector machine (SVM)-based classifiers that employ both our novel topic-based representation and the BOW representation. RESULTS: Our results show that the SVM classifier is able to identify a greater number of relevant studies when using the LDA representation than the BOW representation. These observations hold for two systematic reviews of the clinical domain and three reviews of the social science domain. CONCLUSIONS: A topic-based feature representation of documents outperforms the BOW representation when applied to the task of automatic citation screening. The proposed term-enriched topics are more informative and less ambiguous to systematic reviewers."," Biomedical Research/*classification; Data Mining/*methods; Decision Making, Computer-Assisted; Humans; *Models, Statistical; *Review Literature as Topic; *Support Vector Machine",2,,
222,Wang,2015,PloS one,Identifying Triple-Negative Breast Cancer Using Background Parenchymal Enhancement Heterogeneity on Dynamic Contrast-Enhanced MRI: A Pilot Radiomics Study,"OBJECTIVES: To determine the added discriminative value of detailed quantitative characterization of background parenchymal enhancement in addition to the tumor itself on dynamic contrast-enhanced (DCE) MRI at 3.0 Tesla in identifying ""triple-negative"" breast cancers. MATERIALS AND METHODS: In this Institutional Review Board-approved retrospective study, DCE-MRI of 84 women presenting 88 invasive carcinomas were evaluated by a radiologist and analyzed using quantitative computer-aided techniques. Each tumor and its surrounding parenchyma were segmented semi-automatically in 3-D. A total of 85 imaging features were extracted from the two regions, including morphologic, densitometric, and statistical texture measures of enhancement. A small subset of optimal features was selected using an efficient sequential forward floating search algorithm. To distinguish triple-negative cancers from other subtypes, we built predictive models based on support vector machines. Their classification performance was assessed with the area under receiver operating characteristic curve (AUC) using cross-validation. RESULTS: Imaging features based on the tumor region achieved an AUC of 0.782 in differentiating triple-negative cancers from others, in line with the current state of the art. When background parenchymal enhancement features were included, the AUC increased significantly to 0.878 (p<0.01). Similar improvements were seen in nearly all subtype classification tasks undertaken. Notably, amongst the most discriminating features for predicting triple-negative cancers were textures of background parenchymal enhancement. CONCLUSIONS: Considering the tumor as well as its surrounding parenchyma on DCE-MRI for radiomic image phenotyping provides useful information for identifying triple-negative breast cancers. Heterogeneity of background parenchymal enhancement, characterized by quantitative texture features on DCE-MRI, adds value to such differentiation models as they are strongly associated with the triple-negative subtype. Prospective validation studies are warranted to confirm these findings and determine potential implications."," Aged; Biomarkers, Tumor; Cluster Analysis; Contrast Media; Female; Humans; *Image Enhancement; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Magnetic Resonance Imaging/*methods; Middle Aged; Pilot Projects; Prognosis; Risk Factors; Triple Negative Breast Neoplasms/*diagnosis/pathology",-2,,
223,Tsertsvadze,2015,Systematic reviews,How to conduct systematic reviews more expeditiously?,"Healthcare consumers, researchers, patients and policy makers increasingly use systematic reviews (SRs) to aid their decision-making process. However, the conduct of SRs can be a time-consuming and resource-intensive task. Often, clinical practice guideline developers or other decision-makers need to make informed decisions in a timely fashion (e.g. outbreaks of infection, hospital-based health technology assessments). Possible approaches to address the issue of timeliness in the production of SRs are to (a) implement process parallelisation, (b) adapt and apply innovative technologies, and/or (c) modify SR processes (e.g. study eligibility criteria, search sources, data extraction or quality assessment). Highly parallelised systematic reviewing requires substantial resources to support a team of experienced information specialists, reviewers and methodologists working alongside with clinical content experts to minimise the time for completing individual review steps while maximising the parallel progression of multiple steps. Effective coordination and management within the team and across external stakeholders are essential elements of this process. Emerging innovative technologies have a great potential for reducing workload and improving efficiency of SR production. The most promising areas of application would be to allow automation of specific SR tasks, in particular if these tasks are time consuming and resource intensive (e.g. language translation, study selection, data extraction). Modification of SR processes involves restricting, truncating and/or bypassing one or more SR steps, which may risk introducing bias to the review findings. Although the growing experiences in producing various types of rapid reviews (RR) and the accumulation of empirical studies exploring potential bias associated with specific SR tasks have contributed to the methodological development for expediting SR production, there is still a dearth of research examining the actual impact of methodological modifications and comparing the findings between RRs and SRs. This evidence would help to inform as to which SR tasks can be accelerated or truncated and to what degree, while maintaining the validity of review findings. Timely delivered SRs can be of value in informing healthcare decisions and recommendations, especially when there is practical urgency and there is no other relevant synthesised evidence."," Administrative Personnel; Cost-Benefit Analysis; Decision Making; Disease Outbreaks; *Evidence-Based Medicine; Humans; *Research Design; *Review Literature as Topic; Technology Assessment, Biomedical",2,,
224,Taggart,2015,Journal of medical Internet research,Social Media and HIV: A Systematic Review of Uses of Social Media in HIV Communication,"BACKGROUND: Social media, including mobile technologies and social networking sites, are being used increasingly as part of human immunodeficiency virus (HIV) prevention and treatment efforts. As an important avenue for communication about HIV, social media use may continue to increase and become more widespread. OBJECTIVE: The objective of this paper is to present a comprehensive systematic review of the current published literature on the design, users, benefits, and limitations of using social media to communicate about HIV prevention and treatment. METHODS: This review paper used a systematic approach to survey all literature published before February 2014 using 7 electronic databases and a manual search. The inclusion criteria were (1) primary focus on communication/interaction about HIV/acquired immunodeficiency syndrome (AIDS), (2) discusses the use of social media to facilitate communication, (3) communication on the social media platform is between individuals or a group of individuals rather than the use of preset, automated responses from a platform, (4) published before February 19, 2014, and (5) all study designs. RESULTS: The search identified 35 original research studies. Thirty studies had low or unclear risk of at least one of the bias items in the methodological quality assessment. Among the 8 social media platform types described, short message service text messaging was most commonly used. Platforms served multiple purposes including disseminating health information, conducting health promotion, sharing experiences, providing social support, and promoting medication adherence. Social media users were diverse in geographic location and race/ethnicity; studies commonly reported users aged 18-40 years and users with lower income. Although most studies did not specify whether use was anonymous, studies reported the importance of anonymity in social media use to communicate about HIV largely due to the stigma associated with HIV. The ability to share and receive information about HIV was the most commonly reported benefit of social media use and the most common challenges were related to technology. Measures of frequency of use, satisfaction, and effects of use varied across studies. CONCLUSIONS: Using social media to bridge communication among a diverse range of users, in various geographic and social contexts, may be leveraged through pre-existing platforms and with attention to the roles of anonymity and confidentiality in communication about HIV prevention and treatment. More robust research is needed to determine the effects of social media use on various health and social outcomes related to HIV.", Adolescent; Adult; Female; HIV Infections/*prevention & control; Health Communication/*methods; Health Promotion/*methods; Humans; Male; Middle Aged; Social Media/*statistics & numerical data; Social Networking; Surveys and Questionnaires; Young Adult; Hiv; communication; social media,-2,,
225,Ruzich,2015,PloS one,Sex and STEM Occupation Predict Autism-Spectrum Quotient (AQ) Scores in Half a Million People,"This study assesses Autism-Spectrum Quotient (AQ) scores in a 'big data' sample collected through the UK Channel 4 television website, following the broadcasting of a medical education program. We examine correlations between the AQ and age, sex, occupation, and UK geographic region in 450,394 individuals. We predicted that age and geography would not be correlated with AQ, whilst sex and occupation would have a correlation. Mean AQ for the total sample score was m = 19.83 (SD = 8.71), slightly higher than a previous systematic review of 6,900 individuals in a non-clinical sample (mean of means = 16.94) This likely reflects that this big-data sample includes individuals with autism who in the systematic review score much higher (mean of means = 35.19). As predicted, sex and occupation differences were observed: on average, males (m = 21.55, SD = 8.82) scored higher than females (m = 18.95; SD = 8.52), and individuals working in a STEM career (m = 21.92, SD = 8.92) scored higher than individuals non-STEM careers (m = 18.92, SD = 8.48). Also as predicted, age and geographic region were not meaningfully correlated with AQ. These results support previous findings relating to sex and STEM careers in the largest set of individuals for which AQ scores have been reported and suggest the AQ is a useful self-report measure of autistic traits.", Adult; Autistic Disorder/*physiopathology; Female; Humans; Male; Occupations; Personality Assessment; Psychometrics/methods; Self Report; United Kingdom,-2,,
226,Hassanpour,2016,Artificial intelligence in medicine,Information extraction from multi-institutional radiology reports,"OBJECTIVES: The radiology report is the most important source of clinical imaging information. It documents critical information about the patient's health and the radiologist's interpretation of medical findings. It also communicates information to the referring physicians and records that information for future clinical and research use. Although efforts to structure some radiology report information through predefined templates are beginning to bear fruit, a large portion of radiology report information is entered in free text. The free text format is a major obstacle for rapid extraction and subsequent use of information by clinicians, researchers, and healthcare information systems. This difficulty is due to the ambiguity and subtlety of natural language, complexity of described images, and variations among different radiologists and healthcare organizations. As a result, radiology reports are used only once by the clinician who ordered the study and rarely are used again for research and data mining. In this work, machine learning techniques and a large multi-institutional radiology report repository are used to extract the semantics of the radiology report and overcome the barriers to the re-use of radiology report information in clinical research and other healthcare applications. MATERIAL AND METHODS: We describe a machine learning system to annotate radiology reports and extract report contents according to an information model. This information model covers the majority of clinically significant contents in radiology reports and is applicable to a wide variety of radiology study types. Our automated approach uses discriminative sequence classifiers for named-entity recognition to extract and organize clinically significant terms and phrases consistent with the information model. We evaluated our information extraction system on 150 radiology reports from three major healthcare organizations and compared its results to a commonly used non-machine learning information extraction method. We also evaluated the generalizability of our approach across different organizations by training and testing our system on data from different organizations. RESULTS: Our results show the efficacy of our machine learning approach in extracting the information model's elements (10-fold cross-validation average performance: precision: 87%, recall: 84%, F1 score: 85%) and its superiority and generalizability compared to the common non-machine learning approach (p-value<0.05). CONCLUSIONS: Our machine learning information extraction approach provides an effective automatic method to annotate and extract clinically significant information from a large collection of free text radiology reports. This information extraction system can help clinicians better understand the radiology reports and prioritize their review process. In addition, the extracted information can be used by researchers to link radiology reports to information from other data sources such as electronic health records and the patient's genome. Extracted information also can facilitate disease surveillance, real-time clinical decision support for the radiologist, and content-based image retrieval."," Data Mining; Databases, Factual; Discriminant Analysis; Electronic Health Records; Humans; Information Storage and Retrieval/*methods; *Machine Learning; Medical Record Linkage; Models, Theoretical; *Natural Language Processing; Observer Variation; Pattern Recognition, Automated; Predictive Value of Tests; Radiographic Image Interpretation, Computer-Assisted/*methods; *Radiology Information Systems; Reproducibility of Results; *Semantics; United States; Discriminative sequence classifier; Information extraction; Natural language processing; Radiology report narrative",-2,,
227,van Valkenhoef,2016,Research synthesis methods,Automated generation of node-splitting models for assessment of inconsistency in network meta-analysis,"Network meta-analysis enables the simultaneous synthesis of a network of clinical trials comparing any number of treatments. Potential inconsistencies between estimates of relative treatment effects are an important concern, and several methods to detect inconsistency have been proposed. This paper is concerned with the node-splitting approach, which is particularly attractive because of its straightforward interpretation, contrasting estimates from both direct and indirect evidence. However, node-splitting analyses are labour-intensive because each comparison of interest requires a separate model. It would be advantageous if node-splitting models could be estimated automatically for all comparisons of interest. We present an unambiguous decision rule to choose which comparisons to split, and prove that it selects only comparisons in potentially inconsistent loops in the network, and that all potentially inconsistent loops in the network are investigated. Moreover, the decision rule circumvents problems with the parameterisation of multi-arm trials, ensuring that model generation is trivial in all cases. Thus, our methods eliminate most of the manual work involved in using the node-splitting approach, enabling the analyst to focus on interpreting the results."," Algorithms; Automation; Bayes Theorem; *Clinical Trials as Topic; Decision Making; Electronic Data Processing; Humans; Models, Statistical; *Network Meta-Analysis; Programming Languages; Research Design; Statistics as Topic; Bayesian modelling; meta-analysis; mixed treatment comparisons; model generation; network meta-analysis; node splitting",1,,
228,Carrell,2015,International journal of medical informatics,Using natural language processing to identify problem usage of prescription opioids,"BACKGROUND: Accurate and scalable surveillance methods are critical to understand widespread problems associated with misuse and abuse of prescription opioids and for implementing effective prevention and control measures. Traditional diagnostic coding incompletely documents problem use. Relevant information for each patient is often obscured in vast amounts of clinical text. OBJECTIVES: We developed and evaluated a method that combines natural language processing (NLP) and computer-assisted manual review of clinical notes to identify evidence of problem opioid use in electronic health records (EHRs). METHODS: We used the EHR data and text of 22,142 patients receiving chronic opioid therapy (>/=70 days' supply of opioids per calendar quarter) during 2006-2012 to develop and evaluate an NLP-based surveillance method and compare it to traditional methods based on International Classification of Disease, Ninth Edition (ICD-9) codes. We developed a 1288-term dictionary for clinician mentions of opioid addiction, abuse, misuse or overuse, and an NLP system to identify these mentions in unstructured text. The system distinguished affirmative mentions from those that were negated or otherwise qualified. We applied this system to 7336,445 electronic chart notes of the 22,142 patients. Trained abstractors using a custom computer-assisted software interface manually reviewed 7751 chart notes (from 3156 patients) selected by the NLP system and classified each note as to whether or not it contained textual evidence of problem opioid use. RESULTS: Traditional diagnostic codes for problem opioid use were found for 2240 (10.1%) patients. NLP-assisted manual review identified an additional 728 (3.1%) patients with evidence of clinically diagnosed problem opioid use in clinical notes. Inter-rater reliability among pairs of abstractors reviewing notes was high, with kappa=0.86 and 97% agreement for one pair, and kappa=0.71 and 88% agreement for another pair. CONCLUSIONS: Scalable, semi-automated NLP methods can efficiently and accurately identify evidence of problem opioid use in vast amounts of EHR text. Incorporating such methods into surveillance efforts may increase prevalence estimates by as much as one-third relative to traditional methods."," Data Mining/methods; Drug Prescriptions/statistics & numerical data; Electronic Health Records/*statistics & numerical data; Humans; Inappropriate Prescribing/*statistics & numerical data; Narcotic Antagonists/*therapeutic use; Natural Language Processing; Opioid-Related Disorders/*epidemiology/prevention & control; Pain/*epidemiology/*prevention & control; Pattern Recognition, Automated/methods; Prevalence; Risk Factors; Vocabulary, Controlled; Washington/epidemiology; Computer-assisted records review; Opioid-related disorders; Surveillance",-2,,
229,Serghiou,2016,Journal of clinical epidemiology,Field-wide meta-analyses of observational associations can map selective availability of risk factors and the impact of model specifications,"OBJECTIVES: Instead of evaluating one risk factor at a time, we illustrate the utility of ""field-wide meta-analyses"" in considering all available data on all putative risk factors of a disease simultaneously. STUDY DESIGN AND SETTING: We identified studies on putative risk factors of pterygium (surfer's eye) in PubMed, EMBASE, and Web of Science. We mapped which factors were considered, reported, and adjusted for in each study. For each putative risk factor, four meta-analyses were done using univariate only, multivariate only, preferentially univariate, or preferentially multivariate estimates. RESULTS: A total of 2052 records were screened to identify 60 eligible studies reporting on 65 putative risk factors. Only 4 of 60 studies reported both multivariate and univariate regression analyses. None of the 32 studies using multivariate analysis adjusted for the same set of risk factors. Effect sizes from different types of regression analyses led to significantly different summary effect sizes (P-value < 0.001). Observed heterogeneity was very high for both multivariate (median I(2), 76.1%) and univariate (median I(2), 85.8%) estimates. No single study investigated all 11 risk factors that were statistically significant in at least one of our meta-analyses. CONCLUSION: Field-wide meta-analyses can map availability of risk factors and trends in modeling, adjustments and reporting, as well as the impact of differences in model specification."," Humans; *Models, Theoretical; Observational Studies as Topic/*statistics & numerical data; Research Design/*statistics & numerical data; Risk Factors; Big data; Exposome-wide association study; Meta-analysis; Observational study; Risk factor epidemiology; Statistical modeling",-2,,
230,Swain,2015,International journal of medical informatics,Feasibility of 30-day hospital readmission prediction modeling based on health information exchange data,"INTRODUCTION: Unplanned 30-day hospital readmission account for roughly $17 billion in annual Medicare spending. Many factors contribute to unplanned hospital readmissions and multiple models have been developed over the years to predict them. Most researchers have used insurance claims or administrative data to train and operationalize their Readmission Risk Prediction Models (RRPMs). Some RRPM developers have also used electronic health records data; however, using health informatics exchange data has been uncommon among such predictive models and can be beneficial in its ability to provide real-time alerts to providers at the point of care. METHODS: We conducted a semi-systematic review of readmission predictive factors published prior to March 2013. Then, we extracted and merged all significant variables listed in those articles for RRPMs. Finally, we matched these variables with common HL7 messages transmitted by a sample of health information exchange organizations (HIO). RESULTS: The semi-systematic review resulted in identification of 32 articles and 297 predictive variables. The mapping of these variables with common HL7 segments resulted in an 89.2% total coverage, with the DG1 (diagnosis) segment having the highest coverage of 39.4%. The PID (patient identification) and OBX (observation results) segments cover 13.9% and 9.1% of the variables. Evaluating the same coverage in three sample HIOs showed data incompleteness. DISCUSSION: HIOs can utilize HL7 messages to develop unique RRPMs for their stakeholders; however, data completeness of exchanged messages should meet certain thresholds. If data quality standards are met by stakeholders, HIOs would be able to provide real-time RRPMs that not only predict intra-hospital readmissions but also inter-hospital cases. CONCLUSION: A RRPM derived using HIO data exchanged through may prove to be a useful method to prevent unplanned hospital readmissions. In order for the RRPM derived from HIO data to be effective, hospitals must actively exchange clinical information through the HIO and develop actionable methods that integrate into the workflow of providers to ensure that patients at high-risk for readmission receive the care they need."," Computer Simulation; Data Mining/*methods; Feasibility Studies; Health Information Exchange/classification/*statistics & numerical data; Humans; *Models, Statistical; *Natural Language Processing; Patient Readmission/*statistics & numerical data; Pattern Recognition, Automated/methods; Prevalence; Reproducibility of Results; Sensitivity and Specificity; Vocabulary, Controlled; Health information exchange; Health information organization; Health information technology; Hospital readmissions; Risk prediction model",-2,,
231,Sigdel,2015,PloS one,A Computational Gene Expression Score for Predicting Immune Injury in Renal Allografts,"BACKGROUND: Whole genome microarray meta-analyses of 1030 kidney, heart, lung and liver allograft biopsies identified a common immune response module (CRM) of 11 genes that define acute rejection (AR) across different engrafted tissues. We evaluated if the CRM genes can provide a molecular microscope to quantify graft injury in acute rejection (AR) and predict risk of progressive interstitial fibrosis and tubular atrophy (IFTA) in histologically normal kidney biopsies. METHODS: Computational modeling was done on tissue qPCR based gene expression measurements for the 11 CRM genes in 146 independent renal allografts from 122 unique patients with AR (n = 54) and no-AR (n = 92). 24 demographically matched patients with no-AR had 6 and 24 month paired protocol biopsies; all had histologically normal 6 month biopsies, and 12 had evidence of progressive IFTA (pIFTA) on their 24 month biopsies. Results were correlated with demographic, clinical and pathology variables. RESULTS: The 11 gene qPCR based tissue CRM score (tCRM) was significantly increased in AR (5.68 +/- 0.91) when compared to STA (1.29 +/- 0.28; p < 0.001) and pIFTA (7.94 +/- 2.278 versus 2.28 +/- 0.66; p = 0.04), with greatest significance for CXCL9 and CXCL10 in AR (p <0.001) and CD6 (p<0.01), CXCL9 (p<0.05), and LCK (p<0.01) in pIFTA. tCRM was a significant independent correlate of biopsy confirmed AR (p < 0.001; AUC of 0.900; 95% CI = 0.705-903). Gene expression modeling of 6 month biopsies across 7/11 genes (CD6, INPP5D, ISG20, NKG7, PSMB9, RUNX3, and TAP1) significantly (p = 0.037) predicted the development of pIFTA at 24 months. CONCLUSIONS: Genome-wide tissue gene expression data mining has supported the development of a tCRM-qPCR based assay for evaluating graft immune inflammation. The tCRM score quantifies injury in AR and stratifies patients at increased risk of future pIFTA prior to any perturbation of graft function or histology."," Acute Kidney Injury/*immunology; Adolescent; Adult; Allografts; Child; *Computer Simulation; Female; *Gene Expression Regulation; Graft Rejection/*immunology; Humans; *Kidney Transplantation; Male; *Models, Immunological",-2,,
232,Park,2015,Journal of medical Internet research,Automatically Detecting Failures in Natural Language Processing Tools for Online Community Text,"BACKGROUND: The prevalence and value of patient-generated health text are increasing, but processing such text remains problematic. Although existing biomedical natural language processing (NLP) tools are appealing, most were developed to process clinician- or researcher-generated text, such as clinical notes or journal articles. In addition to being constructed for different types of text, other challenges of using existing NLP include constantly changing technologies, source vocabularies, and characteristics of text. These continuously evolving challenges warrant the need for applying low-cost systematic assessment. However, the primarily accepted evaluation method in NLP, manual annotation, requires tremendous effort and time. OBJECTIVE: The primary objective of this study is to explore an alternative approach-using low-cost, automated methods to detect failures (eg, incorrect boundaries, missed terms, mismapped concepts) when processing patient-generated text with existing biomedical NLP tools. We first characterize common failures that NLP tools can make in processing online community text. We then demonstrate the feasibility of our automated approach in detecting these common failures using one of the most popular biomedical NLP tools, MetaMap. METHODS: Using 9657 posts from an online cancer community, we explored our automated failure detection approach in two steps: (1) to characterize the failure types, we first manually reviewed MetaMap's commonly occurring failures, grouped the inaccurate mappings into failure types, and then identified causes of the failures through iterative rounds of manual review using open coding, and (2) to automatically detect these failure types, we then explored combinations of existing NLP techniques and dictionary-based matching for each failure cause. Finally, we manually evaluated the automatically detected failures. RESULTS: From our manual review, we characterized three types of failure: (1) boundary failures, (2) missed term failures, and (3) word ambiguity failures. Within these three failure types, we discovered 12 causes of inaccurate mappings of concepts. We used automated methods to detect almost half of 383,572 MetaMap's mappings as problematic. Word sense ambiguity failure was the most widely occurring, comprising 82.22% of failures. Boundary failure was the second most frequent, amounting to 15.90% of failures, while missed term failures were the least common, making up 1.88% of failures. The automated failure detection achieved precision, recall, accuracy, and F1 score of 83.00%, 92.57%, 88.17%, and 87.52%, respectively. CONCLUSIONS: We illustrate the challenges of processing patient-generated online health community text and characterize failures of NLP tools on this patient-generated health text, demonstrating the feasibility of our low-cost approach to automatically detect those failures. Our approach shows the potential for scalable and effective solutions to automatically assess the constantly evolving NLP tools and source vocabularies to process patient-generated text.", *Electronic Data Processing; Humans; *Internet; *Natural Language Processing; Umls; automatic data processing; information extraction; natural language processing; quantitative evaluation,-1,,
233,Aldridge,2015,PloS one,Accuracy of Probabilistic Linkage Using the Enhanced Matching System for Public Health and Epidemiological Studies,"BACKGROUND: The Enhanced Matching System (EMS) is a probabilistic record linkage program developed by the tuberculosis section at Public Health England to match data for individuals across two datasets. This paper outlines how EMS works and investigates its accuracy for linkage across public health datasets. METHODS: EMS is a configurable Microsoft SQL Server database program. To examine the accuracy of EMS, two public health databases were matched using National Health Service (NHS) numbers as a gold standard unique identifier. Probabilistic linkage was then performed on the same two datasets without inclusion of NHS number. Sensitivity analyses were carried out to examine the effect of varying matching process parameters. RESULTS: Exact matching using NHS number between two datasets (containing 5931 and 1759 records) identified 1071 matched pairs. EMS probabilistic linkage identified 1068 record pairs. The sensitivity of probabilistic linkage was calculated as 99.5% (95%CI: 98.9, 99.8), specificity 100.0% (95%CI: 99.9, 100.0), positive predictive value 99.8% (95%CI: 99.3, 100.0), and negative predictive value 99.9% (95%CI: 99.8, 100.0). Probabilistic matching was most accurate when including address variables and using the automatically generated threshold for determining links with manual review. CONCLUSION: With the establishment of national electronic datasets across health and social care, EMS enables previously unanswerable research questions to be tackled with confidence in the accuracy of the linkage process. In scenarios where a small sample is being matched into a very large database (such as national records of hospital attendance) then, compared to results presented in this analysis, the positive predictive value or sensitivity may drop according to the prevalence of matches between databases. Despite this possible limitation, probabilistic linkage has great potential to be used where exact matching using a common identifier is not possible, including in low-income settings, and for vulnerable groups such as homeless populations, where the absence of unique identifiers and lower data quality has historically hindered the ability to identify individuals across datasets."," Databases, Factual; *Electronic Health Records; England/epidemiology; *Epidemiologic Studies; Humans; Medical Record Linkage; *Public Health; Tuberculosis/*epidemiology",-1,,
234,Newe,2015,PloS one,Towards a Computable Data Corpus of Temporal Correlations between Drug Administration and Lab Value Changes,"BACKGROUND: The analysis of electronic health records for an automated detection of adverse drug reactions is an approach to solve the problems that arise from traditional methods like spontaneous reporting or manual chart review. Algorithms addressing this task should be modeled on the criteria for a standardized case causality assessment defined by the World Health Organization. One of these criteria is the temporal relationship between drug intake and the occurrence of a reaction or a laboratory test abnormality. Appropriate data that would allow for developing or validating related algorithms is not publicly available, though. METHODS: In order to provide such data, retrospective routine data of drug administrations and temporally corresponding laboratory observations from a university clinic were extracted, transformed and evaluated by experts in terms of a reasonable time relationship between drug administration and lab value alteration. RESULT: The result is a data corpus of 400 episodes of normalized laboratory parameter values in temporal context with drug administrations. Each episode has been manually classified whether it contains data that might indicate a temporal correlation between the drug administration and the change of the lab value course, whether such a change is not observable or whether a decision between those two options is not possible due to the data. In addition, each episode has been assigned a concordance value which indicates how difficult it is to assess. This is the first open data corpus of a computable ground truth of temporal correlations between drug administration and lab value alterations. DISCUSSION: The main purpose of this data corpus is the provision of data for further research and the provision of a ground truth which allows for comparing the outcome of other assessments of this data with the outcome of assessments made by human experts. It can serve as a contribution towards systematic, computerized ADR detection in retrospective data. With this lab value curve data as a basis, algorithms for detecting temporal relationships can be developed, and with the classification made by human experts, these algorithms can immediately be validated. Due to the normalization of the lab value data, it allows for a generic approach rather than for specific or solitary drug/lab value combinations.", Adverse Drug Reaction Reporting Systems/*statistics & numerical data/trends; Algorithms; Drug-Related Side Effects and Adverse Reactions/*epidemiology; *Electronic Health Records; Humans; Laboratories; World Health Organization,-2,,
235,Chung,2015,Journal of medical Internet research,More Than Telemonitoring: Health Provider Use and Nonuse of Life-Log Data in Irritable Bowel Syndrome and Weight Management,"BACKGROUND: The quantified self, self-monitoring or life-logging movement is a trend to incorporate technology into data acquisition on aspects of a person's daily life in terms of inputs (eg food consumed), states (eg mood), and performance (mental and physical). Consumer self-monitoring mobile phone apps have been widely studied and used to promote healthy behavior changes. Data collected through life-logging apps also have the potential to support clinical care. OBJECTIVE: We sought to develop an in-depth understanding of providers' facilitators and barriers to successfully integrating life-log data into their practices and creating better experiences. We specifically investigated three research questions: How do providers currently use patient-collected life-log data in clinical practice? What are provider concerns and needs with respect to this data? What are the constraints for providers to integrate this type of data into their workflows? METHODS: We interviewed 21 health care providers-physicians, dietitians, a nurse practitioner, and a behavioral psychologist-who work with obese and irritable bowel syndrome patients. We transcribed and analyzed interviews according to thematic analysis and an affinity diagramming process. RESULTS: Providers reported using self-monitoring data to enhance provider-patient communication, develop personalized treatment plans, and to motivate and educate patients, in addition to using them as diagnostic and adherence tools. However, limitations associated with current systems and workflows create barriers to regular and effective review of this data. These barriers include a lack of time to review detailed records, questions about providers' expertise to review it, and skepticism about additional benefits offered by reviewing data. Current self-monitoring tools also often lack flexibility, standardized formats, and mechanisms to share data with providers. CONCLUSIONS: Variations in provider needs affect tracking and reviewing needs. Systems to support diagnosis might require better reliability and resolution, while systems to support interaction should support collaborative reflection and communication. Automatic synthesis of data logs could help providers focus on educational goals while communication of contextual information might help providers better understand patient values. We also discuss how current mobile apps and provider systems do, and do not, support these goals, and future design opportunities to realize the potential benefits of using life-logging tools in clinical care."," *Attitude of Health Personnel; Communication; Data Collection; *Health Records, Personal; Humans; *Irritable Bowel Syndrome; *Mobile Applications; *Obesity; Professional-Patient Relations; Reproducibility of Results; behavioral self-monitoring; chronic disease; clinical care; health; life logs; personal informatics; quantified self; wellness",-2,,
236,Burzagli,2015,Studies in health technology and informatics,Open Ambient Intelligence Environments,"The present impact of ambient intelligence concepts in eInclusion is first briefly reviewed. Suggestions and examples of how ambient intelligent environments should be specified, designed and used to favour independent living of people with activity limitations are presented.", *Artificial Intelligence; Expert Systems/*instrumentation; Humans; *Self-Help Devices; *User-Computer Interface,-2,,
237,Parrott,2015,Systematic reviews,Metacognition and evidence analysis instruction: an educational framework and practical experience,"The role of metacognitive skills in the evidence analysis process has received little attention in the research literature. While the steps of the evidence analysis process are well defined, the role of higher-level cognitive operations (metacognitive strategies) in integrating the steps of the process is not well understood. In part, this is because it is not clear where and how metacognition is implicated in the evidence analysis process nor how these skills might be taught. The purposes of this paper are to (a) suggest a model for identifying critical thinking and metacognitive skills in evidence analysis instruction grounded in current educational theory and research and (b) demonstrate how freely available systematic review/meta-analysis tools can be used to focus on higher-order metacognitive skills, while providing a framework for addressing common student weaknesses. The final goal of this paper is to provide an instructional framework that can generate critique and elaboration while providing the conceptual basis and rationale for future research agendas on this topic."," *Data Mining; Humans; Meta-Analysis as Topic; *Metacognition; Models, Biological; *Models, Educational; Research/*education; Review Literature as Topic; Statistics as Topic/*education; Teaching/*methods; *Thinking",-2,,
238,Hartling,2015,Journal of clinical epidemiology,A taxonomy of rapid reviews links report types and methods to specific decision-making contexts,"OBJECTIVES: Describe characteristics of rapid reviews and examine the impact of methodological variations on their reliability and validity. STUDY DESIGN AND SETTING: We conducted a literature review and interviews with organizations that produce rapid reviews or related products to identify methods, guidance, empiric evidence, and current practices. RESULTS: We identified 36 rapid products from 20 organizations (production time, 5 minutes to 8 months). Methods differed from systematic reviews at all stages. As time frames increased, methods became more rigorous; however, restrictions on database searching, inclusion criteria, data extracted, and independent dual review remained. We categorized rapid products based on extent of synthesis. ""Inventories"" list what evidence is available. ""Rapid responses"" present best available evidence with no formal synthesis. ""Rapid reviews"" synthesize the quality of and findings from the evidence. ""Automated approaches"" generate meta-analyses in response to user-defined queries. Rapid products rely on a close relationship with end users and support specific decisions in an identified time frame. Limited empiric evidence exists comparing rapid and systematic reviews. CONCLUSIONS: Rapid products have tremendous methodological variation; categorization based on time frame or type of synthesis reveals patterns. The similarity across rapid products lies in the close relationship with the end user to meet time-sensitive decision-making needs.", *Classification; Data Collection/statistics & numerical data; *Decision Making; Humans; Reproducibility of Results; *Review Literature as Topic; Automation; Interviews; Methodology; Rapid reviews; Stakeholders; Systematic reviews,2,,
239,Peek,2015,Artificial intelligence in medicine,Thirty years of artificial intelligence in medicine (AIME) conferences: A review of research themes,"BACKGROUND: Over the past 30 years, the international conference on Artificial Intelligence in MEdicine (AIME) has been organized at different venues across Europe every 2 years, establishing a forum for scientific exchange and creating an active research community. The Artificial Intelligence in Medicine journal has published theme issues with extended versions of selected AIME papers since 1998. OBJECTIVES: To review the history of AIME conferences, investigate its impact on the wider research field, and identify challenges for its future. METHODS: We analyzed a total of 122 session titles to create a taxonomy of research themes and topics. We classified all 734 AIME conference papers published between 1985 and 2013 with this taxonomy. We also analyzed the citations to these conference papers and to 55 special issue papers. RESULTS: We identified 30 research topics across 12 themes. AIME was dominated by knowledge engineering research in its first decade, while machine learning and data mining prevailed thereafter. Together these two themes have contributed about 51% of all papers. There have been eight AIME papers that were cited at least 10 times per year since their publication. CONCLUSIONS: There has been a major shift from knowledge-based to data-driven methods while the interest for other research themes such as uncertainty management, image and signal processing, and natural language processing has been stable since the early 1990s. AIME papers relating to guidelines and protocols are among the most highly cited."," Artificial Intelligence/*statistics & numerical data; Congresses as Topic/*statistics & numerical data; Data Mining/statistics & numerical data; Europe; Humans; Image Processing, Computer-Assisted/instrumentation; Machine Learning/statistics & numerical data; Medicine; Signal Processing, Computer-Assisted/instrumentation; Uncertainty; Artificial Intelligence in Medicine; History of science; Literature review",-2,,
240,Sittig,2015,Studies in health technology and informatics,Developing an Open-Source Bibliometric Ranking Website Using Google Scholar Citation Profiles for Researchers in the Field of Biomedical Informatics,"We developed the Biomedical Informatics Researchers ranking website (rank.informatics-review.com) to overcome many of the limitations of previous scientific productivity ranking strategies. The website is composed of four key components that work together to create an automatically updating ranking website: (1) list of biomedical informatics researchers, (2) Google Scholar scraper, (3) display page, and (4) updater. The site has been useful to other groups in evaluating researchers, such as tenure and promotions committees in interpreting the various citation statistics reported by candidates. Creation of the Biomedical Informatics Researchers ranking website highlights the vast differences in scholarly productivity among members of the biomedical informatics research community.", *Bibliometrics; Biomedical Research; Internet/*organization & administration; *Journal Impact Factor; Medical Informatics/*organization & administration; Search Engine/*methods; *Software; User-Computer Interface,1,,
241,Khodambashi,2015,Studies in health technology and informatics,Lessons Learnt from Evaluation of Computer Interpretable Clinical Guidelines Tools and Methods: Literature Review,"Representation of clinical guidelines in a computer interpretable format is an active area of research. Various methods and tools have been proposed which have been evaluated based on different evaluation criteria. The evaluation results in the literature and their lessons learnt can be a valuable learning resource in order to redesign and improve the tools. Therefore, this research investigates the lessons learnt from the evaluation studies. Broad search in literature together with a purposeful snowball method were performed to identify the related papers that report any type of evaluation or comparison. We reviewed and analysed the lessons learnt from the evaluation results and classified them into 17 themes which reflect the suggestion concerns. The results indicate that the lessons learnt are more focused on tool functionalities, integration, sharing and maintenance domain. We provide suggestions for the area which had less attention.", Humans; *Natural Language Processing; *Practice Guidelines as Topic,1,,
242,Khodambashi,2015,Studies in health technology and informatics,Computer-Interpretable Clinical Guidelines: A Review and Analysis of Evaluation Criteria for Authoring Methods,"There are a variety of authoring tools and methods for producing computer-interpretable clinical guideline (CIG). This work is a review of the evaluation of tools and methods currently in use to author CIGs. The aim of this paper is to present the results of a literature review on the evaluation criteria. Both controlled database search and a subsequent snowballing were used to identify relevant literature. The evaluation criteria and evaluation methods of CIG-related themes were manually identified in the found literature. Based on the 32 relevant papers found, 68 evaluation criteria were identified which were then classified into ten themes. We identified the most and least frequently mentioned areas of concern in evaluation which indicate areas that have been neglected in system evaluation.", Humans; *Natural Language Processing; *Practice Guidelines as Topic/standards,-1,,
243,Kury,2015,Studies in health technology and informatics,Identifying Repetitive Institutional Review Board Stipulations by Natural Language Processing and Network Analysis,"The corrections (""stipulations"") to a proposed research study protocol produced by an institutional review board (IRB) can often be repetitive across many studies; however, there is no standard set of stipulations that could be used, for example, by researchers wishing to anticipate and correct problems in their research proposals prior to submitting to an IRB. The objective of the research was to computationally identify the most repetitive types of stipulations generated in the course of IRB deliberations. The text of each stipulation was normalized using the natural language processing techniques. An undirected weighted network was constructed in which each stipulation was represented by a node, and each link, if present, had weight corresponding to the TF-IDF Cosine Similarity of the stipulations. Network analysis software was then used to identify clusters in the network representing similar stipulations. The final results were correlated with additional data to produce further insights about the IRB workflow. From a corpus of 18,582 stipulations we identified 31 types of repetitive stipulations. Those types accounted for 3,870 stipulations (20.8% of the corpus) produced for 697 (88.7%) of all protocols in 392 (also 88.7%) of all the CNS IRB meetings with stipulations entered in our data source. A notable peroportion of the corrections produced by the IRB can be considered highly repetitive. Our shareable method relied on a minimal manual analysis and provides an intuitive exploration with theoretically unbounded granularity. Finer granularity allowed for the insight that is anticipated to prevent the need for identifying the IRB panel expertise or any human supervision."," Biomedical Research/classification/*statistics & numerical data; Data Mining/*methods; Documentation/*statistics & numerical data; Ethics Committees, Research/*statistics & numerical data; Machine Learning; *Natural Language Processing; Research Design/statistics & numerical data; *Vocabulary, Controlled",-2,,
244,Balas,2015,Studies in health technology and informatics,"Big Data Clinical Research: Validity, Ethics, and Regulation","Electronic Health Records (EHR) promise improvement for patient care and also offer great value for biomedical research including clinical, public health, and health services research. Unfortunately, the full potential of EHR big data research has remained largely unrealized. The purpose of this study was to identify rate limiting factors, and develop recommendations to better balance unrestricted extramural EHR access with legitimate safeguarding of EHR data in retrospective research. By exploring primary, secondary, and tertiary sources, this review identifies external constraints and provides a comparative analysis of social influencers in retrospective EHR-based research. Results indicate that EHRs have the advantage of reflecting the reality of patient care but also show a frequency of between 4.3-86% of incomplete and inaccurate data in various fields. The rapid spread of alternative analytics for health data challenges traditional interpretations of confidentiality protections. A confusing multiplicity of controls creates barriers to big data EHR research. More research on the use of EHR big data is likely to improve accuracy and validity. Information governance and research approval processes should be simplified. Comprehensive regulatory policies that do not exclusively cover health care entities, are needed. Finally, new computing safeguards are needed to address public concerns, like research access only to aggregate data and not to individually identifiable information.", Confidentiality/*ethics/*legislation & jurisprudence; Datasets as Topic/*ethics/*legislation & jurisprudence; Electronic Health Records/*ethics/*legislation & jurisprudence/statistics &; numerical data; Government Regulation; Health Services Research/ethics/legislation & jurisprudence; Internationality; Needs Assessment,-2,,
245,Motiwala,2015,Studies in health technology and informatics,Domain Analysis of Integrated Data to Reduce Cost Associated with Liver Disease,"Liver cancer, the fifth most common cancer and second leading cause of cancer-related death among men worldwide, is plagued by not only lack of clinical research, but informatics tools for early detection. Consequently, it presents a major health and cost burden. Among the different types of liver cancer, hepatocellular carcinoma (HCC) is the most common and deadly form, arising from underlying liver disease. Current models for predicting risk of HCC and liver disease are limited to clinical data. A domain analysis of existing research related to screening for HCC and liver disease suggests that metabolic syndrome (MetS) may present oppportunites to detect early signs of liver disease. The purpose of this paper is to (i) provide a domain analysis of the relationship between HCC, liver disease, and metabolic syndrome, (ii) a review of the current disparate sources of data available for MetS diagnosis, and (iii) recommend informatics solutions for the diagnosis of MetS from available administrative (Biometrics, PHA, claims) and laboratory data, towards early prediction of liver disease. Our domain analysis and recommendations incorporate best practices to make meaningful use of available data with the goal of reducing cost associated with liver disease."," Carcinoma, Hepatocellular/diagnosis/*economics/epidemiology; Causality; Cost Control/economics/methods; Data Mining/*methods; Early Detection of Cancer/*economics/methods; Electronic Health Records/statistics & numerical data; Health Care Costs/*statistics & numerical data; Humans; Liver Neoplasms/diagnosis/*economics/epidemiology; Metabolic Syndrome/diagnosis/*economics/epidemiology; Prevalence; Risk Assessment/methods; Systems Integration; United States/epidemiology",-2,,
246,Kim,2015,Studies in health technology and informatics,Web-based Self-management Support Interventions for Cancer Survivors: A Systematic Review and Meta-analyses,"Those who are living with cancer as a chronic disease need to self-manage the late and long-term effects of cancer and its treatment. We conducted systematic searches of English-language peer-reviewed publications in PubMed, Cumulative Index for Nursing and Allied Health Literature, Cochrane Central Register of Controlled Trials, and EMBASE between January 2000 and June 2014. We searched for web-based interventions designed to help cancer survivors manage their symptoms and the side effects of cancer treatments, which yielded 37 studies that were systematically reviewed. For the meta-analyses, five articles were selected for fatigue, seven for depression, five for anxiety, and five for overall quality of life. The most popular mode of intervention delivery was ""peer-to-peer access"" in the communicative functions category, followed by ""the use of an enriched information environment"" in the automated functions category. The effectss across all outcome measures were small to moderate compared to standard care. Healthcare providers could use information technologies to support self-management among cancer survivors based on their needs across the cancer care continuum.", Health Information Systems/*statistics & numerical data; Health Promotion/statistics & numerical data; Humans; Internet/*statistics & numerical data; Neoplasms/diagnosis/*epidemiology/*therapy; Prevalence; Self Care/*statistics & numerical data; Survivors/*statistics & numerical data,-2,,
247,Virginio,2015,Studies in health technology and informatics,Identification of Patient Safety Risks Associated with Electronic Health Records: A Software Quality Perspective,"Although Electronic Health Records (EHR) can offer benefits to the health care process, there is a growing body of evidence that these systems can also incur risks to patient safety when developed or used improperly. This work is a literature review to identify these risks from a software quality perspective. Therefore, the risks were classified based on the ISO/IEC 25010 software quality model. The risks identified were related mainly to the characteristics of ""functional suitability"" (i.e., software bugs) and ""usability"" (i.e., interface prone to user error). This work elucidates the fact that EHR quality problems can adversely affect patient safety, resulting in errors such as incorrect patient identification, incorrect calculation of medication dosages, and lack of access to patient data. Therefore, the risks presented here provide the basis for developers and EHR regulating bodies to pay attention to the quality aspects of these systems that can result in patient harm.", *Data Accuracy; Data Mining/*statistics & numerical data; Electronic Health Records/*statistics & numerical data; Humans; Meaningful Use/*statistics & numerical data; Medical Errors/prevention & control/*statistics & numerical data; Patient Safety/*statistics & numerical data,-2,,
248,Gu,2015,Studies in health technology and informatics,Automatic Detection of Skin and Subcutaneous Tissue Infections from Primary Care Electronic Medical Records,"INTRODUCTION: Skin and subcutaneous tissue infections (SSTI) are common conditions that cause avoidable hospitalisation in New Zealand. As part of a program to improve the management of SSTI in primary care, electronic medical records (EMR) of four Auckland general practices were analysed to identify SSTI occurrences in the last three years. METHODS: An ontology for SSTI risks, manifestation and treatment was created based on literature and guidelines. An SSTI identification algorithm was developed examining EMR data for skin swab tests, diagnoses (READ codes) and textual clinical notes. RESULTS: High occurrence and recurrence rates in those aged 20 or younger were found. Due to low usage of READ coding and laboratory tests, 65% of SSTI occurrences were identified by notes. However, 91% of all identified SSTI occurrences were appropriately treated with oral/topical antibiotics according to prescription records in the EMR. The F1 score of the analysis algorithm is 0.76 using manual review as gold standard. DISCUSSION AND CONCLUSION: The SSTI identification algorithm shows a reasonable accuracy suggesting the feasibility of automatic detecting SSTI occurrences using clinical data that are routinely collected in healthcare delivery."," Data Mining/methods; Diagnosis, Computer-Assisted/*methods; Electronic Health Records/*classification; Humans; *Machine Learning; *Natural Language Processing; New Zealand; Primary Health Care/*classification; Reproducibility of Results; Sensitivity and Specificity; Skin Diseases, Bacterial/*diagnosis; Subcutaneous Tissue/pathology; Vocabulary, Controlled",,,
249,Duggirala,2016,Journal of the American Medical Informatics Association : JAMIA,Use of data mining at the Food and Drug Administration,"OBJECTIVES: This article summarizes past and current data mining activities at the United States Food and Drug Administration (FDA). TARGET AUDIENCE: We address data miners in all sectors, anyone interested in the safety of products regulated by the FDA (predominantly medical products, food, veterinary products and nutrition, and tobacco products), and those interested in FDA activities. SCOPE: Topics include routine and developmental data mining activities, short descriptions of mined FDA data, advantages and challenges of data mining at the FDA, and future directions of data mining at the FDA."," *Data Mining/statistics & numerical data; Pharmacovigilance; *Product Surveillance, Postmarketing; United States; *United States Food and Drug Administration; data mining; disproportionality analysis",2,,
250,Lardon,2015,Journal of medical Internet research,Adverse Drug Reaction Identification and Extraction in Social Media: A Scoping Review,"BACKGROUND: The underreporting of adverse drug reactions (ADRs) through traditional reporting channels is a limitation in the efficiency of the current pharmacovigilance system. Patients' experiences with drugs that they report on social media represent a new source of data that may have some value in postmarketing safety surveillance. OBJECTIVE: A scoping review was undertaken to explore the breadth of evidence about the use of social media as a new source of knowledge for pharmacovigilance. METHODS: Daubt et al's recommendations for scoping reviews were followed. The research questions were as follows: How can social media be used as a data source for postmarketing drug surveillance? What are the available methods for extracting data? What are the different ways to use these data? We queried PubMed, Embase, and Google Scholar to extract relevant articles that were published before June 2014 and with no lower date limit. Two pairs of reviewers independently screened the selected studies and proposed two themes of review: manual ADR identification (theme 1) and automated ADR extraction from social media (theme 2). Descriptive characteristics were collected from the publications to create a database for themes 1 and 2. RESULTS: Of the 1032 citations from PubMed and Embase, 11 were relevant to the research question. An additional 13 citations were added after further research on the Internet and in reference lists. Themes 1 and 2 explored 11 and 13 articles, respectively. Ways of approaching the use of social media as a pharmacovigilance data source were identified. CONCLUSIONS: This scoping review noted multiple methods for identifying target data, extracting them, and evaluating the quality of medical information from social media. It also showed some remaining gaps in the field. Studies related to the identification theme usually failed to accurately assess the completeness, quality, and reliability of the data that were analyzed from social media. Regarding extraction, no study proposed a generic approach to easily adding a new site or data source. Additional studies are required to precisely determine the role of social media in the pharmacovigilance system.", Drug-Related Side Effects and Adverse Reactions/*diagnosis; Humans; Internet/*statistics & numerical data; Pharmacovigilance; Reproducibility of Results; Social Media/*standards; Internet; Web 2.0; adverse drug reaction; adverse event; scoping review; social media; text mining,-2,,
251,Raja Ikram,2015,International journal of medical informatics,An analysis of application of health informatics in Traditional Medicine: A review of four Traditional Medicine Systems,"OBJECTIVE: This paper shall first investigate the informatics areas and applications of the four Traditional Medicine systems - Traditional Chinese Medicine (TCM), Ayurveda, Traditional Arabic and Islamic Medicine and Traditional Malay Medicine. Then, this paper shall examine the national informatics infrastructure initiatives in the four respective countries that support the Traditional Medicine systems. Challenges of implementing informatics in Traditional Medicine Systems shall also be discussed. METHODS: The literature was sourced from four databases: Ebsco Host, IEEE Explore, Proquest and Google scholar. The search term used was ""Traditional Medicine"", ""informatics"", ""informatics infrastructure"", ""traditional Chinese medicine"", ""Ayurveda"", ""traditional Arabic and Islamic medicine"", and ""traditional malay medicine"". A combination of the search terms above was also executed to enhance the searching process. A search was also conducted in Google to identify miscellaneous books, publications, and organization websites using the same terms. RESULTS: Amongst major advancements in TCM and Ayurveda are bioinformatics, development of Traditional Medicine databases for decision system support, data mining and image processing. Traditional Chinese Medicine differentiates itself from other Traditional Medicine systems with documented ISO Standards to support the standardization of TCM. Informatics applications in Traditional Arabic and Islamic Medicine are mostly ehealth applications that focus more on spiritual healing, Islamic obligations and prophetic traditions. Literature regarding development of health informatics to support Traditional Malay Medicine is still insufficient. Major informatics infrastructure that is common in China and India are automated insurance payment systems for Traditional Medicine treatment. National informatics infrastructure in Middle East and Malaysia mainly cater for modern medicine. Other infrastructure such as telemedicine and hospital information systems focus its implementation in modern medicine or are not implemented and strategized at a national level to support Traditional Medicine. CONCLUSION: Informatics may not be able to address all the emerging areas of Traditional Medicine because the concepts in Traditional Medicine system of medicine are different from modern system, though the aim may be same, i.e., to give relief to the patient. Thus, there is a need to synthesize Traditional Medicine systems and informatics with involvements from modern system of medicine. Future research works may include filling the gaps of informatics areas and integrate national informatics infrastructure with established Traditional Medicine systems."," China; Computational Biology; Databases, Factual; Decision Support Systems, Clinical; Hospital Information Systems; Humans; India; Insurance, Health; Malaysia; *Medical Informatics/economics/methods; *Medicine, Traditional/economics/methods; Middle East; Religion and Medicine; Telemedicine; Ayurveda; Health informatics; Traditional Arabic and Islamic Medicine; Traditional Chinese Medicine; Traditional Malay Medicine; Traditional Medicine",-2,,
252,Sawhney,2015,PloS one,Maximising Acute Kidney Injury Alerts--A Cross-Sectional Comparison with the Clinical Diagnosis,"BACKGROUND: Acute kidney injury (AKI) is serious and widespread across healthcare (1 in 7 hospital admissions) but recognition is often delayed causing avoidable harm. Nationwide automated biochemistry alerts for AKI stages 1-3 have been introduced in England to improve recognition. We explored how these alerts compared with clinical diagnosis in different hospital settings. METHODS: We used a large population cohort of 4464 patients with renal impairment. Each patient had case-note review by a nephrologist, using RIFLE criteria to diagnose AKI and chronic kidney disease (CKD). We identified and staged AKI alerts using the new national NHS England AKI algorithm and compared this with nephrologist diagnosis across hospital settings. RESULTS: Of 4464 patients, 525 had RIFLE AKI, 449 had mild AKI, 2185 had CKD (without AKI) and 1305 were of uncertain chronicity. NHS AKI algorithm criteria alerted for 90.5% of RIFLE AKI, 72.4% of mild AKI, 34.1% of uncertain cases and 14.0% of patients who actually had CKD.The algorithm identified AKI particularly well in intensive care (95.5%) and nephrology (94.6%), but less well on surgical wards (86.4%). Restricting the algorithm to stage 2 and 3 alerts reduced the over-diagnosis of AKI in CKD patients from 14.0% to 2.1%, but missed or delayed alerts in two-thirds of RIFLE AKI patients. CONCLUSION: Automated AKI detection performed well across hospital settings, but was less sensitive on surgical wards. Clinicians should be mindful that restricting alerts to stages 2-3 may identify fewer CKD patients, but including stage 1 provides more sensitive and timely alerting."," Acute Kidney Injury/*diagnosis; Aged; Algorithms; Creatinine/*blood; Critical Care/methods; Cross-Sectional Studies; Diagnosis, Computer-Assisted/*methods; Female; Hospital Mortality; Humans; Male; Renal Insufficiency, Chronic/diagnosis; Scotland; *Severity of Illness Index",-1,,
253,Marchione,2015,International journal of medical informatics,Approaches that use software to support the prevention of pressure ulcer: A systematic review,"CONTEXT: The incidence and costs for pressure ulcer (PU) treatment remain high even though preventive methods are applied. Approaches that use software to support the prevention of PU are presented in the literature to make it more effective. OBJECTIVES: Identify the state of art of the approaches that use software to support the prevention of PUs. METHODS: A systematic literature review was performed to analyze approaches that use software to support the prevention of PU. ACM, IEEE, PubMed, Scopus, CINAHL and Embase databases have been searched with a predetermined search string to identify primary studies. We selected the ones that met the established inclusion criteria. RESULTS: Thirty-six articles met the inclusion criteria. To support prevention, most approaches monitor the patient to provide information about exposure to pressure, temperature level, humidity level and estimated body position in bed providing risk factor intensity charts and intensity maps. The main method to perform patient's monitoring is using sensors installed on the mattress, but recently, alternative methods have been proposed such as electronic sensors and tactile sensory coils. Part of the approaches performs automated management of the risk factors using ventilation tubes and mattresses with porous cells to decrease body's temperature and movable cells to automatically redistribute the pressure over the body. Matters as cost of the approach, patient comfort and hygiene of the monitoring equipment is only briefly discussed in the selected articles. No experiments have been conducted to evidence the approached may reduce PU incidence. DISCUSSION AND CONCLUSION: Currently, approaches that use software to support the prevention of PU provide relevant information to health professionals such as risk factor intensity charts and intensity maps. Some of them can even automatically manage risk factors in a limited way. Yet, the approaches are based on risk factor monitoring methods that require patient's contact with the monitoring equipment. Therefore, some matters need to be considered such as patient's comfort and the hygiene or replacement of the equipment due to the risk of infection. With the emergence of new alternative methods of monitoring, new technologies that do not require contact could be explored by new researches. Randomized Control Trials could also be conducted to verify which approaches are really effective to reduce PU incidence."," Beds; Diagnosis, Computer-Assisted/instrumentation/*methods; Equipment Design; Equipment Failure Analysis; Humans; Manometry/*instrumentation/methods; Monitoring, Physiologic/*instrumentation/methods; Point-of-Care Systems; Pressure Ulcer/*diagnosis/*prevention & control; *Software; Technology Assessment, Biomedical; Bed sores; Pressure ulcers; Prevention; Software; Systematic review; Technology",-1,,
254,Marshall,2016,Journal of the American Medical Informatics Association : JAMIA,RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials,"OBJECTIVE: To develop and evaluate RobotReviewer, a machine learning (ML) system that automatically assesses bias in clinical trials. From a (PDF-formatted) trial report, the system should determine risks of bias for the domains defined by the Cochrane Risk of Bias (RoB) tool, and extract supporting text for these judgments. METHODS: We algorithmically annotated 12,808 trial PDFs using data from the Cochrane Database of Systematic Reviews (CDSR). Trials were labeled as being at low or high/unclear risk of bias for each domain, and sentences were labeled as being informative or not. This dataset was used to train a multi-task ML model. We estimated the accuracy of ML judgments versus humans by comparing trials with two or more independent RoB assessments in the CDSR. Twenty blinded experienced reviewers rated the relevance of supporting text, comparing ML output with equivalent (human-extracted) text from the CDSR. RESULTS: By retrieving the top 3 candidate sentences per document (top3 recall), the best ML text was rated more relevant than text from the CDSR, but not significantly (60.4% ML text rated 'highly relevant' v 56.5% of text from reviews; difference +3.9%, [-3.2% to +10.9%]). Model RoB judgments were less accurate than those from published reviews, though the difference was <10% (overall accuracy 71.0% with ML v 78.3% with CDSR). CONCLUSION: Risk of bias assessment may be automated with reasonable accuracy. Automatically identified text supporting bias assessment is of equal quality to the manually identified text in the CDSR. This technology could substantially reduce reviewer workload and expedite evidence syntheses."," *Algorithms; *Bias; *Clinical Trials as Topic; Data Mining; Databases as Topic; *Machine Learning; Natural Language Processing; Peer Review, Research/*methods; Review Literature as Topic; bias; randomized controlled trials as topic; systematic review",2,,
255,Waffenschmidt,2015,Research synthesis methods,Searches for randomized controlled trials of drugs in MEDLINE and EMBASE using only generic drug names compared with searches applied in current practice in systematic reviews,"BACKGROUND: It is unclear which terms should be included in bibliographic searches for randomized controlled trials (RCTs) of drugs, and identifying relevant drug terms can be extremely laborious. The aim of our analysis was to determine whether a bibliographic search using only the generic drug name produces sufficient results for the generation of informative systematic reviews (SRs). METHODS: We conducted a retrospective analysis of relevant references included in SRs of drugs. We determined the proportion of references identified by a simplified technique consisting of a systematic search for RCTs in MEDLINE and EMBASE via the search interface Ovid and using only the truncated generic drug name in all search fields. We calculated aggregated sensitivity and also evaluated the unidentified references. RESULTS: Forty-eight SRs contained 873 primary publications, of which we found 829 in MEDLINE and 757 in EMBASE (""gold standard""). The simplified search identified 823 of the 829 MEDLINE references (sensitivity 99.3%) and 754 of the 757 EMBASE references (99.6%). Ultimately, only three references could not be found by additional searches. CONCLUSION: Our findings indicate that when searching for RCTs of drugs in MEDLINE and EMBASE, a search using the truncated generic drug name in all fields produces sufficient results."," Data Mining/*methods; Databases, Bibliographic/*statistics & numerical data; Drug Evaluation; Drug Labeling/classification/statistics & numerical data; Drugs, Generic/*classification; MEDLINE/statistics & numerical data; *Natural Language Processing; Randomized Controlled Trials as Topic/*statistics & numerical data; *Review Literature as Topic; Vocabulary, Controlled; comprehensive literature search for SRs; generic drug name; information storage and retrieval; randomized controlled trials as topic; review literature as topic; search strategy",-2,,
256,Carroll,2015,Research synthesis methods,"Quality assessment of qualitative evidence for systematic review and synthesis: Is it meaningful, and if so, how should it be performed?","The critical appraisal and quality assessment of primary research are key stages in systematic review and evidence synthesis. These processes are driven by the need to determine how far the primary research evidence, singly and collectively, should inform findings and, potentially, practice recommendations. Quality assessment of primary qualitative research remains a contested area. This article reviews recent developments in the field charting a perceptible shift from whether such quality assessment should be conducted to how it might be performed. It discusses the criteria that are used in the assessment of quality and how the findings of the process are used in synthesis. It argues that recent research indicates that sensitivity analysis offers one potentially useful means for advancing this controversial issue."," *Data Accuracy; *Data Interpretation, Statistical; Data Mining/*standards; Evidence-Based Medicine/standards; *Meta-Analysis as Topic; Quality Control; *Research Design; *Review Literature as Topic; critical appraisal; qualitative evidence synthesis; quality assessment; systematic review",-2,,
257,Bayliss,2015,Research synthesis methods,Information retrieval for ecological syntheses,"Research syntheses are increasingly being conducted within the fields of ecology and environmental management. Information retrieval is crucial in any synthesis in identifying data for inclusion whilst potentially reducing biases in the dataset gathered, yet the nature of ecological information provides several challenges when compared with medicine that should be considered when planning and undertaking searches. We present ten recommendations for anyone considering undertaking information retrieval for ecological research syntheses that highlight the main differences with medicine and, if adopted, may help reduce biases in the dataset retrieved, increase search efficiency and improve reporting standards. They are as follows: (1) plan for information retrieval at an early stage, (2) identify and use sources of help, (3) clearly define the question to be addressed, (4) ensure that provisions for managing, recording and reporting the search are in place, (5) select an appropriate search type, (6) identify sources to be used, (7) identify limitations of the sources, (8) ensure that the search vocabulary is appropriate, (9) identify limits and filters that can help direct the search, and (10) test the strategy to ensure that it is realistic and manageable. These recommendations may be of value for other disciplines where search infrastructures are not yet sufficiently well developed."," Data Accuracy; *Data Interpretation, Statistical; Data Mining/*methods; Ecology/*methods; Evidence-Based Medicine/methods; *Meta-Analysis as Topic; *Research Design; *Review Literature as Topic; data retrieval; ecology; literature review; literature search; systematic review",-2,,
258,Lortie,2015,Research synthesis methods,How to critically read ecological meta-analyses,"Meta-analysis offers ecologists a powerful tool for knowledge synthesis. Albeit a form of review, it also shares many similarities with primary empirical research. Consequently, critical reading of meta-analyses incorporates criteria from both sets of approaches particularly because ecology is a discipline that embraces heterogeneity and broad methodologies. The most important issues in critically assessing a meta-analysis initially include transparency, replicability, and clear statement of purpose by the authors. Specific to ecology, more so than other disciplines, tests of the same hypothesis are generally conducted at different study sites, have variable ecological contexts (i.e., seasonality), and use very different methods. Clear reporting and careful examination of heterogeneity in ecological meta-analyses is thus crucial. Ecologists often also test similar hypotheses with different species, and in these meta-analyses, the reader should expect exploration of phylogenetic dependencies. Finally, observational studies not only provide the substrate for potential current manipulative experiments in this discipline but also form an important body of literature historically for synthesis. Sensitivity analyses of observational versus manipulative experiments when aggregated in the same ecological meta-analysis are also frequent and appropriate. This brief conceptual review is not intended as an instrument to rate meta-analyses for ecologists but does provide the appropriate framing for those purposes and directs the reader to ongoing developments in this direction in other disciplines."," *Data Accuracy; *Data Interpretation, Statistical; Data Mining/*methods; Ecology/*methods; *Meta-Analysis as Topic; Reading; *Research Design; and synthesis; criteria; ecology; guidelines; interpretation; meta-analysis",-2,,
259,Mpimbaza,2015,PloS one,Verbal Autopsy: Evaluation of Methods to Certify Causes of Death in Uganda,"To assess different methods for determining cause of death from verbal autopsy (VA) questionnaire data, the intra-rater reliability of Physician-Certified Verbal Autopsy (PCVA) and the accuracy of PCVA, expert-derived (non-hierarchical) and data-driven (hierarchal) algorithms were assessed for determining common causes of death in Ugandan children. A verbal autopsy validation study was conducted from 2008-2009 in three different sites in Uganda. The dataset included 104 neonatal deaths (0-27 days) and 615 childhood deaths (1-59 months) with the cause(s) of death classified by PCVA and physician review of hospital medical records (the 'reference standard'). Of the original 719 questionnaires, 141 (20%) were selected for a second review by the same physicians; the repeat cause(s) of death were compared to the original,and agreement assessed using the Kappa statistic.Physician reviewers' refined non-hierarchical algorithms for common causes of death from existing expert algorithms, from which, hierarchal algorithms were developed. The accuracy of PCVA, non-hierarchical, and hierarchical algorithms for determining cause(s) of death from all 719 VA questionnaires was determined using the reference standard. Overall, intra-rater repeatability was high (83% agreement, Kappa 0.79 [95% CI 0.76-0.82]). PCVA performed well, with high specificity for determining cause of neonatal (>67%), and childhood (>83%) deaths, resulting in fairly accurate cause-specific mortality fraction (CSMF) estimates. For most causes of death in children, non-hierarchical algorithms had higher sensitivity, but correspondingly lower specificity, than PCVA and hierarchical algorithms, resulting in inaccurate CSMF estimates. Hierarchical algorithms were specific for most causes of death, and CSMF estimates were comparable to the reference standard and PCVA. Inter-rater reliability of PCVA was high, and overall PCVA performed well. Hierarchical algorithms performed better than non-hierarchical algorithms due to higher specificity and more accurate CSMF estimates. Use of PCVA to determine cause of death from VA questionnaire data is reasonable while automated data-driven algorithms are improved."," Autopsy/*methods/standards; *Cause of Death; Child, Preschool; Humans; Infant; Infant, Newborn; Observer Variation; Physicians/standards; Speech; Surveys and Questionnaires; Uganda/epidemiology",,,
260,Rathbone,2015,Systematic reviews,"Faster title and abstract screening? Evaluating Abstrackr, a semi-automated online screening program for systematic reviewers","BACKGROUND: Citation screening is time consuming and inefficient. We sought to evaluate the performance of Abstrackr, a semi-automated online tool for predictive title and abstract screening. METHODS: Four systematic reviews (aHUS, dietary fibre, ECHO, rituximab) were used to evaluate Abstrackr. Citations from electronic searches of biomedical databases were imported into Abstrackr, and titles and abstracts were screened and included or excluded according to the entry criteria. This process was continued until Abstrackr predicted and classified the remaining unscreened citations as relevant or irrelevant. These classification predictions were checked for accuracy against the original review decisions. Sensitivity analyses were performed to assess the effects of including case reports in the aHUS dataset whilst screening and the effects of using larger imbalanced datasets with the ECHO dataset. The performance of Abstrackr was calculated according to the number of relevant studies missed, the workload saving, the false negative rate, and the precision of the algorithm to correctly predict relevant studies for inclusion, i.e. further full text inspection. RESULTS: Of the unscreened citations, Abstrackr's prediction algorithm correctly identified all relevant citations for the rituximab and dietary fibre reviews. However, one relevant citation in both the aHUS and ECHO reviews was incorrectly predicted as not relevant. The workload saving achieved with Abstrackr varied depending on the complexity and size of the reviews (9 % rituximab, 40 % dietary fibre, 67 % aHUS, and 57 % ECHO). The proportion of citations predicted as relevant, and therefore, warranting further full text inspection (i.e. the precision of the prediction) ranged from 16 % (aHUS) to 45 % (rituximab) and was affected by the complexity of the reviews. The false negative rate ranged from 2.4 to 21.7 %. Sensitivity analysis performed on the aHUS dataset increased the precision from 16 to 25 % and increased the workload saving by 10 % but increased the number of relevant studies missed. Sensitivity analysis performed with the larger ECHO dataset increased the workload saving (80 %) but reduced the precision (6.8 %) and increased the number of missed citations. CONCLUSIONS: Semi-automated title and abstract screening with Abstrackr has the potential to save time and reduce research waste.", *Abstracting and Indexing as Topic; *Algorithms; Humans; Reproducibility of Results; *Review Literature as Topic; Sensitivity and Specificity,2,,
261,Jonnalagadda,2015,Systematic reviews,Automating data extraction in systematic reviews: a systematic review,"BACKGROUND: Automation of the parts of systematic review process, specifically the data extraction step, may be an important strategy to reduce the time necessary to complete a systematic review. However, the state of the science of automatically extracting data elements from full texts has not been well described. This paper performs a systematic review of published and unpublished methods to automate data extraction for systematic reviews. METHODS: We systematically searched PubMed, IEEEXplore, and ACM Digital Library to identify potentially relevant articles. We included reports that met the following criteria: 1) methods or results section described what entities were or need to be extracted, and 2) at least one entity was automatically extracted with evaluation results that were presented for that entity. We also reviewed the citations from included reports. RESULTS: Out of a total of 1190 unique citations that met our search criteria, we found 26 published reports describing automatic extraction of at least one of more than 52 potential data elements used in systematic reviews. For 25 (48 %) of the data elements used in systematic reviews, there were attempts from various researchers to extract information automatically from the publication text. Out of these, 14 (27 %) data elements were completely extracted, but the highest number of data elements extracted automatically by a single study was 7. Most of the data elements were extracted with F-scores (a mean of sensitivity and positive predictive value) of over 70 %. CONCLUSIONS: We found no unified information extraction framework tailored to the systematic review process, and published reports focused on a limited (1-7) number of data elements. Biomedical natural language processing techniques have not been fully utilized to fully or even partially automate the data extraction step of systematic reviews.", Data Mining/*methods; Humans; Information Storage and Retrieval; *Publishing; Research Report; *Review Literature as Topic,2,,
262,Payr,2015,Studies in health technology and informatics,AAL robotics: state of the field and challenges,"The field of ""AAL Robotics"", combining AAL and robotics as disciplines, has not yet been precisely defined and does not present accepted structures and concepts that would allow to communicate unequivocally its methods, projects, and approaches. The paper presents a method of defining and categorizing AAL robots and presents the resulting classes of robots with regard to the activities they assist. The classification is useful in that it is able to cover the breadth of the field, but a more fine-grained description of functionalities will be needed in further research to establish the potential of robots to assist independent living of older adults."," Assisted Living Facilities/*trends; Austria; *Man-Machine Systems; Robotics/*trends; Self-Help Devices/*trends; Technology Assessment, Biomedical; Telemedicine/*trends",-1,,
263,Restrepo,2015,PloS one,Extracting Primary Open-Angle Glaucoma from Electronic Medical Records for Genetic Association Studies,"Electronic medical records (EMRs) are being widely implemented for use in genetic and genomic studies. As a phenotypic rich resource, EMRs provide researchers with the opportunity to identify disease cohorts and perform genotype-phenotype association studies. The Epidemiologic Architecture for Genes Linked to Environment (EAGLE) study, as part of the Population Architecture using Genomics and Epidemiology (PAGE) I study, has genotyped more than 15,000 individuals of diverse genetic ancestry in BioVU, the Vanderbilt University Medical Center's biorepository linked to a de-identified version of the EMR (EAGLE BioVU). Here we develop and deploy an algorithm utilizing data mining techniques to identify primary open-angle glaucoma (POAG) in African Americans from EAGLE BioVU for genetic association studies. The algorithm described here was designed using a combination of diagnostic codes, current procedural terminology billing codes, and free text searches to identify POAG status in situations where gold-standard digital photography cannot be accessed. The case algorithm identified 267 potential POAG subjects but underperformed after manual review with a positive predictive value of 51.6% and an accuracy of 76.3%. The control algorithm identified controls with a negative predictive value of 98.3%. Although the case algorithm requires more downstream manual review for use in large-scale studies, it provides a basis by which to extract a specific clinical subtype of glaucoma from EMRs in the absence of digital photographs."," Adult; African Americans; Aged; *Algorithms; Data Mining/*methods; *Electronic Health Records; Female; *Genetic Association Studies; *Glaucoma, Open-Angle; Humans; Male; Middle Aged; Young Adult",1,,
264,,,,,,,,,
265,,,,,,,,,
266,Mahood,2014,Research synthesis methods,Searching for grey literature for systematic reviews: challenges and benefits,"There is ongoing interest in including grey literature in systematic reviews. Including grey literature can broaden the scope to more relevant studies, thereby providing a more complete view of available evidence. Searching for grey literature can be challenging despite greater access through the Internet, search engines and online bibliographic databases. There are a number of publications that list sources for finding grey literature in systematic reviews. However, there is scant information about how searches for grey literature are executed and how it is included in the review process. This level of detail is important to ensure that reviews follow explicit methodology to be systematic, transparent and reproducible. The purpose of this paper is to provide a detailed account of one systematic review team's experience in searching for grey literature and including it throughout the review. We provide a brief overview of grey literature before describing our search and review approach. We also discuss the benefits and challenges of including grey literature in our systematic review, as well as the strengths and limitations to our approach. Detailed information about incorporating grey literature in reviews is important in advancing methodology as review teams adapt and build upon the approaches described."," Data Mining/*methods; *Databases, Bibliographic; Natural Language Processing; *Peer Review, Research; *Periodicals as Topic; *Review Literature as Topic; *Search Engine; Vocabulary, Controlled; grey literature; literature searching; systematic reviews",,,
267,Pieper,2014,Research synthesis methods,Methodological approaches in conducting overviews: current state in HTA agencies,"OBJECTIVES: Overviews search for reviews rather than for primary studies. They might have the potential to support decision making within a shorter time frame by reducing production time. We aimed to summarize available instructions for authors intending to conduct overviews as well as the currently applied methodology of overviews in international Health Technology Assessment (HTA) agencies. METHODS: We identified 127 HTA agencies and scanned their websites for methodological handbooks as well as published overviews as HTA reports. Additionally, we contacted HTA agencies by e-mail to retrieve possible unidentified handbooks or other related sources. RESULTS: In total, eight HTA agencies providing methodological support were found. Thirteen HTA agencies were found to have produced overviews since 2007, but only six of them published more than four overviews. Overviews were mostly employed in HTA products related to rapid assessment. Additional searches for primary studies published after the last review are often mentioned in order to update results. CONCLUSIONS: Although the interest in overviews is rising, little methodological guidance for the conduct of overviews is provided by HTA agencies. Overviews are of special interest in the context of rapid assessments to support policy-making within a short time frame. Therefore, empirical work on overviews needs to be extended. National strategies and experience should be disclosed and discussed."," Data Mining/*standards; Evidence-Based Medicine/*standards; Internationality; Periodicals as Topic/standards; *Practice Guidelines as Topic; *Review Literature as Topic; Technology Assessment, Biomedical/*methods/*standards/trends; policy making; public policy; review; technology assessment; time factor",-2,,
268,Mullins,2014,Research synthesis methods,"Reporting quality of search methods in systematic reviews of HIV behavioral interventions (2000-2010): are the searches clearly explained, systematic and reproducible?","Systematic reviews are an essential tool for researchers, prevention providers and policy makers who want to remain current with the evidence in the field. Systematic review must adhere to strict standards, as the results can provide a more objective appraisal of evidence for making scientific decisions than traditional narrative reviews. An integral component of a systematic review is the development and execution of a comprehensive systematic search to collect available and relevant information. A number of reporting guidelines have been developed to ensure quality publications of systematic reviews. These guidelines provide the essential elements to include in the review process and report in the final publication for complete transparency. We identified the common elements of reporting guidelines and examined the reporting quality of search methods in HIV behavioral intervention literature. Consistent with the findings from previous evaluations of reporting search methods of systematic reviews in other fields, our review shows a lack of full and transparent reporting within systematic reviews even though a plethora of guidelines exist. This review underscores the need for promoting the completeness of and adherence to transparent systematic search reporting within systematic reviews."," Behavior Therapy/*methods; Data Mining/*standards; HIV Infections/*epidemiology/prevention & control; Humans; Outcome Assessment (Health Care)/standards; *Practice Guidelines as Topic; Quality Assurance, Health Care/standards; Reproducibility of Results; Research Report/*standards; *Review Literature as Topic; literature review; meta-analysis; search reporting; systematic review; systematic search",-2,,
269,Thompson,2014,Research synthesis methods,A systematic method for search term selection in systematic reviews,"The wide variety of readily available electronic media grants anyone the freedom to retrieve published references from almost any area of research around the world. Despite this privilege, keeping up with primary research evidence is almost impossible because of the increase in professional publishing across disciplines. Systematic reviews are a solution to this problem as they aim to synthesize all current information on a particular topic and present a balanced and unbiased summary of the findings. They are fast becoming an important method of research across a number of fields, yet only a small number of guidelines exist on how to define and select terms for a systematic search. This article presents a replicable method for selecting terms in a systematic search using the semantic concept recognition software called leximancer (Leximancer, University of Queensland, Brisbane, Australia). We use this software to construct a set of terms from a corpus of literature pertaining to transborder interventions for drug control and discuss the applicability of this method to systematic reviews in general. This method aims to contribute a more 'systematic' approach for selecting terms in a manner that is entirely replicable for any user."," Data Mining/*methods; Machine Learning; *Natural Language Processing; Pattern Recognition, Automated; *Periodicals as Topic; Research Design; *Review Literature as Topic; Search Engine/*methods; Semantics; *Software; Vocabulary, Controlled; systematic review; systematic search; term",1,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
275,Chazard,2015,Studies in health technology and informatics,Process assessment by automated computation of healthcare quality indicators in hospital electronic health records: a systematic review of indicators,"The objective of the work is to extract healthcare process quality indicators from the literature, and to evaluate which of them could be automatically computed using routinely collected data from electronic health records (EHRs). A minimal set of data commonly available in EHRs is first defined. The initial bibliographic query enables to identify 8,744 papers, among which 126 papers describe 440 process indicators. 22.3% of indicators can be automatically computed. The computation of the indicators mostly require diagnoses (99%), drug prescriptions (59%), medical procedures (48%), administrative data (30%), laboratory results (20%), free-text reports with basic keyword research (19%), linkage with the patient's previous stays (11%) and dependence assessment (3%). 77.7% of indicators cannot be automatically computed, mostly because they require a linkage with outpatient data (61%), structured data that are usually not available (43%), unstructured data (26%) or the trace of an information that was given to the patient (8%)."," *Data Accuracy; Data Mining/*methods; Electronic Health Records/standards/*statistics & numerical data; Medical Record Linkage/*methods; Quality Assurance, Health Care/standards/*statistics & numerical data; Quality Indicators, Health Care/standards/*statistics & numerical data",-1,,
276,Djennaoui,2015,Studies in health technology and informatics,Improvement of the quality of medical databases: data-mining-based prediction of diagnostic codes from previous patient codes,"INTRODUCTION: Diagnoses and medical procedures collected under the French system of information are recorded in a nationwide database, the ""PMSI national database"", which is accessible for exploitation. Quality of the data in this database is directly related to the quality of coding, which can be of poor quality. Among the proposed methods for the exploitation of health databases, data mining techniques are particularly interesting. Our objective is to build sequential rules for missing diagnoses prediction by data mining of the PMSI national database. METHOD: Our working sample was constructed from the national database for years 2007 to 2010. The information retained for rules construction were medical diagnoses and medical procedures. The rules were selected using a statistical filter, and selected rules were validated by case review based on medical letters, which enabled to estimate the improvement of diagnoses recoding. RESULTS: The work sample was made of 59,170 inpatient stays. The predicted ICD codes were E11 (non-insulin-dependent diabetes mellitus), I48 (atrial fibrillation and flutter) and I50 (heart failure).We validated three sequential rules with a substantial improvement of positive predictive value: {E11,I10,DZQM006}=>{E11} {E11,I10,I48}=>{E11} {I48,I69}=>{I48} DISCUSSION: We were able to extract by data mining three simple, reliable and effective sequential rules, with a substantial improvement in diagnoses recoding. The results of our study indicate the opportunity to improve the data quality of the national database by data mining methods."," *Algorithms; Data Mining/*methods; *Databases, Factual; Decision Support Systems, Clinical/organization & administration; *Diagnosis-Related Groups; Electronic Health Records/*organization & administration; France; *International Classification of Diseases; Pattern Recognition, Automated/methods; Quality Improvement/organization & administration",-1,,
278,Kamdar,2015,International journal of technology assessment in health care,A NOVEL SEARCH BUILDER TO EXPEDITE SEARCH STRATEGIES FOR SYSTEMATIC REVIEWS,"OBJECTIVES: Developing a search strategy for use in a systematic review is a time-consuming process requiring construction of detailed search strings using complicated syntax, followed by iterative fine-tuning and trial-and-error testing of these strings in online biomedical search engines. METHODS: Building upon limitations of existing online-only search builders, a user-friendly computer-based tool was created to expedite search strategy development as part of production of a systematic review. RESULTS: Search Builder 1.0 is a Microsoft Excel(R)-based tool that automatically assembles search strategy text strings for PubMed (www.pubmed.com) and Embase (www.embase.com), based on a list of user-defined search terms and preferences. With the click of a button, Search Builder 1.0 automatically populates the syntax needed for functional search strings, and copies the string to the clipboard for pasting into Pubmed or Embase. The offline file-based interface of Search Builder 1.0 also allows for searches to be easily shared and saved for future reference. CONCLUSIONS: This novel, user-friendly tool can save considerable time and streamline a cumbersome step in the systematic review process.", Humans; PubMed; *Review Literature as Topic; Search Engine/*methods; User-Computer Interface; Microsoft Excel; Search builder; Software tool; Systematic review,,,
279,Denecke,2015,Artificial intelligence in medicine,Sentiment analysis in medical settings: New opportunities and challenges,"OBJECTIVE: Clinical documents reflect a patient's health status in terms of observations and contain objective information such as descriptions of examination results, diagnoses and interventions. To evaluate this information properly, assessing positive or negative clinical outcomes or judging the impact of a medical condition on patient's well being are essential. Although methods of sentiment analysis have been developed to address these tasks, they have not yet found broad application in the medical domain. METHODS AND MATERIAL: In this work, we characterize the facets of sentiment in the medical sphere and identify potential use cases. Through a literature review, we summarize the state of the art in healthcare settings. To determine the linguistic peculiarities of sentiment in medical texts and to collect open research questions of sentiment analysis in medicine, we perform a quantitative assessment with respect to word usage and sentiment distribution of a dataset of clinical narratives and medical social media derived from six different sources. RESULTS: Word usage in clinical narratives differs from that in medical social media: Nouns predominate. Even though adjectives are also frequently used, they mainly describe body locations. Between 12% and 15% of sentiment terms are determined in medical social media datasets when applying existing sentiment lexicons. In contrast, in clinical narratives only between 5% and 11% opinionated terms were identified. This proves the less subjective use of language in clinical narratives, requiring adaptations to existing methods for sentiment analysis. CONCLUSIONS: Medical sentiment concerns the patient's health status, medical conditions and treatment. Its analysis and extraction from texts has multiple applications, even for clinical narratives that remained so far unconsidered. Given the varying usage and meanings of terms, sentiment analysis from medical documents requires a domain-specific sentiment source and complementary context-dependent features to be able to correctly interpret the implicit sentiment.", Data Mining/*methods; *Health Status; Medical Informatics/*methods; *Natural Language Processing; Clinical text mining; Health status analysis; Medical language processing; Sentiment analysis,-1,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
282,Morro,2015,PloS one,Ultra-fast data-mining hardware architecture based on stochastic computing,Minimal hardware implementations able to cope with the processing of large amounts of data in reasonable times are highly desired in our information-driven society. In this work we review the application of stochastic computing to probabilistic-based pattern-recognition analysis of huge database sets. The proposed technique consists in the hardware implementation of a parallel architecture implementing a similarity search of data with respect to different pre-stored categories. We design pulse-based stochastic-logic blocks to obtain an efficient pattern recognition system. The proposed architecture speeds up the screening process of huge databases by a factor of 7 when compared to a conventional digital implementation using the same hardware area.," *Computers; *Data Mining; Databases as Topic; Humans; Probability; *Signal Processing, Computer-Assisted; *Stochastic Processes; Time Factors",1,,
,,,,,,,,,
284,O'Mara-Eves,2015,Systematic reviews,Erratum to: Using text mining for study identification in systematic reviews: a systematic review of current approaches,, ,2,,
285,Bravo,2015,BMC bioinformatics,Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research,"BACKGROUND: Current biomedical research needs to leverage and exploit the large amount of information reported in scientific publications. Automated text mining approaches, in particular those aimed at finding relationships between entities, are key for identification of actionable knowledge from free text repositories. We present the BeFree system aimed at identifying relationships between biomedical entities with a special focus on genes and their associated diseases. RESULTS: By exploiting morpho-syntactic information of the text, BeFree is able to identify gene-disease, drug-disease and drug-target associations with state-of-the-art performance. The application of BeFree to real-case scenarios shows its effectiveness in extracting information relevant for translational research. We show the value of the gene-disease associations extracted by BeFree through a number of analyses and integration with other data sources. BeFree succeeds in identifying genes associated to a major cause of morbidity worldwide, depression, which are not present in other public resources. Moreover, large-scale extraction and analysis of gene-disease associations, and integration with current biomedical knowledge, provided interesting insights on the kind of information that can be found in the literature, and raised challenges regarding data prioritization and curation. We found that only a small proportion of the gene-disease associations discovered by using BeFree is collected in expert-curated databases. Thus, there is a pressing need to find alternative strategies to manual curation, in order to review, prioritize and curate text-mining data and incorporate it into domain-specific databases. We present our strategy for data prioritization and discuss its implications for supporting biomedical research and applications. CONCLUSIONS: BeFree is a novel text mining system that performs competitively for the identification of gene-disease, drug-disease and drug-target associations. Our analyses show that mining only a small fraction of MEDLINE results in a large dataset of gene-disease associations, and only a small proportion of this dataset is actually recorded in curated resources (2%), raising several issues on data prioritization and curation. We propose that joint analysis of text mined data with data curated by experts appears as a suitable approach to both assess data quality and highlight novel and interesting information."," Data Mining/*methods; Databases, Factual; Depression/genetics; Disease/classification/*genetics; Humans; *Information Storage and Retrieval; Knowledge Bases; *Medline; *Publications; *Translational Medical Research",2,,
286,Pivovarov,2015,Journal of the American Medical Informatics Association : JAMIA,Automated methods for the summarization of electronic health records,"OBJECTIVES: This review examines work on automated summarization of electronic health record (EHR) data and in particular, individual patient record summarization. We organize the published research and highlight methodological challenges in the area of EHR summarization implementation. TARGET AUDIENCE: The target audience for this review includes researchers, designers, and informaticians who are concerned about the problem of information overload in the clinical setting as well as both users and developers of clinical summarization systems. SCOPE: Automated summarization has been a long-studied subject in the fields of natural language processing and human-computer interaction, but the translation of summarization and visualization methods to the complexity of the clinical workflow is slow moving. We assess work in aggregating and visualizing patient information with a particular focus on methods for detecting and removing redundancy, describing temporality, determining salience, accounting for missing data, and taking advantage of encoded clinical knowledge. We identify and discuss open challenges critical to the implementation and use of robust EHR summarization systems.", *Electronic Health Records; Humans; Information Storage and Retrieval/*methods; *Natural Language Processing; Clinical summarization; electronic health records; missing data; natural language processing; semantic similarity; temporality,1,,
287,Cook,2015,PloS one,An automated clinical alert system for newly-diagnosed atrial fibrillation,"OBJECTIVE: Clinical decision support systems that notify providers of abnormal test results have produced mixed results. We sought to develop, implement, and evaluate the impact of a computer-based clinical alert system intended to improve atrial fibrillation stroke prophylaxis, and identify reasons providers do not implement a guideline-concordant response. MATERIALS AND METHODS: We conducted a cohort study with historical controls among patients at a tertiary care hospital. We developed a decision rule to identify newly-diagnosed atrial fibrillation, automatically notify providers, and direct them to online evidence-based management guidelines. We tracked all notifications from December 2009 to February 2010 (notification period) and applied the same decision rule to all patients from December 2008 to February 2009 (control period). Primary outcomes were accuracy of notification (confirmed through chart review) and prescription of warfarin within 30 days. RESULTS: During the notification period 604 notifications were triggered, of which 268 (44%) were confirmed as newly-diagnosed atrial fibrillation. The notifications not confirmed as newly-diagnosed involved patients with no recent electrocardiogram at our institution. Thirty-four of 125 high-risk patients (27%) received warfarin in the notification period, compared with 34 of 94 (36%) in the control period (odds ratio, 0.66 [95% CI, 0.37-1.17]; p = 0.16). Common reasons to not prescribe warfarin (identified from chart review of 151 patients) included upcoming surgical procedure, choice to use aspirin, and discrepancy between clinical notes and the medication record. CONCLUSIONS: An automated system to identify newly-diagnosed atrial fibrillation, notify providers, and encourage access to management guidelines did not change provider behaviors."," Aged; Anticoagulants/pharmacology; Atrial Fibrillation/complications/*diagnosis; Automation; *Clinical Alarms; Cohort Studies; Decision Support Systems, Clinical; Drug Prescriptions; Female; Humans; Male; Outcome Assessment (Health Care); Stroke/complications/prevention & control; Tertiary Healthcare; Warfarin/pharmacology",-2,,
288,Stewart,2015,PloS one,The use of Bayesian networks to assess the quality of evidence from research synthesis: 1,"BACKGROUND: The grades of recommendation, assessment, development and evaluation (GRADE) approach is widely implemented in systematic reviews, health technology assessment and guideline development organisations throughout the world. A key advantage to this approach is that it aids transparency regarding judgments on the quality of evidence. However, the intricacies of making judgments about research methodology and evidence make the GRADE system complex and challenging to apply without training. METHODS: We have developed a semi-automated quality assessment tool (SAQAT) l based on GRADE. This is informed by responses by reviewers to checklist questions regarding characteristics that may lead to unreliability. These responses are then entered into the Bayesian network to ascertain the probabilities of risk of bias, inconsistency, indirectness, imprecision and publication bias conditional on review characteristics. The model then combines these probabilities to provide a probability for each of the GRADE overall quality categories. We tested the model using a range of plausible scenarios that guideline developers or review authors could encounter. RESULTS: Overall, the model reproduced GRADE judgements for a range of scenarios. Potential advantages over standard assessment are use of explicit and consistent weightings for different review characteristics, forcing consideration of important but sometimes neglected characteristics and principled downgrading where small but important probabilities of downgrading are accrued across domains. CONCLUSIONS: Bayesian networks have considerable potential for use as tools to assess the validity of research evidence. The key strength of such networks lies in the provision of a statistically coherent method for combining probabilities across a complex framework based on both belief and evidence. In addition to providing tools for less experienced users to implement reliability assessment, the potential for sensitivity analyses and automation may be beneficial for application and the methodological development of reliability tools."," *Bayes Theorem; Checklist; *Epidemiologic Studies; Evidence-Based Practice/*methods; Humans; Meta-Analysis as Topic; *Models, Statistical; Randomized Controlled Trials as Topic; Research Design; Technology Assessment, Biomedical/*methods",2,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
292,Malhotra,2015,PloS one,Knowledge retrieval from PubMed abstracts and electronic medical records with the Multiple Sclerosis Ontology,"BACKGROUND: In order to retrieve useful information from scientific literature and electronic medical records (EMR) we developed an ontology specific for Multiple Sclerosis (MS). METHODS: The MS Ontology was created using scientific literature and expert review under the Protege OWL environment. We developed a dictionary with semantic synonyms and translations to different languages for mining EMR. The MS Ontology was integrated with other ontologies and dictionaries (diseases/comorbidities, gene/protein, pathways, drug) into the text-mining tool SCAIView. We analyzed the EMRs from 624 patients with MS using the MS ontology dictionary in order to identify drug usage and comorbidities in MS. Testing competency questions and functional evaluation using F statistics further validated the usefulness of MS ontology. RESULTS: Validation of the lexicalized ontology by means of named entity recognition-based methods showed an adequate performance (F score = 0.73). The MS Ontology retrieved 80% of the genes associated with MS from scientific abstracts and identified additional pathways targeted by approved disease-modifying drugs (e.g. apoptosis pathways associated with mitoxantrone, rituximab and fingolimod). The analysis of the EMR from patients with MS identified current usage of disease modifying drugs and symptomatic therapy as well as comorbidities, which are in agreement with recent reports. CONCLUSION: The MS Ontology provides a semantic framework that is able to automatically extract information from both scientific literature and EMR from patients with MS, revealing new pathogenesis insights as well as new clinical information.", Antineoplastic Agents/therapeutic use; Antirheumatic Agents/therapeutic use; *Biological Ontologies; Computational Biology/methods; *Electronic Health Records; Fingolimod Hydrochloride/therapeutic use; Humans; Immunosuppressive Agents/therapeutic use; *Information Storage and Retrieval; Literature Based Discovery; Mitoxantrone/therapeutic use; Multiple Sclerosis/*classification/drug therapy; *PubMed; Rituximab/therapeutic use,1,,
293,Cohen,2015,Journal of the American Medical Informatics Association : JAMIA,Automated confidence ranked classification of randomized controlled trial articles: an aid to evidence-based medicine,"OBJECTIVE: For many literature review tasks, including systematic review (SR) and other aspects of evidence-based medicine, it is important to know whether an article describes a randomized controlled trial (RCT). Current manual annotation is not complete or flexible enough for the SR process. In this work, highly accurate machine learning predictive models were built that include confidence predictions of whether an article is an RCT. MATERIALS AND METHODS: The LibSVM classifier was used with forward selection of potential feature sets on a large human-related subset of MEDLINE to create a classification model requiring only the citation, abstract, and MeSH terms for each article. RESULTS: The model achieved an area under the receiver operating characteristic curve of 0.973 and mean squared error of 0.013 on the held out year 2011 data. Accurate confidence estimates were confirmed on a manually reviewed set of test articles. A second model not requiring MeSH terms was also created, and performs almost as well. DISCUSSION: Both models accurately rank and predict article RCT confidence. Using the model and the manually reviewed samples, it is estimated that about 8000 (3%) additional RCTs can be identified in MEDLINE, and that 5% of articles tagged as RCTs in Medline may not be identified. CONCLUSION: Retagging human-related studies with a continuously valued RCT confidence is potentially more useful for article ranking and review than a simple yes/no prediction. The automated RCT tagging tool should offer significant savings of time and effort during the process of writing SRs, and is a key component of a multistep text mining pipeline that we are building to streamline SR workflow. In addition, the model may be useful for identifying errors in MEDLINE publication types. The RCT confidence predictions described here have been made available to users as a web service with a user query form front end at: http://arrowsmith.psych.uic.edu/cgi-bin/arrowsmith_uic/RCT_Tagger.cgi.", *Artificial Intelligence; Evidence-Based Medicine; Humans; Information Storage and Retrieval/*methods; Medline; ROC Curve; *Randomized Controlled Trials as Topic; *Review Literature as Topic; *Support Vector Machine; Information Retrieval; Natural Language Processing; Randomized Controlled Trials as Topic; Support Vector Machines; Systematic Reviews,2,,
294,Zulu,2015,BMC health services research,Innovation in health service delivery: integrating community health assistants into the health system at district level in Zambia,"BACKGROUND: To address the huge human resources for health gap in Zambia, the Ministry of Health launched the National Community Health Assistant Strategy in 2010. The strategy aims to integrate community-based health workers into the health system by creating a new group of workers, called community health assistants (CHAs). However, literature suggests that the integration process of national community-based health worker programmes into health systems has not been optimal. Conceptually informed by the diffusion of innovations theory, this paper qualitatively aimed to explore the factors that shaped the acceptability and adoption of CHAs into the health system at district level in Zambia during the pilot phase. METHODS: Data gathered through review of documents, 6 focus group discussions with community leaders, and 12 key informant interviews with CHA trainers, supervisors and members of the District Health Management Team were analysed using thematic analysis. RESULTS: The perceived relative advantage of CHAs over existing community-based health workers in terms of their quality of training and scope of responsibilities, and the perceived compatibility of CHAs with existing groups of health workers and community healthcare expectations positively facilitated the integration process. However, limited integration of CHAs in the district health governance system hindered effective programme trialability, simplicity and observability at district level. Specific challenges at this level included a limited information flow and sense of programme ownership, and insufficient documentation of outcomes. The district also had difficulties in responding to emergent challenges such as delayed or non-payment of CHA incentives, as well as inadequate supervision and involvement of CHAs in the health posts where they are supposed to be working. Furthermore, failure of the health system to secure regular drug supplies affected health service delivery and acceptability of CHA services at community level. CONCLUSION: The study has demonstrated that implementation of policy guidelines for integrating community-based health workers in the health system may not automatically guarantee successful integration at the local or district level, at least at the start of the process. The study reiterates the need for fully integrating such innovations into the district health governance system if they are to be effective."," Community Health Services/*organization & administration; Community Health Workers/*organization & administration; Delivery of Health Care/*organization & administration; *Efficiency, Organizational; Focus Groups; Government Programs/*organization & administration; Humans; Organizational Innovation; Pilot Projects; Program Evaluation; Zambia",-2,,
295,O'Mara-Eves,2015,Systematic reviews,Using text mining for study identification in systematic reviews: a systematic review of current approaches,"BACKGROUND: The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities. METHODS: Five research questions led our review: what is the state of the evidence base; how has workload reduction been evaluated; what are the purposes of semi-automation and how effective are they; how have key contextual problems of applying text mining to the systematic review field been addressed; and what challenges to implementation have emerged? We answered these questions using standard systematic review methods: systematic and exhaustive searching, quality-assured data extraction and a narrative synthesis to synthesise findings. RESULTS: The evidence base is active and diverse; there is almost no replication between studies or collaboration between research teams and, whilst it is difficult to establish any overall conclusions about best approaches, it is clear that efficiencies and reductions in workload are potentially achievable. On the whole, most suggested that a saving in workload of between 30% and 70% might be possible, though sometimes the saving in workload is accompanied by the loss of 5% of relevant studies (i.e. a 95% recall). CONCLUSIONS: Using text mining to prioritise the order in which items are screened should be considered safe and ready for use in 'live' reviews. The use of text mining as a 'second screener' may also be used cautiously. The use of text mining to eliminate studies automatically should be considered promising, but not yet fully proven. In highly technical/clinical areas, it may be used with a high degree of confidence; but more developmental and evaluative work is needed in other disciplines."," *Computational Biology/methods/trends; Data Mining/*methods/trends; Databases, Factual; Evidence-Based Medicine; Humans; Information Storage and Retrieval/trends; Publications",2,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
298,Garin-Muga,2014,Studies in health technology and informatics,Review and Challenges of Brain Analysis through DTI Measurements,"Medical images are being studied to analyse the brain in neurological disorders. Measurements extracted from Diffusion tensor image (DTI) such as Fractional Anisotropy (FA) describe the brain changes caused by diseases. However, there is no single best method for the quantitative brain analysis. This paper presents a review of the existing methods and software tools for brain analysis through DTI measurements. It also states some challenges that current software tools still have to meet in order to improve automation and usability and become smarter software tools."," *Algorithms; Brain/*diagnostic imaging/pathology; Brain Diseases/*diagnostic imaging/pathology; Diagnosis, Differential; Diffusion Tensor Imaging/*methods; Humans; Image Enhancement/*methods; Reproducibility of Results; Sensitivity and Specificity; White Matter/*diagnostic imaging/pathology",-2,,
300,Venco,2014,BMC bioinformatics,SMITH: a LIMS for handling next-generation sequencing workflows,"BACKGROUND: Life-science laboratories make increasing use of Next Generation Sequencing (NGS) for studying bio-macromolecules and their interactions. Array-based methods for measuring gene expression or protein-DNA interactions are being replaced by RNA-Seq and ChIP-Seq. Sequencing is generally performed by specialized facilities that have to keep track of sequencing requests, trace samples, ensure quality and make data available according to predefined privileges. An integrated tool helps to troubleshoot problems, to maintain a high quality standard, to reduce time and costs. Commercial and non-commercial tools called LIMS (Laboratory Information Management Systems) are available for this purpose. However, they often come at prohibitive cost and/or lack the flexibility and scalability needed to adjust seamlessly to the frequently changing protocols employed. In order to manage the flow of sequencing data produced at the Genomic Unit of the Italian Institute of Technology (IIT), we developed SMITH (Sequencing Machine Information Tracking and Handling). METHODS: SMITH is a web application with a MySQL server at the backend. Wet-lab scientists of the Centre for Genomic Science and database experts from the Politecnico of Milan in the context of a Genomic Data Model Project developed SMITH. The data base schema stores all the information of an NGS experiment, including the descriptions of all protocols and algorithms used in the process. Notably, an attribute-value table allows associating an unconstrained textual description to each sample and all the data produced afterwards. This method permits the creation of metadata that can be used to search the database for specific files as well as for statistical analyses. RESULTS: SMITH runs automatically and limits direct human interaction mainly to administrative tasks. SMITH data-delivery procedures were standardized making it easier for biologists and analysts to navigate the data. Automation also helps saving time. The workflows are available through an API provided by the workflow management system. The parameters and input data are passed to the workflow engine that performs de-multiplexing, quality control, alignments, etc. CONCLUSIONS: SMITH standardizes, automates, and speeds up sequencing workflows. Annotation of data with key-value pairs facilitates meta-analysis."," Algorithms; Automation; Genomics; High-Throughput Nucleotide Sequencing/instrumentation/methods; Sequence Analysis, DNA/instrumentation/*methods; *Software; Workflow",-2,,
301,Fernandes,2014,PloS one,The INTERGROWTH-21st Project Neurodevelopment Package: a novel method for the multi-dimensional assessment of neurodevelopment in pre-school age children,"BACKGROUND: The International Fetal and Newborn Growth Consortium for the 21st Century (INTERGROWTH-21st) Project is a population-based, longitudinal study describing early growth and development in an optimally healthy cohort of 4607 mothers and newborns. At 24 months, children are assessed for neurodevelopmental outcomes with the INTERGROWTH-21st Neurodevelopment Package. This paper describes neurodevelopment tools for preschoolers and the systematic approach leading to the development of the Package. METHODS: An advisory panel shortlisted project-specific criteria (such as multi-dimensional assessments and suitability for international populations) to be fulfilled by a neurodevelopment instrument. A literature review of well-established tools for preschoolers revealed 47 candidates, none of which fulfilled all the project's criteria. A multi-dimensional assessment was, therefore, compiled using a package-based approach by: (i) categorizing desired outcomes into domains, (ii) devising domain-specific criteria for tool selection, and (iii) selecting the most appropriate measure for each domain. RESULTS: The Package measures vision (Cardiff tests); cortical auditory processing (auditory evoked potentials to a novelty oddball paradigm); and cognition, language skills, behavior, motor skills and attention (the INTERGROWTH-21st Neurodevelopment Assessment) in 35-45 minutes. Sleep-wake patterns (actigraphy) are also assessed. Tablet-based applications with integrated quality checks and automated, wireless electroencephalography make the Package easy to administer in the field by non-specialist staff. The Package is in use in Brazil, India, Italy, Kenya and the United Kingdom. CONCLUSIONS: The INTERGROWTH-21st Neurodevelopment Package is a multi-dimensional instrument measuring early child development (ECD). Its developmental approach may be useful to those involved in large-scale ECD research and surveillance efforts."," Child Development; Child, Preschool; Humans; Nervous System/*growth & development; Neuropsychological Tests",-2,,
302,Moseley,2014,Journal of medical Internet research,Beyond open big data: addressing unreliable research,"The National Institute of Health invests US $30.9 billion annually in medical research. However, the subsequent impact of this research output on society and the economy is amplified dramatically as a result of the actual medical treatments, biomedical innovations, and various commercial enterprises that emanate from and depend on these findings. It is therefore a great concern to discover that much of published research is unreliable. We propose extending the open data concept to the culture of the scientific research community. By dialing down unproductive features of secrecy and competition, while ramping up cooperation and transparency, we make a case that what is published would then be less susceptible to the sometimes corrupting and confounding pressures to be first or journalistically attractive, which can compromise the more fundamental need to be robustly correct."," Biomedical Research/*standards; *Datasets as Topic; Editorial Policies; *Information Dissemination; *Peer Review, Research; Periodicals as Topic; Research Design; Review Literature as Topic; collaborative learning; knowledge discovery; open data; peer review; research culture; unreliable research",-2,,
303,Xi,2014,PloS one,A novel region-growing based semi-automatic segmentation protocol for three-dimensional condylar reconstruction using cone beam computed tomography (CBCT),"OBJECTIVE: To present and validate a semi-automatic segmentation protocol to enable an accurate 3D reconstruction of the mandibular condyles using cone beam computed tomography (CBCT). MATERIALS AND METHODS: Approval from the regional medical ethics review board was obtained for this study. Bilateral mandibular condyles in ten CBCT datasets of patients were segmented using the currently proposed semi-automatic segmentation protocol. This segmentation protocol combined 3D region-growing and local thresholding algorithms. The segmentation of a total of twenty condyles was performed by two observers. The Dice-coefficient and distance map calculations were used to evaluate the accuracy and reproducibility of the segmented and 3D rendered condyles. RESULTS: The mean inter-observer Dice-coefficient was 0.98 (range [0.95-0.99]). An average 90th percentile distance of 0.32 mm was found, indicating an excellent inter-observer similarity of the segmented and 3D rendered condyles. No systematic errors were observed in the currently proposed segmentation protocol. CONCLUSION: The novel semi-automated segmentation protocol is an accurate and reproducible tool to segment and render condyles in 3D. The implementation of this protocol in the clinical practice allows the CBCT to be used as an imaging modality for the quantitative analysis of condylar morphology."," Adult; Cone-Beam Computed Tomography/*methods; Female; Humans; Image Processing, Computer-Assisted/*methods; Imaging, Three-Dimensional/*methods; Male; Mandibular Condyle/*diagnostic imaging/surgery; Mandibular Reconstruction; Middle Aged; Reproducibility of Results; Temporomandibular Joint/*diagnostic imaging/surgery; Young Adult",-2,,
304,Wu,2014,PloS one,Negation's not solved: generalizability versus optimizability in clinical natural language processing,"A review of published work in clinical natural language processing (NLP) may suggest that the negation detection task has been ""solved."" This work proposes that an optimizable solution does not equal a generalizable solution. We introduce a new machine learning-based Polarity Module for detecting negation in clinical text, and extensively compare its performance across domains. Using four manually annotated corpora of clinical text, we show that negation detection performance suffers when there is no in-domain development (for manual methods) or training data (for machine learning-based methods). Various factors (e.g., annotation guidelines, named entity characteristics, the amount of data, and lexical and syntactic context) play a role in making generalizability difficult, but none completely explains the phenomenon. Furthermore, generalizability remains challenging because it is unclear whether to use a single source for accurate data, combine all sources into a single model, or apply domain adaptation methods. The most reliable means to improve negation detection is to manually annotate in-domain training data (or, perhaps, manually modify rules); this is a strategy for optimizing performance, rather than generalizing it. These results suggest a direction for future work in domain-adaptive and task-adaptive methods for clinical NLP."," *Algorithms; Artificial Intelligence/*statistics & numerical data; Clinical Medicine/education; Humans; *Natural Language Processing; Semantics; Textbooks as Topic; Vocabulary, Controlled",-1,,
305,Golder,2014,International journal of technology assessment in health care,The contribution of different information sources to identify adverse effects of a medical device: a case study using a systematic review of spinal fusion,"BACKGROUND: The most effective sources to search to identify adverse effects data for medical devices are currently unknown. METHODS: The included studies from a systematic review of the safety of recombinant human bone morphogenetic protein-2 (rhBMP-2) for spinal fusion were used for analysis. For each source searched, a record was made for each relevant publication of whether it was retrieved by the search strategy used and whether it was available in the database but not retrieved. To account for multiple publications of the same study, a record was made of the relevant studies identified. The sensitivity, precision, and number needed to read were calculated as well as the minimum combination of sources to identify all the publications or studies. RESULTS: There were eighty-two publications (forty-nine studies) included in the systematic review. Only one article was available in a database searched but not retrieved by our search strategy. Science Citation Index (SCI) and EMBASE both achieved the highest sensitivity (62 percent), followed closely by MEDLINE/PubMED (56 percent). With the search strategies used, the minimum combination of sources needed to identify all the publications was SCI, EMBASE, CENTRAL, and either MEDLINE or PubMED, in addition to reference checking, contacting authors and an automated current awareness service. In relation to identifying all the relevant studies, the minimum combination of studies was similar with the exclusion of CENTRAL. CONCLUSIONS: To identify all the relevant publications or studies included in this case study systematic review, several different sources needed to be searched."," Bone Morphogenetic Protein 2/administration & dosage/*adverse effects; Case-Control Studies; *Databases, Bibliographic; *Equipment Safety; Humans; *Information Storage and Retrieval; Review Literature as Topic; *Spinal Fusion; Surgical Sponges/*adverse effects; Titanium; Bibliographic databases",-2,,
306,Abath Neto,2014,PloS one,Integrative data mining highlights candidate genes for monogenic myopathies,"Inherited myopathies are a heterogeneous group of disabling disorders with still barely understood pathological mechanisms. Around 40% of afflicted patients remain without a molecular diagnosis after exclusion of known genes. The advent of high-throughput sequencing has opened avenues to the discovery of new implicated genes, but a working list of prioritized candidate genes is necessary to deal with the complexity of analyzing large-scale sequencing data. Here we used an integrative data mining strategy to analyze the genetic network linked to myopathies, derive specific signatures for inherited myopathy and related disorders, and identify and rank candidate genes for these groups. Training sets of genes were selected after literature review and used in Manteia, a public web-based data mining system, to extract disease group signatures in the form of enriched descriptor terms, which include functional annotation, human and mouse phenotypes, as well as biological pathways and protein interactions. These specific signatures were then used as an input to mine and rank candidate genes, followed by filtration against skeletal muscle expression and association with known diseases. Signatures and identified candidate genes highlight both potential common pathological mechanisms and allelic disease groups. Recent discoveries of gene associations to diseases, like B3GALNT2, GMPPB and B3GNT1 to congenital muscular dystrophies, were prioritized in the ranked lists, suggesting a posteriori validation of our approach and predictions. We show an example of how the ranked lists can be used to help analyze high-throughput sequencing data to identify candidate genes, and highlight the best candidate genes matching genomic regions linked to myopathies without known causative genes. This strategy can be automatized to generate fresh candidate gene lists, which help cope with database annotation updates as new knowledge is incorporated.", Data Mining/*methods; Gene Ontology; Gene Regulatory Networks; Genetic Association Studies/*methods; Genetic Predisposition to Disease; Humans; Muscular Diseases/*genetics; Phenotype,-2,,
307,Hirsch,2015,Journal of the American Medical Informatics Association : JAMIA,"HARVEST, a longitudinal patient record summarizer","OBJECTIVE: To describe HARVEST, a novel point-of-care patient summarization and visualization tool, and to conduct a formative evaluation study to assess its effectiveness and gather feedback for iterative improvements. MATERIALS AND METHODS: HARVEST is a problem-based, interactive, temporal visualization of longitudinal patient records. Using scalable, distributed natural language processing and problem salience computation, the system extracts content from the patient notes and aggregates and presents information from multiple care settings. Clinical usability was assessed with physician participants using a timed, task-based chart review and questionnaire, with performance differences recorded between conditions (standard data review system and HARVEST). RESULTS: HARVEST displays patient information longitudinally using a timeline, a problem cloud as extracted from notes, and focused access to clinical documentation. Despite lack of familiarity with HARVEST, when using a task-based evaluation, performance and time-to-task completion was maintained in patient review scenarios using HARVEST alone or the standard clinical information system at our institution. Subjects reported very high satisfaction with HARVEST and interest in using the system in their daily practice. DISCUSSION: HARVEST is available for wide deployment at our institution. Evaluation provided informative feedback and directions for future improvements. CONCLUSIONS: HARVEST was designed to address the unmet need for clinicians at the point of care, facilitating review of essential patient information. The deployment of HARVEST in our institution allows us to study patient record summarization as an informatics intervention in a real-world setting. It also provides an opportunity to learn how clinicians use the summarizer, enabling informed interface and content iteration and optimization to improve patient care.", Attitude to Computers; *Data Display; *Electronic Health Records; Humans; Natural Language Processing; Software; Surveys and Questionnaires; *User-Computer Interface; electronic health record; summarization; visualization,-2,,
308,West,2015,Journal of the American Medical Informatics Association : JAMIA,Innovative information visualization of electronic health record data: a systematic review,"OBJECTIVE: This study investigates the use of visualization techniques reported between 1996 and 2013 and evaluates innovative approaches to information visualization of electronic health record (EHR) data for knowledge discovery. METHODS: An electronic literature search was conducted May-July 2013 using MEDLINE and Web of Knowledge, supplemented by citation searching, gray literature searching, and reference list reviews. General search terms were used to assure a comprehensive document search. RESULTS: Beginning with 891 articles, the number of articles was reduced by eliminating 191 duplicates. A matrix was developed for categorizing all abstracts and to assist with determining those to be excluded for review. Eighteen articles were included in the final analysis. DISCUSSION: Several visualization techniques have been extensively researched. The most mature system is LifeLines and its applications as LifeLines2, EventFlow, and LifeFlow. Initially, research focused on records from a single patient and visualization of the complex data related to one patient. Since 2010, the techniques under investigation are for use with large numbers of patient records and events. Most are linear and allow interaction through scaling and zooming to resize. Color, density, and filter techniques are commonly used for visualization. CONCLUSIONS: With the burgeoning increase in the amount of electronic healthcare data, the potential for knowledge discovery is significant if data are managed in innovative and effective ways. We identify challenges discovered by previous EHR visualization research, which will help researchers who seek to design and improve visualization techniques."," *Audiovisual Aids; Data Display; *Electronic Health Records; Humans; *Pattern Recognition, Automated; User-Computer Interface; Electronic Health Records; Health care data; Information visualization; Systematic review",-2,,
309,Rochefort,2015,Journal of the American Medical Informatics Association : JAMIA,A novel method of adverse event detection can accurately identify venous thromboembolisms (VTEs) from narrative electronic health record data,"BACKGROUND: Venous thromboembolisms (VTEs), which include deep vein thrombosis (DVT) and pulmonary embolism (PE), are associated with significant mortality, morbidity, and cost in hospitalized patients. To evaluate the success of preventive measures, accurate and efficient methods for monitoring VTE rates are needed. Therefore, we sought to determine the accuracy of statistical natural language processing (NLP) for identifying DVT and PE from electronic health record data. METHODS: We randomly sampled 2000 narrative radiology reports from patients with a suspected DVT/PE in Montreal (Canada) between 2008 and 2012. We manually identified DVT/PE within each report, which served as our reference standard. Using a bag-of-words approach, we trained 10 alternative support vector machine (SVM) models predicting DVT, and 10 predicting PE. SVM training and testing was performed with nested 10-fold cross-validation, and the average accuracy of each model was measured and compared. RESULTS: On manual review, 324 (16.2%) reports were DVT-positive and 154 (7.7%) were PE-positive. The best DVT model achieved an average sensitivity of 0.80 (95% CI 0.76 to 0.85), specificity of 0.98 (98% CI 0.97 to 0.99), positive predictive value (PPV) of 0.89 (95% CI 0.85 to 0.93), and an area under the curve (AUC) of 0.98 (95% CI 0.97 to 0.99). The best PE model achieved sensitivity of 0.79 (95% CI 0.73 to 0.85), specificity of 0.99 (95% CI 0.98 to 0.99), PPV of 0.84 (95% CI 0.75 to 0.92), and AUC of 0.99 (95% CI 0.98 to 1.00). CONCLUSIONS: Statistical NLP can accurately identify VTE from narrative radiology reports.", *Electronic Health Records; Hospitalization; Humans; *Natural Language Processing; Pulmonary Embolism; *Support Vector Machine; *Venous Thromboembolism; acute care hospital; automated text classification; deep vein thrombosis; natural language processing; support vector machines,-2,,
310,Zeng-Treitler,2014,Journal of the American Medical Informatics Association : JAMIA,Evaluation of a pictograph enhancement system for patient instruction: a recall study,"OBJECTIVE: We developed a novel computer application called Glyph that automatically converts text to sets of illustrations using natural language processing and computer graphics techniques to provide high quality pictographs for health communication. In this study, we evaluated the ability of the Glyph system to illustrate a set of actual patient instructions, and tested patient recall of the original and Glyph illustrated instructions. METHODS: We used Glyph to illustrate 49 patient instructions representing 10 different discharge templates from the University of Utah Cardiology Service. 84 participants were recruited through convenience sampling. To test the recall of illustrated versus non-illustrated instructions, participants were asked to review and then recall a set questionnaires that contained five pictograph-enhanced and five non-pictograph-enhanced items. RESULTS: The mean score without pictographs was 0.47 (SD 0.23), or 47% recall. With pictographs, this mean score increased to 0.52 (SD 0.22), or 52% recall. In a multivariable mixed effects linear regression model, this 0.05 mean increase was statistically significant (95% CI 0.03 to 0.06, p<0.001). DISCUSSION: In our study, the presence of Glyph pictographs improved discharge instruction recall (p<0.001). Education, age, and English as first language were associated with better instruction recall and transcription. CONCLUSIONS: Automated illustration is a novel approach to improve the comprehension and recall of discharge instructions. Our results showed a statistically significant in recall with automated illustrations. Subjects with no-colleague education and younger subjects appeared to benefit more from the illustrations than others.", Adult; Age Factors; Aged; *Computer Graphics; Educational Status; Female; Humans; Linear Models; Male; *Medical Illustration; *Mental Recall; Middle Aged; *Patient Discharge; Patient Education as Topic/*methods; Software; Young Adult; consumer health informatics; discharge instructions; evaluation; patient education; pictographs,-2,,
311,Jiang,2015,International journal of medical informatics,The divided communities of shared concerns: mapping the intellectual structure of e-Health research in social science journals,"PURPOSE: Social scientific approach has become an important approach in e-Health studies over the past decade. However, there has been little systematical examination of what aspects of e-Health social scientists have studied and how relevant and informative knowledge has been produced and diffused by this line of inquiry. This study performed a systematic review of the body of e-Health literature in mainstream social science journals over the past decade by testing the applicability of a 5A categorization (i.e., access, availability, appropriateness, acceptability, and applicability), proposed by the U.S. Department of Health and Human Services, as a framework for understanding social scientific research in e-Health. METHODS: This study used a quantitative, bottom-up approach to review the e-Health literature in social sciences published from 2000 to 2009. A total of 3005 e-Health studies identified from two social sciences databases (i.e., Social Sciences Citation Index and Arts & Humanities Citation Index) were analyzed with text topic modeling and structural analysis of co-word network, co-citation network, and scientific food web. RESULTS: There have been dramatic increases in the scale of e-Health studies in social sciences over the past decade in terms of the numbers of publications, journal outlets and participating disciplines. The results empirically confirm the presence of the 5A clusters in e-Health research, with the cluster of applicability as the dominant research area and the cluster of availability as the major knowledge producer for other clusters. The network analysis also reveals that the five distinctive clusters share much more in common in research concerns than what e-Health scholars appear to recognize. CONCLUSIONS: It is time to explicate and, more importantly, tap into the shared concerns cutting across the seemingly divided scholarly communities. In particular, more synergy exercises are needed to promote adherence of the field.", *Bibliometrics; *Health Services Research; Humans; Medical Informatics; Periodicals as Topic/*standards; *Social Sciences; Internet; Knowledge; Review; Social sciences; Technology; e-Health,-2,,
312,Choong,2014,Journal of medical Internet research,Automatic evidence retrieval for systematic reviews,"BACKGROUND: Snowballing involves recursively pursuing relevant references cited in the retrieved literature and adding them to the search results. Snowballing is an alternative approach to discover additional evidence that was not retrieved through conventional search. Snowballing's effectiveness makes it best practice in systematic reviews despite being time-consuming and tedious. OBJECTIVE: Our goal was to evaluate an automatic method for citation snowballing's capacity to identify and retrieve the full text and/or abstracts of cited articles. METHODS: Using 20 review articles that contained 949 citations to journal or conference articles, we manually searched Microsoft Academic Search (MAS) and identified 78.0% (740/949) of the cited articles that were present in the database. We compared the performance of the automatic citation snowballing method against the results of this manual search, measuring precision, recall, and F1 score. RESULTS: The automatic method was able to correctly identify 633 (as proportion of included citations: recall=66.7%, F1 score=79.3%; as proportion of citations in MAS: recall=85.5%, F1 score=91.2%) of citations with high precision (97.7%), and retrieved the full text or abstract for 490 (recall=82.9%, precision=92.1%, F1 score=87.3%) of the 633 correctly retrieved citations. CONCLUSIONS: The proposed method for automatic citation snowballing is accurate and is capable of obtaining the full texts or abstracts for a substantial proportion of the scholarly citations in review articles. By automating the process of citation snowballing, it may be possible to reduce the time and effort of common evidence surveillance tasks such as keeping trial registries up to date and conducting systematic reviews."," Databases, Factual; Evidence-Based Medicine; Humans; Information Storage and Retrieval/*methods; Medical Informatics/*methods; Registries; Review Literature as Topic; information storage and retrieval; medical informatics",2,,
313,Brown,2014,Systematic reviews,A Microsoft-Excel-based tool for running and critically appraising network meta-analyses--an overview and application of NetMetaXL,"BACKGROUND: The use of network meta-analysis has increased dramatically in recent years. WinBUGS, a freely available Bayesian software package, has been the most widely used software package to conduct network meta-analyses. However, the learning curve for WinBUGS can be daunting, especially for new users. Furthermore, critical appraisal of network meta-analyses conducted in WinBUGS can be challenging given its limited data manipulation capabilities and the fact that generation of graphical output from network meta-analyses often relies on different software packages than the analyses themselves. METHODS: We developed a freely available Microsoft-Excel-based tool called NetMetaXL, programmed in Visual Basic for Applications, which provides an interface for conducting a Bayesian network meta-analysis using WinBUGS from within Microsoft Excel. . This tool allows the user to easily prepare and enter data, set model assumptions, and run the network meta-analysis, with results being automatically displayed in an Excel spreadsheet. It also contains macros that use NetMetaXL's interface to generate evidence network diagrams, forest plots, league tables of pairwise comparisons, probability plots (rankograms), and inconsistency plots within Microsoft Excel. All figures generated are publication quality, thereby increasing the efficiency of knowledge transfer and manuscript preparation. RESULTS: We demonstrate the application of NetMetaXL using data from a network meta-analysis published previously which compares combined resynchronization and implantable defibrillator therapy in left ventricular dysfunction. We replicate results from the previous publication while demonstrating result summaries generated by the software. CONCLUSIONS: Use of the freely available NetMetaXL successfully demonstrated its ability to make running network meta-analyses more accessible to novice WinBUGS users by allowing analyses to be conducted entirely within Microsoft Excel. NetMetaXL also allows for more efficient and transparent critical appraisal of network meta-analyses, enhanced standardization of reporting, and integration with health economic evaluations which are frequently Excel-based."," Bayes Theorem; *Computer Graphics; Data Interpretation, Statistical; Defibrillators, Implantable; Humans; *Meta-Analysis as Topic; *Software; Ventricular Dysfunction, Left/therapy",-1,,
314,Guillaud,2014,PloS one,Evaluation of HPV infection and smoking status impacts on cell proliferation in epithelial layers of cervical neoplasia,"Accurate cervical intra-epithelial neoplasia (CIN) lesion grading is needed for effective patient management. We applied computer-assisted scanning and analytic approaches to immuno-stained CIN lesion sections to more accurately delineate disease states and decipher cell proliferation impacts from HPV and smoking within individual epithelial layers. A patient cohort undergoing cervical screening was identified (n = 196) and biopsies of varying disease grades and with intact basement membranes and epithelial layers were obtained (n = 261). Specimens were sectioned, stained (Mib1), and scanned using a high-resolution imaging system. We achieved semi-automated delineation of proliferation status and epithelial cell layers using Otsu segmentation, manual image review, Voronoi tessellation, and immuno-staining. Data were interrogated against known status for HPV infection, smoking, and disease grade. We observed increased cell proliferation and decreased epithelial thickness with increased disease grade (when analyzing the epithelium at full thickness). Analysis within individual cell layers showed a >/=50% increase in cell proliferation for CIN2 vs. CIN1 lesions in higher epithelial layers (with minimal differences seen in basal/parabasal layers). Higher rates of proliferation for HPV-positive vs. -negative cases were seen in epithelial layers beyond the basal/parabasal layers in normal and CIN1 tissues. Comparing smokers vs. non-smokers, we observed increased cell proliferation in parabasal (low and high grade lesions) and basal layers (high grade only). In sum, we report CIN grade-specific differences in cell proliferation within individual epithelial layers. We also show HPV and smoking impacts on cell layer-specific proliferation. Our findings yield insight into CIN progression biology and demonstrate that rigorous, semi-automated imaging of histopathological specimens may be applied to improve disease grading accuracy."," Adolescent; Adult; Aged; Basement Membrane/*pathology; Biomarkers, Tumor; Biopsy; *Cell Proliferation; Epithelium/pathology; Female; Humans; Ki-67 Antigen/biosynthesis; Middle Aged; Neoplasm Grading; Papillomaviridae/genetics/*isolation & purification/pathogenicity; Smoking/pathology; Uterine Cervical Neoplasms/*diagnosis/pathology/virology",-2,,
315,Botsis,2014,Studies in health technology and informatics,Novel algorithms for improved pattern recognition using the US FDA Adverse Event Network Analyzer,"The medical review of adverse event reports for medical products requires the processing of ""big data"" stored in spontaneous reporting systems, such as the US Vaccine Adverse Event Reporting System (VAERS). VAERS data are not well suited to traditional statistical analyses so we developed the FDA Adverse Event Network Analyzer (AENA) and three novel network analysis approaches to extract information from these data. Our new approaches include a weighting scheme based on co-occurring triplets in reports, a visualization layout inspired by the islands algorithm, and a network growth methodology for the detection of outliers. We explored and verified these approaches by analysing the historical signal of Intussusception (IS) after the administration of RotaShield vaccine (RV) in 1999. We believe that our study supports the use of AENA for pattern recognition in medical product safety and other clinical data."," Adverse Drug Reaction Reporting Systems/*organization & administration; *Algorithms; Artificial Intelligence; Electronic Health Records/*organization & administration; Humans; Incidence; Intussusception/*epidemiology; Pattern Recognition, Automated/*methods; Reproducibility of Results; Risk Assessment/methods; Rotavirus Vaccines/*therapeutic use; Sensitivity and Specificity; *Sentinel Surveillance; United States/epidemiology; United States Food and Drug Administration",-1,,
316,Okhmatovskaia,2014,Studies in health technology and informatics,Addressing the challenge of encoding causal epidemiological knowledge in formal ontologies: a practical perspective,The paper presents an overview of approaches to encoding uncertain causal knowledge in formal ontologies and demonstrates how these approaches can be used in a semantic-driven application for public health using the Population Health Record (PopHR) platform as an example.," Biological Ontologies/*organization & administration/statistics & numerical data; *Causality; Electronic Health Records/*organization & administration; *Epidemiologic Methods; *Health Status; Humans; Information Storage and Retrieval/*methods; Medical Record Linkage/*methods; Natural Language Processing; Semantics; Vocabulary, Controlled",-1,,
317,Lyons,2014,Journal of medical Internet research,Behavior change techniques implemented in electronic lifestyle activity monitors: a systematic content analysis,"BACKGROUND: Electronic activity monitors (such as those manufactured by Fitbit, Jawbone, and Nike) improve on standard pedometers by providing automated feedback and interactive behavior change tools via mobile device or personal computer. These monitors are commercially popular and show promise for use in public health interventions. However, little is known about the content of their feedback applications and how individual monitors may differ from one another. OBJECTIVE: The purpose of this study was to describe the behavior change techniques implemented in commercially available electronic activity monitors. METHODS: Electronic activity monitors (N=13) were systematically identified and tested by 3 trained coders for at least 1 week each. All monitors measured lifestyle physical activity and provided feedback via an app (computer or mobile). Coding was based on a hierarchical list of 93 behavior change techniques. Further coding of potentially effective techniques and adherence to theory-based recommendations were based on findings from meta-analyses and meta-regressions in the research literature. RESULTS: All monitors provided tools for self-monitoring, feedback, and environmental change by definition. The next most prevalent techniques (13 out of 13 monitors) were goal-setting and emphasizing discrepancy between current and goal behavior. Review of behavioral goals, social support, social comparison, prompts/cues, rewards, and a focus on past success were found in more than half of the systems. The monitors included a range of 5-10 of 14 total techniques identified from the research literature as potentially effective. Most of the monitors included goal-setting, self-monitoring, and feedback content that closely matched recommendations from social cognitive theory. CONCLUSIONS: Electronic activity monitors contain a wide range of behavior change techniques typically used in clinical behavioral interventions. Thus, the monitors may represent a medium by which these interventions could be translated for widespread use. This technology has broad applications for use in clinical, public health, and rehabilitation settings."," Actigraphy/*instrumentation; Behavior Therapy; *Exercise; *Health Behavior; Humans; Life Style; Monitoring, Ambulatory/*instrumentation; Social Support; Telemedicine; behavior change technique; electronic activity monitor; mhealth; mobile; physical activity",-2,,
318,Holstiege,2015,Journal of the American Medical Informatics Association : JAMIA,Effects of computer-aided clinical decision support systems in improving antibiotic prescribing by primary care providers: a systematic review,"OBJECTIVE: To assess the effectiveness of computer-aided clinical decision support systems (CDSS) in improving antibiotic prescribing in primary care. METHODS: A literature search utilizing Medline (via PubMed) and Embase (via Embase) was conducted up to November 2013. Randomized controlled trials (RCTs) and cluster randomized trials (CRTs) that evaluated the effects of CDSS aiming at improving antibiotic prescribing practice in an ambulatory primary care setting were included for review. Two investigators independently extracted data about study design and quality, participant characteristics, interventions, and outcomes. RESULTS: Seven studies (4 CRTs, 3 RCTs) met our inclusion criteria. All studies were performed in the USA. Proportions of eligible patient visits that triggered CDSS use varied substantially between intervention arms of studies (range 2.8-62.8%). Five out of seven trials showed marginal to moderate statistically significant effects of CDSS in improving antibiotic prescribing behavior. CDSS that automatically provided decision support were more likely to improve prescribing practice in contrast to systems that had to be actively initiated by healthcare providers. CONCLUSIONS: CDSS show promising effectiveness in improving antibiotic prescribing behavior in primary care. Magnitude of effects compared to no intervention, appeared to be similar to other moderately effective single interventions directed at primary care providers. Additional research is warranted to determine CDSS characteristics crucial to triggering high adoption by providers as a perquisite of clinically relevant improvement of antibiotic prescribing."," Anti-Bacterial Agents/*therapeutic use; *Decision Support Systems, Clinical; *Drug Therapy, Computer-Assisted; Electronic Health Records; Humans; *Practice Patterns, Physicians'; Primary Health Care; Cdss; antibiotic prescribing; computer-aided clinical decision support; electronic health record; medical decision-making; primary care",-2,,
319,Luo,2014,International journal of medical informatics,A systematic review of predictive modeling for bronchiolitis,"PURPOSE: Bronchiolitis is the most common cause of illness leading to hospitalization in young children. At present, many bronchiolitis management decisions are made subjectively, leading to significant practice variation among hospitals and physicians caring for children with bronchiolitis. To standardize care for bronchiolitis, researchers have proposed various models to predict the disease course to help determine a proper management plan. This paper reviews the existing state of the art of predictive modeling for bronchiolitis. Predictive modeling for respiratory syncytial virus (RSV) infection is covered whenever appropriate, as RSV accounts for about 70% of bronchiolitis cases. METHODS: A systematic review was conducted through a PubMed search up to April 25, 2014. The literature on predictive modeling for bronchiolitis was retrieved using a comprehensive search query, which was developed through an iterative process. Search results were limited to human subjects, the English language, and children (birth to 18 years). RESULTS: The literature search returned 2312 references in total. After manual review, 168 of these references were determined to be relevant and are discussed in this paper. We identify several limitations and open problems in predictive modeling for bronchiolitis, and provide some preliminary thoughts on how to address them, with the hope to stimulate future research in this domain. CONCLUSIONS: Many problems remain open in predictive modeling for bronchiolitis. Future studies will need to address them to achieve optimal predictive models."," Bronchiolitis/diagnosis/drug therapy/*physiopathology; Humans; *Models, Theoretical; Bronchiolitis; Machine learning; Predictive modeling; Respiratory syncytial virus",-2,,
320,Liu,2014,PloS one,A systematic review and meta-analysis of diagnostic and prognostic serum biomarkers of colorectal cancer,"BACKGROUND: Our systematic review summarizes the evidence concerning the accuracy of serum diagnostic and prognostic tests for colorectal cancer (CRC). METHODS: The databases MEDLINE and EMBASE were searched iteratively to identify the relevant literature for serum markers of CRC published from 1950 to August 2012. The articles that provided adequate information to meet the requirements of the meta-analysis of diagnostic and prognostic markers were included. A 2-by-2 table of each diagnostic marker and its hazard ratio (HR) and the confidence interval (CI) of each prognostic marker was directly or indirectly extracted from the included papers, and the pooled sensitivity and specificity of the diagnostic marker and the pooled HR and the CI of the prognostic marker were subsequently calculated using the extracted data. RESULTS: In total, 104 papers related to the diagnostic markers and 49 papers related to the prognostic serum markers of CRC were collected, and only 19 of 92 diagnostic markers were investigated in more than two studies, whereas 21 out of 44 prognostic markers were included in two or more studies. All of the pooled sensitivities of the diagnostic markers with > = 3 repetitions were less than 50%, and the meta-analyses of the prognostic markers with more than 3 studies were performed, VEGF with highest (2.245, CI: 1.347-3.744) and MMP-7 with lowest (1.099, CI: 1.018-1.187)) pooled HRs are presented. CONCLUSIONS: The quality of studies addressing the diagnostic and prognostic accuracy of the tests was poor, and the results were highly heterogeneous. The poor characteristics indicate that these tests are of little value for clinical practice.", Biomarkers/*blood; Colorectal Neoplasms/*blood/*diagnosis; Data Mining; Humans; Likelihood Functions; Odds Ratio; Prognosis; Reproducibility of Results; Sensitivity and Specificity,-2,,
321,Chen,2014,Studies in health technology and informatics,Methods for assessing the quality of data in public health information systems: a critical review,"The quality of data in public health information systems can be ensured by effective data quality assessment. In order to conduct effective data quality assessment, measurable data attributes have to be precisely defined. Then reliable and valid measurement methods for data attributes have to be used to measure each attribute. We conducted a systematic review of data quality assessment methods for public health using major databases and well-known institutional websites. 35 studies were eligible for inclusion in the study. A total of 49 attributes of data quality were identified from the literature. Completeness, accuracy and timeliness were the three most frequently assessed attributes of data quality. Most studies directly examined data values. This is complemented by exploring either data users' perception or documentation quality. However, there are limitations of current data quality assessment methods: a lack of consensus on attributes measured; inconsistent definition of the data quality attributes; a lack of mixed methods for assessing data quality; and inadequate attention to reliability and validity. Removal of these limitations is an opportunity for further improvement."," Data Mining/methods/*standards/statistics & numerical data; Databases, Factual/*standards; Guideline Adherence/statistics & numerical data; Guidelines as Topic; Health Information Systems/*standards/*statistics & numerical data; Internationality; Public Health Informatics/standards; Quality Assurance, Health Care/*methods/standards; Research Design/*standards/*statistics & numerical data",-2,,
322,de Andrade,2014,PloS one,System dynamics modeling in the evaluation of delays of care in ST-segment elevation myocardial infarction patients within a tiered health system,"BACKGROUND: Mortality rates amongst ST segment elevation myocardial infarction (STEMI) patients remain high, especially in developing countries. The aim of this study was to evaluate the factors related with delays in the treatment of STEMI patients to support a strategic plan toward structural and personnel modifications in a primary hospital aligning its process with international guidelines. METHODS AND FINDINGS: The study was conducted in a primary hospital localized in Foz do Iguacu, Brazil. We utilized a qualitative and quantitative integrated analysis including on-site observations, interviews, medical records analysis, Qualitative Comparative Analysis (QCA) and System Dynamics Modeling (SD). Main cause of delays were categorized into three themes: a) professional, b) equipment and c) transportation logistics. QCA analysis confirmed four main stages of delay to STEMI patient's care in relation to the 'Door-in-Door-out' time at the primary hospital. These stages and their average delays in minutes were: a) First Medical Contact (From Door-In to the first contact with the nurse and/or physician): 7 minutes; b) Electrocardiogram acquisition and review by a physician: 28 minutes; c) ECG transmission and Percutaneous Coronary Intervention Center team feedback time: 76 minutes; and d) Patient's Transfer Waiting Time: 78 minutes. SD baseline model confirmed the system's behavior with all occurring delays and the need of improvements. Moreover, after model validation and sensitivity analysis, results suggested that an overall improvement of 40% to 50% in each of these identified stages would reduce the delay. CONCLUSIONS: This evaluation suggests that investment in health personnel training, diminution of bureaucracy, and management of guidelines might lead to important improvements decreasing the delay of STEMI patients' care. In addition, this work provides evidence that SD modeling may highlight areas where health system managers can implement and evaluate the necessary changes in order to improve the process of care."," Brazil; Delivery of Health Care/*organization & administration; Electrocardiography; Humans; *Models, Organizational; Myocardial Infarction/physiopathology/*therapy; Time and Motion Studies",-2,,
323,Anholt,2014,PloS one,Using informatics and the electronic medical record to describe antimicrobial use in the clinical management of diarrhea cases at 12 companion animal practices,"Antimicrobial drugs may be used to treat diarrheal illness in companion animals. It is important to monitor antimicrobial use to better understand trends and patterns in antimicrobial resistance. There is no monitoring of antimicrobial use in companion animals in Canada. To explore how the use of electronic medical records could contribute to the ongoing, systematic collection of antimicrobial use data in companion animals, anonymized electronic medical records were extracted from 12 participating companion animal practices and warehoused at the University of Calgary. We used the pre-diagnostic, clinical features of diarrhea as the case definition in this study. Using text-mining technologies, cases of diarrhea were described by each of the following variables: diagnostic laboratory tests performed, the etiological diagnosis and antimicrobial therapies. The ability of the text miner to accurately describe the cases for each of the variables was evaluated. It could not reliably classify cases in terms of diagnostic tests or etiological diagnosis; a manual review of a random sample of 500 diarrhea cases determined that 88/500 (17.6%) of the target cases underwent diagnostic testing of which 36/88 (40.9%) had an etiological diagnosis. Text mining, compared to a human reviewer, could accurately identify cases that had been treated with antimicrobials with high sensitivity (92%, 95% confidence interval, 88.1%-95.4%) and specificity (85%, 95% confidence interval, 80.2%-89.1%). Overall, 7400/15,928 (46.5%) of pets presenting with diarrhea were treated with antimicrobials. Some temporal trends and patterns of the antimicrobial use are described. The results from this study suggest that informatics and the electronic medical records could be useful for monitoring trends in antimicrobial use."," Animals; Anti-Infective Agents/*therapeutic use; Canada/epidemiology; Clinical Laboratory Techniques/statistics & numerical data; Data Mining/methods; Diagnosis, Differential; Diarrhea/diagnosis/*drug therapy/epidemiology/*veterinary; *Electronic Health Records/statistics & numerical data; Humans; Medical Informatics; *Pets/microbiology; Practice Patterns, Physicians'/statistics & numerical data/trends; Veterinarians/statistics & numerical data",-2,,
324,Wang,2014,PloS one,"Hematoma shape, hematoma size, Glasgow coma scale score and ICH score: which predicts the 30-day mortality better for intracerebral hematoma?","PURPOSE: To investigate the performance of hematoma shape, hematoma size, Glasgow coma scale (GCS) score, and intracerebral hematoma (ICH) score in predicting the 30-day mortality for ICH patients. To examine the influence of the estimation error of hematoma size on the prediction of 30-day mortality. MATERIALS AND METHODS: This retrospective study, approved by a local institutional review board with written informed consent waived, recruited 106 patients diagnosed as ICH by non-enhanced computed tomography study. The hemorrhagic shape, hematoma size measured by computer-assisted volumetric analysis (CAVA) and estimated by ABC/2 formula, ICH score and GCS score was examined. The predicting performance of 30-day mortality of the aforementioned variables was evaluated. Statistical analysis was performed using Kolmogorov-Smirnov tests, paired t test, nonparametric test, linear regression analysis, and binary logistic regression. The receiver operating characteristics curves were plotted and areas under curve (AUC) were calculated for 30-day mortality. A P value less than 0.05 was considered as statistically significant. RESULTS: The overall 30-day mortality rate was 15.1% of ICH patients. The hematoma shape, hematoma size, ICH score, and GCS score all significantly predict the 30-day mortality for ICH patients, with an AUC of 0.692 (P = 0.0018), 0.715 (P = 0.0008) (by ABC/2) to 0.738 (P = 0.0002) (by CAVA), 0.877 (P<0.0001) (by ABC/2) to 0.882 (P<0.0001) (by CAVA), and 0.912 (P<0.0001), respectively. CONCLUSION: Our study shows that hematoma shape, hematoma size, ICH scores and GCS score all significantly predict the 30-day mortality in an increasing order of AUC. The effect of overestimation of hematoma size by ABC/2 formula in predicting the 30-day mortality could be remedied by using ICH score."," Area Under Curve; Cerebral Hemorrhage/*diagnosis/*mortality; Cone-Beam Computed Tomography; *Glasgow Coma Scale; Hematoma/*pathology; Humans; Predictive Value of Tests; Regression Analysis; *Research Design; Retrospective Studies; Statistics, Nonparametric",-2,,
325,Spasic,2014,International journal of medical informatics,Text mining of cancer-related information: review of current status and future directions,"PURPOSE: This paper reviews the research literature on text mining (TM) with the aim to find out (1) which cancer domains have been the subject of TM efforts, (2) which knowledge resources can support TM of cancer-related information and (3) to what extent systems that rely on knowledge and computational methods can convert text data into useful clinical information. These questions were used to determine the current state of the art in this particular strand of TM and suggest future directions in TM development to support cancer research. METHODS: A review of the research on TM of cancer-related information was carried out. A literature search was conducted on the Medline database as well as IEEE Xplore and ACM digital libraries to address the interdisciplinary nature of such research. The search results were supplemented with the literature identified through Google Scholar. RESULTS: A range of studies have proven the feasibility of TM for extracting structured information from clinical narratives such as those found in pathology or radiology reports. In this article, we provide a critical overview of the current state of the art for TM related to cancer. The review highlighted a strong bias towards symbolic methods, e.g. named entity recognition (NER) based on dictionary lookup and information extraction (IE) relying on pattern matching. The F-measure of NER ranges between 80% and 90%, while that of IE for simple tasks is in the high 90s. To further improve the performance, TM approaches need to deal effectively with idiosyncrasies of the clinical sublanguage such as non-standard abbreviations as well as a high degree of spelling and grammatical errors. This requires a shift from rule-based methods to machine learning following the success of similar trends in biological applications of TM. Machine learning approaches require large training datasets, but clinical narratives are not readily available for TM research due to privacy and confidentiality concerns. This issue remains the main bottleneck for progress in this area. In addition, there is a need for a comprehensive cancer ontology that would enable semantic representation of textual information found in narrative reports.", Computational Biology/*methods; Data Mining/*trends; Humans; Information Storage and Retrieval; *Medical Oncology; *Neoplasms; Cancer; Data mining; Electronic medical records; Natural language processing,-1,,
326,Stapleton,2014,PloS one,Polar bears from space: assessing satellite imagery as a tool to track Arctic wildlife,"Development of efficient techniques for monitoring wildlife is a priority in the Arctic, where the impacts of climate change are acute and remoteness and logistical constraints hinder access. We evaluated high resolution satellite imagery as a tool to track the distribution and abundance of polar bears. We examined satellite images of a small island in Foxe Basin, Canada, occupied by a high density of bears during the summer ice-free season. Bears were distinguished from other light-colored spots by comparing images collected on different dates. A sample of ground-truthed points demonstrated that we accurately classified bears. Independent observers reviewed images and a population estimate was obtained using mark-recapture models. This estimate (N: 94; 95% Confidence Interval: 92-105) was remarkably similar to an abundance estimate derived from a line transect aerial survey conducted a few days earlier (N: 102; 95% CI: 69-152). Our findings suggest that satellite imagery is a promising tool for monitoring polar bears on land, with implications for use with other Arctic wildlife. Large scale applications may require development of automated detection processes to expedite review and analysis. Future research should assess the utility of multi-spectral imagery and examine sites with different environmental characteristics.", Animal Distribution; Animals; Arctic Regions; Female; Islands; Nunavut; Satellite Imagery; Ursidae/*physiology,-2,,
327,Karagianni,2014,Studies in health technology and informatics,Breast cancer in social media: a literature review,, Breast Neoplasms/*diagnosis/*therapy; Consumer Health Information/*statistics & numerical data; Evidence-Based Medicine/*statistics & numerical data; Female; Humans; Information Seeking Behavior; Natural Language Processing; Periodicals as Topic/*statistics & numerical data; Social Media/*statistics & numerical data,-2,,
328,Bahkali,2014,Studies in health technology and informatics,The state public health informatics in saudi arabia,"The purpose of this exploratory study is to provide an overview on the state of Public health informatics (PHI) in the Kingdom of Saudi Arabia (KSA). The study defines PHI and discusses the current status and future challenges which face the Saudi health system. Data collection methods included interviews with public health and PHI experts, and database search, using relevant keyword terms in PubMed. Results of this research show that public health information systems (PHIS) are not well-developed to deliver efficient health care in Saudi Arabia. There are several challenges that need to be addressed with the implementation of PHIS such as the need for readiness assessment, resistant to change, integration of systems, and confidentiality and privacy of health information. Future challenges include profiling users, developing a national PHIS and monitoring the impact of PHIS on healthcare outcomes need to be addressed.", Forecasting; Health Care Reform/*trends; Public Health/*trends; Public Health Informatics/*trends; Telemedicine/*trends,-2,,
329,Walker,2014,Journal of the American Medical Informatics Association : JAMIA,Using the CER Hub to ensure data quality in a multi-institution smoking cessation study,"Comparative effectiveness research (CER) studies involving multiple institutions with diverse electronic health records (EHRs) depend on high quality data. To ensure uniformity of data derived from different EHR systems and implementations, the CER Hub informatics platform developed a quality assurance (QA) process using tools and data formats available through the CER Hub. The QA process, implemented here in a study of smoking cessation services in primary care, used the 'emrAdapter' tool programmed with a set of quality checks to query large samples of primary care encounter records extracted in accord with the CER Hub common data framework. The tool, deployed to each study site, generated error reports indicating data problems to be fixed locally and aggregate data sharable with the central site for quality review. Across the CER Hub network of six health systems, data completeness and correctness issues were prevalent in the first iteration and were considerably improved after three iterations of the QA process. A common issue encountered was incomplete mapping of local EHR data values to those defined by the common data framework. A highly automated and distributed QA process helped to ensure the correctness and completeness of patient care data extracted from EHRs for a multi-institution CER study in smoking cessation."," *Comparative Effectiveness Research; Datasets as Topic/*standards; Electronic Health Records/*standards; Humans; Internet; Medical Records Systems, Computerized; Quality Control; *Smoking Cessation; Electronic medical records; Research data quality assurance",-2,,
330,Kopcke,2014,Journal of medical Internet research,Employing computers for the recruitment into clinical trials: a comprehensive systematic review,"BACKGROUND: Medical progress depends on the evaluation of new diagnostic and therapeutic interventions within clinical trials. Clinical trial recruitment support systems (CTRSS) aim to improve the recruitment process in terms of effectiveness and efficiency. OBJECTIVE: The goals were to (1) create an overview of all CTRSS reported until the end of 2013, (2) find and describe similarities in design, (3) theorize on the reasons for different approaches, and (4) examine whether projects were able to illustrate the impact of CTRSS. METHODS: We searched PubMed titles, abstracts, and keywords for terms related to CTRSS research. Query results were classified according to clinical context, workflow integration, knowledge and data sources, reasoning algorithm, and outcome. RESULTS: A total of 101 papers on 79 different systems were found. Most lacked details in one or more categories. There were 3 different CTRSS that dominated: (1) systems for the retrospective identification of trial participants based on existing clinical data, typically through Structured Query Language (SQL) queries on relational databases, (2) systems that monitored the appearance of a key event of an existing health information technology component in which the occurrence of the event caused a comprehensive eligibility test for a patient or was directly communicated to the researcher, and (3) independent systems that required a user to enter patient data into an interface to trigger an eligibility assessment. Although the treating physician was required to act for the patient in older systems, it is now becoming increasingly popular to offer this possibility directly to the patient. CONCLUSIONS: Many CTRSS are designed to fit the existing infrastructure of a clinical care provider or the particularities of a trial. We conclude that the success of a CTRSS depends more on its successful workflow integration than on sophisticated reasoning and data processing algorithms. Furthermore, some of the most recent literature suggest that an increase in recruited patients and improvements in recruitment efficiency can be expected, although the former will depend on the error rate of the recruitment process being replaced. Finally, to increase the quality of future CTRSS reports, we propose a checklist of items that should be included."," Algorithms; *Clinical Trials as Topic; Computers; Humans; *Information Systems; *Patient Selection; automation; clinical trials as topic; decision support systems, clinical; patient selection; research subject recruitment",-2,,
331,D'Amore,2014,Journal of the American Medical Informatics Association : JAMIA,Are Meaningful Use Stage 2 certified EHRs ready for interoperability? Findings from the SMART C-CDA Collaborative,"BACKGROUND AND OBJECTIVE: Upgrades to electronic health record (EHR) systems scheduled to be introduced in the USA in 2014 will advance document interoperability between care providers. Specifically, the second stage of the federal incentive program for EHR adoption, known as Meaningful Use, requires use of the Consolidated Clinical Document Architecture (C-CDA) for document exchange. In an effort to examine and improve C-CDA based exchange, the SMART (Substitutable Medical Applications and Reusable Technology) C-CDA Collaborative brought together a group of certified EHR and other health information technology vendors. MATERIALS AND METHODS: We examined the machine-readable content of collected samples for semantic correctness and consistency. This included parsing with the open-source BlueButton.js tool, testing with a validator used in EHR certification, scoring with an automated open-source tool, and manual inspection. We also conducted group and individual review sessions with participating vendors to understand their interpretation of C-CDA specifications and requirements. RESULTS: We contacted 107 health information technology organizations and collected 91 C-CDA sample documents from 21 distinct technologies. Manual and automated document inspection led to 615 observations of errors and data expression variation across represented technologies. Based upon our analysis and vendor discussions, we identified 11 specific areas that represent relevant barriers to the interoperability of C-CDA documents. CONCLUSIONS: We identified errors and permissible heterogeneity in C-CDA documents that will limit semantic interoperability. Our findings also point to several practical opportunities to improve C-CDA document quality and exchange in the coming years."," Certification; Diffusion of Innovation; Electronic Health Records/*standards; *Meaningful Use/legislation & jurisprudence; *Medical Record Linkage; Medical Records Systems, Computerized; Systems Integration; United States; C-cda; Data Exchange; Ehr; Interoperability; Meaningful Use",-2,,
332,Park,2014,Studies in health technology and informatics,Applied nursing informatics research - state-of-the-art methodologies using electronic health record data,"With the pervasive implementation of electronic health records (EHR), new opportunities arise for nursing research through use of EHR data. Increasingly, comparative effectiveness research within and across health systems is conducted to identify the impact of nursing for improving health, health care, and lowering costs of care. Use of EHR data for this type of research requires use of national and internationally recognized nursing terminologies to normalize data. Research methods are evolving as large data sets become available through EHRs. Little is known about the types of research and analytic methods for applied to nursing research using EHR data normalized with nursing terminologies. The purpose of this paper is to report on a subset of a systematic review of peer reviewed studies related to applied nursing informatics research involving EHR data using standardized nursing terminologies."," *Bibliometrics; Data Mining/*methods; Databases, Bibliographic/*statistics & numerical data; Electronic Health Records/*statistics & numerical data; Natural Language Processing; Nursing Informatics/*statistics & numerical data; Nursing Research/*statistics & numerical data; Periodicals as Topic/*statistics & numerical data",-1,,
333,Albers,2014,PloS one,Dynamical phenotyping: using temporal analysis of clinically collected physiologic data to stratify populations,"Using glucose time series data from a well measured population drawn from an electronic health record (EHR) repository, the variation in predictability of glucose values quantified by the time-delayed mutual information (TDMI) was explained using a mechanistic endocrine model and manual and automated review of written patient records. The results suggest that predictability of glucose varies with health state where the relationship (e.g., linear or inverse) depends on the source of the acuity. It was found that on a fine scale in parameter variation, the less insulin required to process glucose, a condition that correlates with good health, the more predictable glucose values were. Nevertheless, the most powerful effect on predictability in the EHR subpopulation was the presence or absence of variation in health state, specifically, in- and out-of-control glucose versus in-control glucose. Both of these results are clinically and scientifically relevant because the magnitude of glucose is the most commonly used indicator of health as opposed to glucose dynamics, thus providing for a connection between a mechanistic endocrine model and direct insight to human health via clinically collected data."," Biomarkers/*analysis; Computer Simulation; Electronic Health Records; Endocrine System/*physiology; Glucose/*analysis; Humans; Models, Biological; Models, Statistical; Outcome Assessment (Health Care); *Phenotype",-2,,
334,Hanauer,2014,Journal of the American Medical Informatics Association : JAMIA,Applying MetaMap to Medline for identifying novel associations in a large clinical dataset: a feasibility analysis,"OBJECTIVE: We describe experiments designed to determine the feasibility of distinguishing known from novel associations based on a clinical dataset comprised of International Classification of Disease, V.9 (ICD-9) codes from 1.6 million patients by comparing them to associations of ICD-9 codes derived from 20.5 million Medline citations processed using MetaMap. Associations appearing only in the clinical dataset, but not in Medline citations, are potentially novel. METHODS: Pairwise associations of ICD-9 codes were independently identified in both the clinical and Medline datasets, which were then compared to quantify their degree of overlap. We also performed a manual review of a subset of the associations to validate how well MetaMap performed in identifying diagnoses mentioned in Medline citations that formed the basis of the Medline associations. RESULTS: The overlap of associations based on ICD-9 codes in the clinical and Medline datasets was low: only 6.6% of the 3.1 million associations found in the clinical dataset were also present in the Medline dataset. Further, a manual review of a subset of the associations that appeared in both datasets revealed that co-occurring diagnoses from Medline citations do not always represent clinically meaningful associations. DISCUSSION: Identifying novel associations derived from large clinical datasets remains challenging. Medline as a sole data source for existing knowledge may not be adequate to filter out widely known associations. CONCLUSIONS: In this study, novel associations were not readily identified. Further improvements in accuracy and relevance for tools such as MetaMap are needed to realize their expected utility.", *Data Mining; Feasibility Studies; Humans; *International Classification of Diseases; *Medline; Natural Language Processing; *Unified Medical Language System; Data Mining; Electronic Health Records; International Classification of Diseases; Medline; Unified Medical Language System,1,,
335,Cheng,2014,BMC bioinformatics,MiningABs: mining associated biomarkers across multi-connected gene expression datasets,"BACKGROUND: Human disease often arises as a consequence of alterations in a set of associated genes rather than alterations to a set of unassociated individual genes. Most previous microarray-based meta-analyses identified disease-associated genes or biomarkers independent of genetic interactions. Therefore, in this study, we present the first meta-analysis method capable of taking gene combination effects into account to efficiently identify associated biomarkers (ABs) across different microarray platforms. RESULTS: We propose a new meta-analysis approach called MiningABs to mine ABs across different array-based datasets. The similarity between paired probe sequences is quantified as a bridge to connect these datasets together. The ABs can be subsequently identified from an ""improved"" common logit model (c-LM) by combining several sibling-like LMs in a heuristic genetic algorithm selection process. Our approach is evaluated with two sets of gene expression datasets: i) 4 esophageal squamous cell carcinoma and ii) 3 hepatocellular carcinoma datasets. Based on an unbiased reciprocal test, we demonstrate that each gene in a group of ABs is required to maintain high cancer sample classification accuracy, and we observe that ABs are not limited to genes common to all platforms. Investigating the ABs using Gene Ontology (GO) enrichment, literature survey, and network analyses indicated that our ABs are not only strongly related to cancer development but also highly connected in a diverse network of biological interactions. CONCLUSIONS: The proposed meta-analysis method called MiningABs is able to efficiently identify ABs from different independently performed array-based datasets, and we show its validity in cancer biology via GO enrichment, literature survey and network analyses. We postulate that the ABs may facilitate novel target and drug discovery, leading to improved clinical treatment. Java source code, tutorial, example and related materials are available at ""http://sourceforge.net/projects/miningabs/""."," Algorithms; Biomarkers, Tumor/genetics; Data Mining/*methods; *Gene Expression; *Gene Expression Profiling/methods; Genetic Markers/*genetics; Humans; Neoplasms/genetics",-2,,
336,Baxter,2014,BMC medical research methodology,Using logic model methods in systematic review synthesis: describing complex pathways in referral management interventions,"BACKGROUND: There is increasing interest in innovative methods to carry out systematic reviews of complex interventions. Theory-based approaches, such as logic models, have been suggested as a means of providing additional insights beyond that obtained via conventional review methods. METHODS: This paper reports the use of an innovative method which combines systematic review processes with logic model techniques to synthesise a broad range of literature. The potential value of the model produced was explored with stakeholders. RESULTS: The review identified 295 papers that met the inclusion criteria. The papers consisted of 141 intervention studies and 154 non-intervention quantitative and qualitative articles. A logic model was systematically built from these studies. The model outlines interventions, short term outcomes, moderating and mediating factors and long term demand management outcomes and impacts. Interventions were grouped into typologies of practitioner education, process change, system change, and patient intervention. Short-term outcomes identified that may result from these interventions were changed physician or patient knowledge, beliefs or attitudes and also interventions related to changed doctor-patient interaction. A range of factors which may influence whether these outcomes lead to long term change were detailed. Demand management outcomes and intended impacts included content of referral, rate of referral, and doctor or patient satisfaction. CONCLUSIONS: The logic model details evidence and assumptions underpinning the complex pathway from interventions to demand management impact. The method offers a useful addition to systematic review methodologies. TRIAL REGISTRATION NUMBER: PROSPERO registration number: CRD42013004037."," *Data Mining; *Disease Management; Health Knowledge, Attitudes, Practice; Humans; Models, Theoretical; Patient Satisfaction; Physician-Patient Relations; *Referral and Consultation",-1,,
337,Habibi,2014,BMC bioinformatics,A review of machine learning methods to predict the solubility of overexpressed recombinant proteins in Escherichia coli,"BACKGROUND: Over the last 20 years in biotechnology, the production of recombinant proteins has been a crucial bioprocess in both biopharmaceutical and research arena in terms of human health, scientific impact and economic volume. Although logical strategies of genetic engineering have been established, protein overexpression is still an art. In particular, heterologous expression is often hindered by low level of production and frequent fail due to opaque reasons. The problem is accentuated because there is no generic solution available to enhance heterologous overexpression. For a given protein, the extent of its solubility can indicate the quality of its function. Over 30% of synthesized proteins are not soluble. In certain experimental circumstances, including temperature, expression host, etc., protein solubility is a feature eventually defined by its sequence. Until now, numerous methods based on machine learning are proposed to predict the solubility of protein merely from its amino acid sequence. In spite of the 20 years of research on the matter, no comprehensive review is available on the published methods. RESULTS: This paper presents an extensive review of the existing models to predict protein solubility in Escherichia coli recombinant protein overexpression system. The models are investigated and compared regarding the datasets used, features, feature selection methods, machine learning techniques and accuracy of prediction. A discussion on the models is provided at the end. CONCLUSIONS: This study aims to investigate extensively the machine learning based methods to predict recombinant protein solubility, so as to offer a general as well as a detailed understanding for researches in the field. Some of the models present acceptable prediction performances and convenient user interfaces. These models can be considered as valuable tools to predict recombinant protein overexpression results before performing real laboratory experiments, thus saving labour, time and cost.", Amino Acid Sequence; *Artificial Intelligence; Escherichia coli/*genetics/metabolism; Recombinant Proteins/biosynthesis/*chemistry; Solubility,-2,,
338,Kabudula,2014,BMC medical research methodology,The promise of record linkage for assessing the uptake of health services in resource constrained settings: a pilot study from South Africa,"BACKGROUND: Health and Demographic Surveillance Systems (HDSS) have been instrumental in advancing population and health research in low- and middle- income countries where vital registration systems are often weak. However, the utility of HDSS would be enhanced if their databases could be linked with those of local health facilities. We assess the feasibility of record linkage in rural South Africa using data from the Agincourt HDSS and a local health facility. METHODS: Using a gold standard dataset of 623 record pairs matched by means of fingerprints, we evaluate twenty record linkage scenarios (involving different identifiers, string comparison techniques and with and without clerical review) based on the Fellegi-Sunter probabilistic record linkage model. Matching rates and quality are measured by their sensitivity and positive predictive value (PPV). Background characteristics of matched and unmatched cases are compared to assess systematic bias in the resulting record-linked dataset. RESULTS: A hybrid approach of deterministic followed by probabilistic record linkage, and scenarios that use an extended set of identifiers including another household member's first name yield the best results. The best fully automated record linkage scenario has a sensitivity of 83.6% and PPV of 95.1%. The sensitivity and PPV increase to 84.3% and 96.9%, respectively, when clerical review is undertaken on 10% of the record pairs. The likelihood of being linked is significantly lower for females, non-South Africans and the elderly. CONCLUSION: Using records matched by means of fingerprints as the gold standard, we have demonstrated the feasibility of fully automated probabilistic record linkage using identifiers that are routinely collected in health facilities in South Africa. Our study also shows that matching statistics can be improved if other identifiers (e.g., another household member's first name) are added to the set of matching variables, and, to a lesser extent, with clerical review. Matching success is, however, correlated with background characteristics that are indicative of the instability of personal attributes over time (e.g., surname in the case of women) or with misreporting (e.g., age)."," Adolescent; Adult; Aged; Aging; Confidentiality; Databases, Factual; *Dermatoglyphics; Developing Countries; Female; Health Services/*statistics & numerical data; *Health Services Research; Humans; Male; Medical Record Linkage/*methods; Middle Aged; Patient Identification Systems; Pilot Projects; Population Surveillance/*methods; Registries; South Africa; Young Adult",-2,,
339,Girardi,2014,Studies in health technology and informatics,Supporting knowledge discovery in medicine,"Our ontology-based benchmarking infrastructure for hospitals, we presented on the eHealth 2012, has meanwhile proven useful. Besides, we gathered manifold experience in supporting knowledge discovery in medicine. This also led to further functions and plans with our software. We could confirm and extent our experience by a literature review on the knowledge discovery process in medicine, visual analytics and data mining and drafted an according approach for extending our software. We validated our approach by exemplarily implementing a parallel-coordinate data visualization into our software and plan to integrate further algorithms for visual analytics and machine learning to support knowledge discovery in medicine in diverse ways. This is very promising but can also fail due to technical or organizational details.", Algorithms; *Artificial Intelligence; Austria; Data Mining/*standards; Electronic Health Records/*standards; *Guidelines as Topic; Meaningful Use/*standards; Software/*standards; *User-Computer Interface,-1,,
340,Wieland,2014,PloS one,Who has used internal company documents for biomedical and public health research and where did they find them?,"OBJECTIVE: To describe the sources of internal company documents used in public health and healthcare research. METHODS: We searched PubMed and Embase for articles using internal company documents to address a research question about a health-related topic. Our primary interest was where authors obtained internal company documents for their research. We also extracted information on type of company, type of research question, type of internal documents, and funding source. RESULTS: Our searches identified 9,305 citations of which 357 were eligible. Scanning of reference lists and consultation with colleagues identified 4 additional articles, resulting in 361 included articles. Most articles examined internal tobacco company documents (325/361; 90%). Articles using documents from pharmaceutical companies (20/361; 6%) were the next most common. Tobacco articles used documents from repositories; pharmaceutical documents were from a range of sources. Most included articles relied upon internal company documents obtained through litigation (350/361; 97%). The research questions posed were primarily about company strategies to promote or position the company and its products (326/361; 90%). Most articles (346/361; 96%) used information from miscellaneous documents such as memos or letters, or from unspecified types of documents. When explicit information about study funding was provided (290/361 articles), the most common source was the US-based National Cancer Institute. We developed an alternative and more sensitive search targeted at identifying additional research articles using internal pharmaceutical company documents, but the search retrieved an impractical number of citations for review. CONCLUSIONS: Internal company documents provide an excellent source of information on health topics (e.g., corporate behavior, study data) exemplified by articles based on tobacco industry documents. Pharmaceutical and other industry documents appear to have been less used for research, indicating a need for funding for this type of research and well-indexed and curated repositories to provide researchers with ready access to the documents.", Biomedical Research/*methods; Data Mining/*methods; *Industry; PubMed/*statistics & numerical data; Public Health/*methods; Records,-2,,
342,Gianola,2014,PloS one,Enhancing genome-enabled prediction by bagging genomic BLUP,"We examined whether or not the predictive ability of genomic best linear unbiased prediction (GBLUP) could be improved via a resampling method used in machine learning: bootstrap aggregating sampling (""bagging""). In theory, bagging can be useful when the predictor has large variance or when the number of markers is much larger than sample size, preventing effective regularization. After presenting a brief review of GBLUP, bagging was adapted to the context of GBLUP, both at the level of the genetic signal and of marker effects. The performance of bagging was evaluated with four simulated case studies including known or unknown quantitative trait loci, and an application was made to real data on grain yield in wheat planted in four environments. A metric aimed to quantify candidate-specific cross-validation uncertainty was proposed and assessed; as expected, model derived theoretical reliabilities bore no relationship with cross-validation accuracy. It was found that bagging can ameliorate predictive performance of GBLUP and make it more robust against over-fitting. Seemingly, 25-50 bootstrap samples was enough to attain reasonable predictions as well as stable measures of individual predictive mean squared errors."," Algorithms; Computational Biology/*methods; Computer Simulation; Genetic Markers; *Genome; Genomics/*methods; Genotype; Least-Squares Analysis; Linear Models; Models, Genetic; Phenotype; Predictive Value of Tests; *Quantitative Trait Loci; Quantitative Trait, Heritable; Reproducibility of Results; Sample Size; Triticum/genetics",-2,,
343,Shiraishi,2014,PloS one,The effect of additional training on motor outcomes at discharge from recovery phase rehabilitation wards: a survey from multi-center stroke data bank in Japan,"OBJECTIVES: The purpose of the present study was to examine the potential benefits of additional training in patients admitted to recovery phase rehabilitation ward using the data bank of post-stroke patient registry. SUBJECTS AND METHODS: Subjects were 2507 inpatients admitted to recovery phase rehabilitation wards between November 2004 and November 2010. Participants were retrospectively divided into four groups based upon chart review; patients who received no additional rehabilitation, patients who were added with self-initiated off hours training, patients who were added with off hours training by ward staff, patients who received both self-initiated training and training by ward staff. Parameters for assessing outcomes included length of stay, motor/cognitive subscales of functional independent measures (FIM) and motor benefit of FIM calculated by subtracting the score at admission from that at discharge. RESULTS: Participants were stratified into three groups depending on the motor FIM at admission (<==28, 29 approximately 56, 57<==) for comparison. Regarding outcome variables, significant inter-group differences were observed in all items examined within the subgroup who scored 28 or less and between 29 and 56. Meanwhile no such trends were observed in the group who scored 57 or more compared with those who scored less. In a decision tree created based upon Exhaustive Chi-squared Automatic Interaction Detection method, variables chosen were the motor FIM at admission (the first node) additional training (the second node), the cognitive FIM at admission(the third node). CONCLUSIONS: Overall the results suggest that additional training can compensate for the shortage of regular rehabilitation implemented in recovery phase rehabilitation ward, thus may contribute to improved outcomes assessed by motor FIM at discharge."," Adult; Aged; Aged, 80 and over; Cognition/*physiology; *Disability Evaluation; Female; Humans; Japan; Male; Patient Discharge; Patient Outcome Assessment; *Recovery of Function; Rehabilitation Centers; Stroke/physiopathology/therapy; *Stroke Rehabilitation",-2,,
344,Gan,2014,BMC bioinformatics,MAAMD: a workflow to standardize meta-analyses and comparison of affymetrix microarray data,"BACKGROUND: Mandatory deposit of raw microarray data files for public access, prior to study publication, provides significant opportunities to conduct new bioinformatics analyses within and across multiple datasets. Analysis of raw microarray data files (e.g. Affymetrix CEL files) can be time consuming, complex, and requires fundamental computational and bioinformatics skills. The development of analytical workflows to automate these tasks simplifies the processing of, improves the efficiency of, and serves to standardize multiple and sequential analyses. Once installed, workflows facilitate the tedious steps required to run rapid intra- and inter-dataset comparisons. RESULTS: We developed a workflow to facilitate and standardize Meta-Analysis of Affymetrix Microarray Data analysis (MAAMD) in Kepler. Two freely available stand-alone software tools, R and AltAnalyze were embedded in MAAMD. The inputs of MAAMD are user-editable csv files, which contain sample information and parameters describing the locations of input files and required tools. MAAMD was tested by analyzing 4 different GEO datasets from mice and drosophila.MAAMD automates data downloading, data organization, data quality control assesment, differential gene expression analysis, clustering analysis, pathway visualization, gene-set enrichment analysis, and cross-species orthologous-gene comparisons. MAAMD was utilized to identify gene orthologues responding to hypoxia or hyperoxia in both mice and drosophila. The entire set of analyses for 4 datasets (34 total microarrays) finished in ~ one hour. CONCLUSIONS: MAAMD saves time, minimizes the required computer skills, and offers a standardized procedure for users to analyze microarray datasets and make new intra- and inter-dataset comparisons."," Animals; Computational Biology/*methods; *Databases, Genetic; Drosophila; *Meta-Analysis as Topic; Mice; Oligonucleotide Array Sequence Analysis/*methods; Quality Control; *Software",-2,,
345,O'Mara-Eves,2014,Research synthesis methods,Techniques for identifying cross-disciplinary and 'hard-to-detect' evidence for systematic review,"Driven by necessity in our own complex review, we developed alternative systematic ways of identifying relevant evidence where the key concepts are generally not focal to the primary studies' aims and are found across multiple disciplines-that is, hard-to-detect evidence. Specifically, we sought to identify evidence on community engagement in public health interventions that aim to reduce health inequalities. Our initial search strategy used text mining to identify synonyms for the concept 'community engagement'. We conducted a systematic search for reviews on public health interventions, supplemented by searches of trials databases. We then used information in the reviews' evidence tables to gather more information about the included studies than was evident in the primary studies' own titles or abstracts. We identified 319 primary studies cited in reviews after full-text screening. In this paper, we retrospectively reflect on the challenges and benefits of the approach taken. We estimate that more than a quarter of the studies that were identified would have been missed by typical searching and screening methods. This identification strategy was highly effective and could be useful for reviews of broad research questions, or where the key concepts are unlikely to be the main focus of primary research."," Data Mining/*methods; *Evidence-Based Medicine; *Interdisciplinary Studies; *Natural Language Processing; Periodicals as Topic; Research Design; *Review Literature as Topic; *Vocabulary, Controlled; information retrieval; review literature as topic; screening; searching; text mining",2,,
346,Shemilt,2014,Research synthesis methods,Pinpointing needles in giant haystacks: use of text mining to reduce impractical screening workload in extremely large scoping reviews,"In scoping reviews, boundaries of relevant evidence may be initially fuzzy, with refined conceptual understanding of interventions and their proposed mechanisms of action an intended output of the scoping process rather than its starting point. Electronic searches are therefore sensitive, often retrieving very large record sets that are impractical to screen in their entirety. This paper describes methods for applying and evaluating the use of text mining (TM) technologies to reduce impractical screening workload in reviews, using examples of two extremely large-scale scoping reviews of public health evidence (choice architecture (CA) and economic environment (EE)). Electronic searches retrieved >800,000 (CA) and >1 million (EE) records. TM technologies were used to prioritise records for manual screening. TM performance was measured prospectively. TM reduced manual screening workload by 90% (CA) and 88% (EE) compared with conventional screening (absolute reductions of approximately 430 000 (CA) and approximately 378 000 (EE) records). This study expands an emerging corpus of empirical evidence for the use of TM to expedite study selection in reviews. By reducing screening workload to manageable levels, TM made it possible to assemble and configure large, complex evidence bases that crossed research discipline boundaries. These methods are transferable to other scoping and systematic reviews incorporating conceptual development or explanatory dimensions."," Data Mining/*methods; Machine Learning; *Natural Language Processing; Pattern Recognition, Automated/methods; *Periodicals as Topic; *Review Literature as Topic; *Vocabulary, Controlled; *Workload; scoping review methods; study selection; systematic review methods; text mining",2,,
347,Campbell,2014,BMC bioinformatics,Toolboxes for a standardised and systematic study of glycans,"BACKGROUND: Recent progress in method development for characterising the branched structures of complex carbohydrates has now enabled higher throughput technology. Automation of structure analysis then calls for software development since adding meaning to large data collections in reasonable time requires corresponding bioinformatics methods and tools. Current glycobioinformatics resources do cover information on the structure and function of glycans, their interaction with proteins or their enzymatic synthesis. However, this information is partial, scattered and often difficult to find to for non-glycobiologists. METHODS: Following our diagnosis of the causes of the slow development of glycobioinformatics, we review the ""objective"" difficulties encountered in defining adequate formats for representing complex entities and developing efficient analysis software. RESULTS: Various solutions already implemented and strategies defined to bridge glycobiology with different fields and integrate the heterogeneous glyco-related information are presented. CONCLUSIONS: Despite the initial stage of our integrative efforts, this paper highlights the rapid expansion of glycomics, the validity of existing resources and the bright future of glycobioinformatics.", Carbohydrate Sequence; Glycomics/*methods/standards; Internet; Polysaccharides/*analysis/chemistry; Proteins/chemistry/metabolism; Software,-2,,
348,Gupta,2014,Journal of the American Medical Informatics Association : JAMIA,Effect of clinical decision support on documented guideline adherence for head CT in emergency department patients with mild traumatic brain injury,"Imaging utilization in emergency departments (EDs) has increased significantly. More than half of the 1.2 million patients with mild traumatic brain injury (MTBI) presenting to US EDs receive head CT. While evidence-based guidelines can help emergency clinicians decide whether to obtain head CT in these patients, adoption of these guidelines has been highly variable. Promulgation of imaging efficiency guidelines by the National Quality Forum has intensified the need for performance reporting, but measuring adherence to these imaging guidelines currently requires labor-intensive and potentially inaccurate manual chart review. We implemented clinical decision support (CDS) based on published evidence to guide emergency clinicians towards appropriate head CT use in patients with MTBI and automated data capture needed for unambiguous guideline adherence metrics. Implementation of the CDS was associated with a 56% relative increase in documented adherence to evidence-based guidelines for imaging in ED patients with MTBI."," Adult; Brain Injuries/*diagnostic imaging; *Decision Support Systems, Clinical; Female; *Guideline Adherence; Humans; Male; Medical Order Entry Systems; Middle Aged; Practice Guidelines as Topic; Tomography, X-Ray Computed/standards/*statistics & numerical data; Trauma Centers; Computerized Physician Order Entry System; Decision Support Systems, Clinical; Electronic Health Records; Evidence-Based Practice; Radiology",-2,,
349,Hu,2014,PloS one,Efficacy and safety of aldose reductase inhibitor for the treatment of diabetic cardiovascular autonomic neuropathy: systematic review and meta-analysis,"BACKGROUND: Aldose reductase inhibitors (ARIs) can block the metabolism of the polyol pathway, and have been used to slow or reverse the progression of diabetic cardiovascular autonomic neuropathy (DCAN). The purpose of this study was to review the effectiveness and safety of ARIs in the treatment of DCAN as determined by five cardiac autonomic neuropathy function tests. METHODS: CENTRAL, MEDLINE, EMBASE, Scopus databases (inception to May 2012) were searched to identify randomized controlled trials (RCTs) and non-randomized controlled trials (non-RCTs) investigating ARIs for the treatment of DCAN with an English-language restriction. The data were analyzed using RevMan 5.0, and the heterogeneity between the trials was evaluated using the Cochrane's Q-test as well as the I(2) test. The type of model (random or fixed) used for analysis was based on heterogeneity. Weighted mean differences (WMD) with 95% confidence intervals (CI) were computed for the five cardiac automatic neuropathy function tests to evaluate the effects. RESULTS: Ten articles met the prerequisites for this review. Analysis of the results showed that ARIs significantly improved function in at least three of the five automatic neuropathy tests, including the resting heart rate variation coefficients (WMD = 0.25, 95%CI 0.02 to 0.48, P = 0.040); the 30ratio15 ratio (WMD = 0.06, 95%CI 0.01 to 0.10, P = 0.010) and the postural systolic blood pressure change (WMD = -5.94, 95%CI -7.31 to -4.57, P = 0.001). The expiration/inspiration ratio showed a marginally significant benefit (WMD = 0.05, 95%CI 0.00 to 0.09, P = 0.040). Glycaemic control was not significantly affected by ARIs. Adverse effects of ARIs except for Tolerestat were minimal. CONCLUSIONS: Based on these results, we conclude that ARIs could ameliorate cardiac automatic neuropathy especially mild or asymptomatic DCAN but need further investigation."," Aldehyde Reductase/*antagonists & inhibitors; Blood Pressure; Cardiovascular Diseases/*drug therapy; Diabetes Complications/drug therapy; Diabetic Neuropathies/*drug therapy; Enzyme Inhibitors/*therapeutic use; Glycated Hemoglobin A/chemistry; Heart Rate; Humans; Models, Statistical; Randomized Controlled Trials as Topic; Reproducibility of Results; Treatment Outcome",-2,,
350,Orphanou,2014,Artificial intelligence in medicine,Temporal abstraction and temporal Bayesian networks in clinical domains: a survey,"OBJECTIVES: Temporal abstraction (TA) of clinical data aims to abstract and interpret clinical data into meaningful higher-level interval concepts. Abstracted concepts are used for diagnostic, prediction and therapy planning purposes. On the other hand, temporal Bayesian networks (TBNs) are temporal extensions of the known probabilistic graphical models, Bayesian networks. TBNs can represent temporal relationships between events and their state changes, or the evolution of a process, through time. This paper offers a survey on techniques/methods from these two areas that were used independently in many clinical domains (e.g. diabetes, hepatitis, cancer) for various clinical tasks (e.g. diagnosis, prognosis). A main objective of this survey, in addition to presenting the key aspects of TA and TBNs, is to point out important benefits from a potential integration of TA and TBNs in medical domains and tasks. The motivation for integrating these two areas is their complementary function: TA provides clinicians with high level views of data while TBNs serve as a knowledge representation and reasoning tool under uncertainty, which is inherent in all clinical tasks. METHODS: Key publications from these two areas of relevance to clinical systems, mainly circumscribed to the latest two decades, are reviewed and classified. TA techniques are compared on the basis of: (a) knowledge acquisition and representation for deriving TA concepts and (b) methodology for deriving basic and complex temporal abstractions. TBNs are compared on the basis of: (a) representation of time, (b) knowledge representation and acquisition, (c) inference methods and the computational demands of the network, and (d) their applications in medicine. RESULTS: The survey performs an extensive comparative analysis to illustrate the separate merits and limitations of various TA and TBN techniques used in clinical systems with the purpose of anticipating potential gains through an integration of the two techniques, thus leading to a unified methodology for clinical systems. The surveyed contributions are evaluated using frameworks of respective key features. In addition, for the evaluation of TBN methods, a unifying clinical domain (diabetes) is used. CONCLUSION: The main conclusion transpiring from this review is that techniques/methods from these two areas, that so far are being largely used independently of each other in clinical domains, could be effectively integrated in the context of medical decision-support systems. The anticipated key benefits of the perceived integration are: (a) during problem solving, the reasoning can be directed at different levels of temporal and/or conceptual abstractions since the nodes of the TBNs can be complex entities, temporally and structurally and (b) during model building, knowledge generated in the form of basic and/or complex abstractions, can be deployed in a TBN.", *Artificial Intelligence; *Bayes Theorem; Clinical Trials as Topic/*methods; Knowledge; Time; Bayesian networks; Medical knowledge-based systems; Temporal Bayesian networks; Temporal abstraction; Temporal reasoning,-1,,
341,Kajungu,2014,PloS one,Paediatric pharmacovigilance: use of pharmacovigilance data mining algorithms for signal detection in a safety dataset of a paediatric clinical study conducted in seven African countries,"BACKGROUND: Pharmacovigilance programmes monitor and help ensuring the safe use of medicines which is critical to the success of public health programmes. The commonest method used for discovering previously unknown safety risks is spontaneous notifications. In this study we examine the use of data mining algorithms to identify signals from adverse events reported in a phase IIIb/IV clinical trial evaluating the efficacy and safety of several Artemisinin-based combination therapies (ACTs) for treatment of uncomplicated malaria in African children. METHODS: We used paediatric safety data from a multi-site, multi-country clinical study conducted in seven African countries (Burkina Faso, Gabon, Nigeria, Rwanda, Uganda, Zambia, and Mozambique). Each site compared three out of four ACTs, namely amodiaquine-artesunate (ASAQ), dihydroartemisinin-piperaquine (DHAPQ), artemether-lumefantrine (AL) or chlorproguanil/dapsone and artesunate (CD+A). We examine two pharmacovigilance signal detection methods, namely proportional reporting ratio and Bayesian Confidence Propagation Neural Network on the clinical safety dataset. RESULTS: Among the 4,116 children (6-59 months old) enrolled and followed up for 28 days post treatment, a total of 6,238 adverse events were reported resulting into 346 drug-event combinations. Nine signals were generated both by proportional reporting ratio and Bayesian Confidence Propagation Neural Network. A review of the manufacturer package leaflets, an online Multi-Drug Symptom/Interaction Checker (DoubleCheckMD) and further by therapeutic area experts reduced the number of signals to five. The ranking of some drug-adverse reaction pairs on the basis of their signal index differed between the two methods. CONCLUSIONS: Our two data mining methods were equally able to generate suspected signals using the pooled safety data from a phase IIIb/IV clinical trial. This analysis demonstrated the possibility of utilising clinical studies safety data for key pharmacovigilance activities like signal detection and evaluation. This approach can be applied to complement the spontaneous reporting systems which are limited by under reporting."," Adverse Drug Reaction Reporting Systems; Africa; *Algorithms; Antimalarials/*adverse effects; Artemisinins/*adverse effects; Bayes Theorem; Child, Preschool; Clinical Trials as Topic; Data Mining/*methods; Databases, Factual; Drug Monitoring/methods; Humans; Infant; Malaria/*drug therapy; *Pharmacovigilance",-1,,
351,Wick,2014,BMC bioinformatics,DFLAT: functional annotation for human development,"BACKGROUND: Recent increases in genomic studies of the developing human fetus and neonate have led to a need for widespread characterization of the functional roles of genes at different developmental stages. The Gene Ontology (GO), a valuable and widely-used resource for characterizing gene function, offers perhaps the most suitable functional annotation system for this purpose. However, due in part to the difficulty of studying molecular genetic effects in humans, even the current collection of comprehensive GO annotations for human genes and gene products often lacks adequate developmental context for scientists wishing to study gene function in the human fetus. DESCRIPTION: The Developmental FunctionaL Annotation at Tufts (DFLAT) project aims to improve the quality of analyses of fetal gene expression and regulation by curating human fetal gene functions using both manual and semi-automated GO procedures. Eligible annotations are then contributed to the GO database and included in GO releases of human data. DFLAT has produced a considerable body of functional annotation that we demonstrate provides valuable information about developmental genomics. A collection of gene sets (genes implicated in the same function or biological process), made by combining existing GO annotations with the 13,344 new DFLAT annotations, is available for use in novel analyses. Gene set analyses of expression in several data sets, including amniotic fluid RNA from fetuses with trisomies 21 and 18, umbilical cord blood, and blood from newborns with bronchopulmonary dysplasia, were conducted both with and without the DFLAT annotation. CONCLUSIONS: Functional analysis of expression data using the DFLAT annotation increases the number of implicated gene sets, reflecting the DFLAT's improved representation of current knowledge. Blinded literature review supports the validity of newly significant findings obtained with the DFLAT annotations. Newly implicated significant gene sets also suggest specific hypotheses for future research. Overall, the DFLAT project contributes new functional annotation and gene sets likely to enhance our ability to interpret genomic studies of human fetal and neonatal development."," Amniotic Fluid; *Databases, Genetic; Fetal Development/*genetics; Fetal Diseases/genetics; Genes/genetics/physiology; Genomics/*methods; *Human Development; Humans; Infant, Newborn; Molecular Sequence Annotation/*methods; Vocabulary, Controlled",-2,,
352,Bekhuis,2014,PloS one,Feature engineering and a proposed decision-support system for systematic reviewers of medical evidence,"OBJECTIVES: Evidence-based medicine depends on the timely synthesis of research findings. An important source of synthesized evidence resides in systematic reviews. However, a bottleneck in review production involves dual screening of citations with titles and abstracts to find eligible studies. For this research, we tested the effect of various kinds of textual information (features) on performance of a machine learning classifier. Based on our findings, we propose an automated system to reduce screeing burden, as well as offer quality assurance. METHODS: We built a database of citations from 5 systematic reviews that varied with respect to domain, topic, and sponsor. Consensus judgments regarding eligibility were inferred from published reports. We extracted 5 feature sets from citations: alphabetic, alphanumeric(+), indexing, features mapped to concepts in systematic reviews, and topic models. To simulate a two-person team, we divided the data into random halves. We optimized the parameters of a Bayesian classifier, then trained and tested models on alternate data halves. Overall, we conducted 50 independent tests. RESULTS: All tests of summary performance (mean F3) surpassed the corresponding baseline, P<0.0001. The ranks for mean F3, precision, and classification error were statistically different across feature sets averaged over reviews; P-values for Friedman's test were .045, .002, and .002, respectively. Differences in ranks for mean recall were not statistically significant. Alphanumeric(+) features were associated with best performance; mean reduction in screening burden for this feature type ranged from 88% to 98% for the second pass through citations and from 38% to 48% overall. CONCLUSIONS: A computer-assisted, decision support system based on our methods could substantially reduce the burden of screening citations for systematic review teams and solo reviewers. Additionally, such a system could deliver quality assurance both by confirming concordant decisions and by naming studies associated with discordant decisions for further consideration."," *Artificial Intelligence; Bayes Theorem; Databases, Bibliographic; *Decision Support Techniques; Evidence-Based Medicine/*methods; Publications/*classification; *Review Literature as Topic",2,,
355,Yergens,2014,BMC medical research methodology,An overview of the statistical methods reported by studies using the Canadian community health survey,"BACKGROUND: The Canadian Community Health Survey (CCHS) is a cross-sectional survey that has collected information on health determinants, health status and the utilization of the health system in Canada since 2001. Several hundred articles have been written utilizing the CCHS dataset. Previous analyses of statistical methods utilized in the literature have focused on a particular journal or set of journals to understand the statistical literacy required for understanding the published research. In this study, we describe the statistical methods referenced in the published literature utilizing the CCHS dataset(s). METHODS: A descriptive study was undertaken of references published in Medline, Embase, Web of Knowledge and Scopus associated with the CCHS. These references were imported into a Java application utilizing the searchable Apache Lucene text database and screened based upon pre-defined inclusion and exclusion criteria. Full-text PDF articles that met the inclusion criteria were then used for the identification of descriptive, elementary and regression statistical methods referenced in these articles. The identification of statistical methods occurred through an automated search of key words on the full-text articles utilizing the Java application. RESULTS: We identified 4811 references from the 4 bibliographical databases for possible inclusion. After exclusions, 663 references were used for the analysis. Descriptive statistics such as means or proportions were presented in a majority of the articles (97.7%). Elementary-level statistics such as t-tests were less frequently referenced (29.7%) than descriptive statistics. Regression methods were frequently referenced in the articles: 79.8% of articles contained reference to regression in general with logistic regression appearing most frequently in 67.1% of the articles. CONCLUSIONS: Our study shows a diverse set of analysis methods being referenced in the CCHS literature, however, the literature heavily relies on only a subset of all possible statistical tools. This information can be used in identifying gaps in statistical methods that could be applied to future analysis of public health surveys, insight into training and educational programs, and also identifies the level of statistical literacy needed to understand the published literature."," Canada; Cross-Sectional Studies/statistics & numerical data; Data Interpretation, Statistical; *Health Status; Health Surveys/*statistics & numerical data; Humans; National Health Programs/*statistics & numerical data; Research Design/statistics & numerical data",1,,
354,Lai,2014,Journal of medical Internet research,Caught in the web: a review of web-based suicide prevention,"BACKGROUND: Suicide is a serious and increasing problem worldwide. The emergence of the digital world has had a tremendous impact on people's lives, both negative and positive, including an impact on suicidal behaviors. OBJECTIVE: Our aim was to perform a review of the published literature on Web-based suicide prevention strategies, focusing on their efficacy, benefits, and challenges. METHODS: The EBSCOhost (Medline, PsycINFO, CINAHL), OvidSP, the Cochrane Library, and ScienceDirect databases were searched for literature regarding Web-based suicide prevention strategies from 1997 to 2013 according to the modified PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement. The selected articles were subjected to quality rating and data extraction. RESULTS: Good quality literature was surprisingly sparse, with only 15 fulfilling criteria for inclusion in the review, and most were rated as being medium to low quality. Internet-based cognitive behavior therapy (iCBT) reduced suicidal ideation in the general population in two randomized controlled trial (effect sizes, d=0.04-0.45) and in a clinical audit of depressed primary care patients. Descriptive studies reported improved accessibility and reduced barriers to treatment with Internet among students. Besides automated iCBT, preventive strategies were mainly interactive (email communication, online individual or supervised group support) or information-based (website postings). The benefits and potential challenges of accessibility, anonymity, and text-based communication as key components for Web-based suicide prevention strategies were emphasized. CONCLUSIONS: There is preliminary evidence that suggests the probable benefit of Web-based strategies in suicide prevention. Future larger systematic research is needed to confirm the effectiveness and risk benefit ratio of such strategies.", Humans; *Internet; Suicide/*prevention & control; Internet; Web-based; suicide prevention,2,,
353,Lodder,2014,PloS one,Computer-assisted interpretation of the EEG background pattern: a clinical evaluation,"OBJECTIVE: Interpretation of the EEG background pattern in routine recordings is an important part of clinical reviews. We evaluated the feasibility of an automated analysis system to assist reviewers with evaluation of the general properties in the EEG background pattern. METHODS: Quantitative eeg methods were used to describe the following five background properties: posterior dominant rhythm frequency and reactivity, anterior-posterior gradients, presence of diffuse slow-wave activity and asymmetry. Software running the quantitative methods were given to ten experienced electroencephalographers together with 45 routine EEG recordings and computer-generated reports. Participants were asked to review the EEGs by visual analysis first, and afterwards to compare their findings with the generated reports and correct mistakes made by the system. Corrected reports were returned for comparison. RESULTS: Using a gold-standard derived from the consensus of reviewers, inter-rater agreement was calculated for all reviewers and for automated interpretation. Automated interpretation together with most participants showed high (kappa > 0.6) agreement with the gold standard. In some cases, automated analysis showed higher agreement with the gold standard than participants. When asked in a questionnaire after the study, all participants considered computer-assisted interpretation to be useful for every day use in routine reviews. CONCLUSIONS: Automated interpretation methods proved to be accurate and were considered to be useful by all participants. SIGNIFICANCE: Computer-assisted interpretation of the EEG background pattern can bring consistency to reviewing and improve efficiency and inter-rater agreement."," *Electroencephalography/standards; Humans; Reproducibility of Results; *Signal Processing, Computer-Assisted",-2,,
356,Cho,2014,PloS one,Cumulative sum analysis for experiences of a single-session retrograde intrarenal stone surgery and analysis of predictors for stone-free status,"INTRODUCTION: This study investigated the learning curve of a single-session retrograde intrarenal surgery (RIRS) in patients with mid-sized stones. Competence and trainee proficiency for RIRS was assessed using cumulative sum analysis (CUSUM). MATERIALS AND METHODS: The study design and the use of patients' information stored in the hospital database were approved by the Institutional Review Board of our institution. A retrospective review was performed for 100 patients who underwent a single-session RIRS. Patients were included if the main stone had a maximal diameter between 10 and 30 mm. The presence of a residual stone was checked on postoperative day 1 and at one-month follow-up visit. Fragmentation efficacy was calculated ""removed stone volume (mm(3)) divided by operative time (min)"". CUSUM analysis was used for monitoring change in fragmentation efficacy, and we tested whether or not acceptable surgical outcomes were achieved. RESULTS: The mean age was 54.7+/-14.8 years. Serum creatinine level did not change significantly. Estimated GFR and hemoglobin were within normal limits postoperatively. The CUSUM curve tended to be flat until the 25th case and showed a rising pattern but declined again until the 56th case. After that point, the fragmentation efficacy reached a plateau. The acceptable level of fragmentation efficacy was 25 ml/min. Multivariate logistic regression analyses showed that stone-free rate was significantly lower for cases with multiple stones than those with a single stone (OR = 0.147, CI 0.032 - 0.674, P value = 0.005) and for cases with higher number of sites (OR = 0.676, CI 0.517 - 0.882, P value = 0.004). CONCLUSIONS: The statistical analysis of RIRS learning experience revealed that 56 cases were required for reaching a plateau in the learning curve. The number of stones and the number of sites were significant predictors for stone-free status.", *Artificial Intelligence; Female; Humans; Kidney Calculi/*surgery; Male; Middle Aged; Retrospective Studies; Statistics as Topic/*methods; Time Factors; Treatment Outcome,-2,,
357,Walters,2014,PloS one,Meta-analysis of repository data: impact of data regularization on NIMH schizophrenia linkage results,"Human geneticists are increasingly turning to study designs based on very large sample sizes to overcome difficulties in studying complex disorders. This in turn almost always requires multi-site data collection and processing of data through centralized repositories. While such repositories offer many advantages, including the ability to return to previously collected data to apply new analytic techniques, they also have some limitations. To illustrate, we reviewed data from seven older schizophrenia studies available from the NIMH-funded Center for Collaborative Genomic Studies on Mental Disorders, also known as the Human Genetics Initiative (HGI), and assessed the impact of data cleaning and regularization on linkage analyses. Extensive data regularization protocols were developed and applied to both genotypic and phenotypic data. Genome-wide nonparametric linkage (NPL) statistics were computed for each study, over various stages of data processing. To assess the impact of data processing on aggregate results, Genome-Scan Meta-Analysis (GSMA) was performed. Examples of increased, reduced and shifted linkage peaks were found when comparing linkage results based on original HGI data to results using post-processed data within the same set of pedigrees. Interestingly, reducing the number of affected individuals tended to increase rather than decrease linkage peaks. But most importantly, while the effects of data regularization within individual data sets were small, GSMA applied to the data in aggregate yielded a substantially different picture after data regularization. These results have implications for analyses based on other types of data (e.g., case-control GWAS or sequencing data) as well as data obtained from other repositories."," Data Interpretation, Statistical; Data Mining/*methods; Humans; Medical Informatics/*methods; National Institute of Mental Health (U.S.)/*statistics & numerical data; *Schizophrenia; United States",-1,,
358,de Bruin,2014,Journal of the American Medical Informatics Association : JAMIA,Data use and effectiveness in electronic surveillance of healthcare associated infections in the 21st century: a systematic review,"OBJECTIVE: As more electronic health records have become available during the last decade, we aimed to uncover recent trends in use of electronically available patient data by electronic surveillance systems for healthcare associated infections (HAIs) and identify consequences for system effectiveness. METHODS: A systematic review of published literature evaluating electronic HAI surveillance systems was performed. The PubMed service was used to retrieve publications between January 2001 and December 2011. Studies were included in the review if they accurately described what electronic data were used and if system effectiveness was evaluated using sensitivity, specificity, positive predictive value, or negative predictive value. Trends were identified by analyzing changes in the number and types of electronic data sources used. RESULTS: 26 publications comprising discussions on 27 electronic systems met the eligibility criteria. Trend analysis showed that systems use an increasing number of data sources which are either medico-administrative or clinical and laboratory-based data. Trends on the use of individual types of electronic data confirmed the paramount role of microbiology data in HAI detection, but also showed increased use of biochemistry and pharmacy data, and the limited adoption of clinical data and physician narratives. System effectiveness assessments indicate that the use of heterogeneous data sources results in higher system sensitivity at the expense of specificity. CONCLUSIONS: Driven by the increased availability of electronic patient data, electronic HAI surveillance systems use more data, making systems more sensitive yet less specific, but also allow systems to be tailored to the needs of healthcare institutes' surveillance programs.", Cross Infection/*epidemiology; *Electronic Health Records; *Health Information Systems; Humans; Population Surveillance/*methods; Automatic Data Processing; Cross Infection/methods; Expert Systems; Infection Control; Review Literature as Topic,-2,,
359,Chakravorty,2014,PloS one,Labour-efficient in vitro lymphocyte population tracking and fate prediction using automation and manual review,"Interest in cell heterogeneity and differentiation has recently led to increased use of time-lapse microscopy. Previous studies have shown that cell fate may be determined well in advance of the event. We used a mixture of automation and manual review of time-lapse live cell imaging to track the positions, contours, divisions, deaths and lineage of 44 B-lymphocyte founders and their 631 progeny in vitro over a period of 108 hours. Using this data to train a Support Vector Machine classifier, we were retrospectively able to predict the fates of individual lymphocytes with more than 90% accuracy, using only time-lapse imaging captured prior to mitosis or death of 90% of all cells. The motivation for this paper is to explore the impact of labour-efficient assistive software tools that allow larger and more ambitious live-cell time-lapse microscopy studies. After training on this data, we show that machine learning methods can be used for realtime prediction of individual cell fates. These techniques could lead to realtime cell culture segregation for purposes such as phenotype screening. We were able to produce a large volume of data with less effort than previously reported, due to the image processing, computer vision, tracking and human-computer interaction tools used. We describe the workflow of the software-assisted experiments and the graphical interfaces that were needed. To validate our results we used our methods to reproduce a variety of published data about lymphocyte populations and behaviour. We also make all our data publicly available, including a large quantity of lymphocyte spatio-temporal dynamics and related lineage information."," Cell Tracking; Humans; Image Processing, Computer-Assisted; Lymphocytes/*physiology; Microscopy, Fluorescence; Reproducibility of Results; *Support Vector Machine; *Time-Lapse Imaging",-2,,
360,Jones,2014,PloS one,Automatic extraction of nanoparticle properties using natural language processing: NanoSifter an application to acquire PAMAM dendrimer properties,"In this study, we demonstrate the use of natural language processing methods to extract, from nanomedicine literature, numeric values of biomedical property terms of poly(amidoamine) dendrimers. We have developed a method for extracting these values for properties taken from the NanoParticle Ontology, using the General Architecture for Text Engineering and a Nearly-New Information Extraction System. We also created a method for associating the identified numeric values with their corresponding dendrimer properties, called NanoSifter. We demonstrate that our system can correctly extract numeric values of dendrimer properties reported in the cancer treatment literature with high recall, precision, and f-measure. The micro-averaged recall was 0.99, precision was 0.84, and f-measure was 0.91. Similarly, the macro-averaged recall was 0.99, precision was 0.87, and f-measure was 0.92. To our knowledge, these results are the first application of text mining to extract and associate dendrimer property terms and their corresponding numeric values.", Data Mining/methods; Dendrimers/chemistry; Humans; Information Storage and Retrieval/*methods; Nanoparticles/*chemistry; *Natural Language Processing; Reproducibility of Results,2,,
361,Duke,2014,International journal of medical informatics,Regenstrief Institute's Medical Gopher: a next-generation homegrown electronic medical record system,"OBJECTIVE: Regenstrief Institute developed one of the seminal computerized order entry systems, the Medical Gopher, for implementation at Wishard Hospital nearly three decades ago. Wishard Hospital and Regenstrief remain committed to homegrown software development, and over the past 4 years we have fully rebuilt Gopher with an emphasis on usability, safety, leveraging open source technologies, and the advancement of biomedical informatics research. Our objective in this paper is to summarize the functionality of this new system and highlight its novel features. MATERIALS AND METHODS: Applying a user-centered design process, the new Gopher was built upon a rich-internet application framework using an agile development process. The system incorporates order entry, clinical documentation, result viewing, decision support, and clinical workflow. We have customized its use for the outpatient, inpatient, and emergency department settings. RESULTS: The new Gopher is now in use by over 1100 users a day, including an average of 433 physicians caring for over 3600 patients daily. The system includes a wizard-like clinical workflow, dynamic multimedia alerts, and a familiar 'e-commerce'-based interface for order entry. Clinical documentation is enhanced by real-time natural language processing and data review is supported by a rapid chart search feature. DISCUSSION: As one of the few remaining academically developed order entry systems, the Gopher has been designed both to improve patient care and to support next-generation informatics research. It has achieved rapid adoption within our health system and suggests continued viability for homegrown systems in settings of close collaboration between developers and providers."," Documentation/*trends; Electronic Data Processing; Hospitals, University; Humans; *Information Storage and Retrieval; Medical Records Systems, Computerized/*trends; *Patient Care; *Software; User-Computer Interface; Clinical decision support systems; Computerized physician order entry system; Electronic health record; Health information technology",-2,,
362,Pombo,2014,Artificial intelligence in medicine,Knowledge discovery in clinical decision support systems for pain management: a systematic review,"OBJECTIVE: The occurrence of pain accounts for billions of dollars in annual medical expenditures; loss of quality of life and decreased worker productivity contribute to indirect costs. As pain is highly subjective, clinical decision support systems (CDSSs) can be critical for improving the accuracy of pain assessment and offering better support for clinical decision-making. This review is focused on computer technologies for pain management that allow CDSSs to obtain knowledge from the clinical data produced by either patients or health care professionals. METHODS AND MATERIALS: A comprehensive literature search was conducted in several electronic databases to identify relevant articles focused on computerised systems that constituted CDSSs and include data or results related to pain symptoms from patients with acute or chronic pain, published between 1992 and 2011 in the English language. In total, thirty-nine studies were analysed; thirty-two were selected from 1245 citations, and seven were obtained from reference tracking. RESULTS: The results highlighted the following clusters of computer technologies: rule-based algorithms, artificial neural networks, nonstandard set theory, and statistical learning algorithms. In addition, several methodologies were found for content processing such as terminologies, questionnaires, and scores. The median accuracy ranged from 53% to 87.5%. CONCLUSIONS: Computer technologies that have been applied in CDSSs are important but not determinant in improving the systems' accuracy and the clinical practice, as evidenced by the moderate correlation among the studies. However, these systems play an important role in the design of computerised systems oriented to a patient's symptoms as is required for pain management. Several limitations related to CDSSs were observed: the lack of integration with mobile devices, the reduced use of web-based interfaces, and scarce capabilities for data to be inserted by patients."," *Decision Support Systems, Clinical; Humans; *Knowledge; *Pain Management; Clinical decision support system; Machine learning; Pain measurement; Systematic review",-2,,
363,Silva,2013,Artificial intelligence in medicine,Comparing the accuracy of syndrome surveillance systems in detecting influenza-like illness: GUARDIAN vs. RODS vs. electronic medical record reports,"BACKGROUND: A highly sensitive real-time syndrome surveillance system is critical to detect, monitor, and control infectious disease outbreaks, such as influenza. Direct comparisons of diagnostic accuracy of various surveillance systems are scarce. OBJECTIVE: To statistically compare sensitivity and specificity of multiple proprietary and open source syndrome surveillance systems to detect influenza-like illness (ILI). METHODS: A retrospective, cross-sectional study was conducted utilizing data from 1122 patients seen during November 1-7, 2009 in the emergency department of a single urban academic medical center. The study compared the Geographic Utilization of Artificial Intelligence in Real-time for Disease Identification and Alert Notification (GUARDIAN) system to the Complaint Coder (CoCo) of the Real-time Outbreak Detection System (RODS), the Symptom Coder (SyCo) of RODS, and to a standardized report generated via a proprietary electronic medical record (EMR) system. Sensitivity, specificity, and accuracy of each classifier's ability to identify ILI cases were calculated and compared to a manual review by a board-certified emergency physician. Chi-square and McNemar's tests were used to evaluate the statistical difference between the various surveillance systems.ResultsThe performance of GUARDIAN in detecting ILI in terms of sensitivity, specificity, and accuracy, as compared to a physician chart review, was 95.5%, 97.6%, and 97.1%, respectively. The EMR-generated reports were the next best system at identifying disease activity with a sensitivity, specificity, and accuracy of 36.7%, 99.3%, and 83.2%, respectively. RODS (CoCo and SyCo) had similar sensitivity (35.3%) but slightly different specificity (CoCo = 98.9%; SyCo = 99.3%). The GUARDIAN surveillance system with its multiple data sources performed significantly better compared to CoCo (chi2 = 130.6, p < 0.05), SyCo (chi2 = 125.2, p < 0.05), and EMR-based reports (chi2 = 121.3, p < 0.05). In addition, similar significant improvements in the accuracy (>12%) and sensitivity (>47%) were observed for GUARDIAN with only chief complaint data as compared to RODS (CoCo and SyCo) and EMR-based reports. CONCLUSION: In our study population, the GUARDIAN surveillance system, with its ability to utilize multiple data sources from patient encounters and real-time automaticity, demonstrated a more robust performance when compared to standard EMR-based reports and the RODS systems in detecting ILI. More large-scale studies are needed to validate the study findings, and to compare the performance of GUARDIAN in detecting other infectious diseases."," Cross-Sectional Studies; *Electronic Health Records; Humans; Influenza, Human/*epidemiology; *Population Surveillance; Retrospective Studies",-2,,
365,Preissner,2013,PloS one,Polymorphic cytochrome P450 enzymes (CYPs) and their role in personalized therapy,"The cytochrome P450 (CYP) enzymes are major players in drug metabolism. More than 2,000 mutations have been described, and certain single nucleotide polymorphisms (SNPs) have been shown to have a large impact on CYP activity. Therefore, CYPs play an important role in inter-individual drug response and their genetic variability should be factored into personalized medicine. To identify the most relevant polymorphisms in human CYPs, a text mining approach was used. We investigated their frequencies in different ethnic groups, the number of drugs that are metabolized by each CYP, the impact of CYP SNPs, as well as CYP expression patterns in different tissues. The most important polymorphic CYPs were found to be 1A2, 2D6, 2C9 and 2C19. Thirty-four common allele variants in Caucasians led to altered enzyme activity. To compare the relevant Caucasian SNPs with those of other ethnicities a search in 1,000 individual genomes was undertaken. We found 199 non-synonymous SNPs with frequencies over one percent in the 1,000 genomes, many of them not described so far. With knowledge of frequent mutations and their impact on CYP activities, it may be possible to predict patient response to certain drugs, as well as adverse side effects. With improved availability of genotyping, our data may provide a resource for an understanding of the effects of specific SNPs in CYPs, enabling the selection of a more personalized treatment regimen."," *Alleles; *Cytochrome P-450 Enzyme System/biosynthesis/genetics; European Continental Ancestry Group; *Gene Expression Regulation, Enzymologic; Humans; *Polymorphism, Single Nucleotide; *Precision Medicine",-2,,
366,Kum,2014,Journal of the American Medical Informatics Association : JAMIA,Privacy preserving interactive record linkage (PPIRL),"OBJECTIVE: Record linkage to integrate uncoordinated databases is critical in biomedical research using Big Data. Balancing privacy protection against the need for high quality record linkage requires a human-machine hybrid system to safely manage uncertainty in the ever changing streams of chaotic Big Data. METHODS: In the computer science literature, private record linkage is the most published area. It investigates how to apply a known linkage function safely when linking two tables. However, in practice, the linkage function is rarely known. Thus, there are many data linkage centers whose main role is to be the trusted third party to determine the linkage function manually and link data for research via a master population list for a designated region. Recently, a more flexible computerized third-party linkage platform, Secure Decoupled Linkage (SDLink), has been proposed based on: (1) decoupling data via encryption, (2) obfuscation via chaffing (adding fake data) and universe manipulation; and (3) minimum information disclosure via recoding. RESULTS: We synthesize this literature to formalize a new framework for privacy preserving interactive record linkage (PPIRL) with tractable privacy and utility properties and then analyze the literature using this framework. CONCLUSIONS: Human-based third-party linkage centers for privacy preserving record linkage are the accepted norm internationally. We find that a computer-based third-party platform that can precisely control the information disclosed at the micro level and allow frequent human interaction during the linkage process, is an effective human-machine hybrid system that significantly improves on the linkage center model both in terms of privacy and utility."," Biomedical Research; Computer Security; *Confidentiality; Humans; Medical Record Linkage/*methods; *Medical Records Systems, Computerized; User-Computer Interface; Electronic Health Records (EHR); decoupled data; entity resolution; medical record linkage; privacy; privacy preserving interactive record linkage (PPIRL)",-2,,
367,Shivade,2014,Journal of the American Medical Informatics Association : JAMIA,A review of approaches to identifying patient phenotype cohorts using electronic health records,"OBJECTIVE: To summarize literature describing approaches aimed at automatically identifying patients with a common phenotype. MATERIALS AND METHODS: We performed a review of studies describing systems or reporting techniques developed for identifying cohorts of patients with specific phenotypes. Every full text article published in (1) Journal of American Medical Informatics Association, (2) Journal of Biomedical Informatics, (3) Proceedings of the Annual American Medical Informatics Association Symposium, and (4) Proceedings of Clinical Research Informatics Conference within the past 3 years was assessed for inclusion in the review. Only articles using automated techniques were included. RESULTS: Ninety-seven articles met our inclusion criteria. Forty-six used natural language processing (NLP)-based techniques, 24 described rule-based systems, 41 used statistical analyses, data mining, or machine learning techniques, while 22 described hybrid systems. Nine articles described the architecture of large-scale systems developed for determining cohort eligibility of patients. DISCUSSION: We observe that there is a rise in the number of studies associated with cohort identification using electronic medical records. Statistical analyses or machine learning, followed by NLP techniques, are gaining popularity over the years in comparison with rule-based systems. CONCLUSIONS: There are a variety of approaches for classifying patients into a particular phenotype. Different techniques and data sources are used, and good performance is reported on datasets at respective institutions. However, no system makes comprehensive use of electronic medical records addressing all of their known weaknesses."," *Artificial Intelligence; Data Mining/*methods; Diagnosis; *Electronic Health Records; Humans; *Natural Language Processing; Phenotype; Statistics as Topic; Vocabulary, Controlled; Cohort Identification; Electronic Health Records; Phenotyping; Review",-2,,
368,Bellows,2014,Journal of the American Medical Informatics Association : JAMIA,Automated identification of patients with a diagnosis of binge eating disorder from narrative electronic health records,"Binge eating disorder (BED) does not have an International Classification of Diseases, 9th or 10th edition code, but is included under 'eating disorder not otherwise specified' (EDNOS). This historical cohort study identified patients with clinician-diagnosed BED from electronic health records (EHR) in the Department of Veterans Affairs between 2000 and 2011 using natural language processing (NLP) and compared their characteristics to patients identified by EDNOS diagnosis codes. NLP identified 1487 BED patients with classification accuracy of 91.8% and sensitivity of 96.2% compared to human review. After applying study inclusion criteria, 525 patients had NLP-identified BED only, 1354 had EDNOS only, and 68 had both BED and EDNOS. Patient characteristics were similar between the groups. This is the first study to use NLP as a method to identify BED patients from EHR data and will allow further epidemiological study of patients with BED in systems with adequate clinical notes.", *Algorithms; Binge-Eating Disorder/*diagnosis; *Electronic Health Records; Humans; Narration; *Natural Language Processing; binge eating disorder; eating disorder; electronic health record; natural language processing,-2,,
369,Salmasian,2013,Journal of the American Medical Informatics Association : JAMIA,Deriving comorbidities from medical records using natural language processing,"Extracting comorbidity information is crucial for phenotypic studies because of the confounding effect of comorbidities. We developed an automated method that accurately determines comorbidities from electronic medical records. Using a modified version of the Charlson comorbidity index (CCI), two physicians created a reference standard of comorbidities by manual review of 100 admission notes. We processed the notes using the MedLEE natural language processing system, and wrote queries to extract comorbidities automatically from its structured output. Interrater agreement for the reference set was very high (97.7%). Our method yielded an F1 score of 0.761 and the summed CCI score was not different from the reference standard (p=0.329, power 80.4%). In comparison, obtaining comorbidities from claims data yielded an F1 score of 0.741, due to lower sensitivity (66.1%). Because CCI has previously been validated as a predictor of mortality and readmission, our method could allow automated prediction of these outcomes."," Adolescent; Adult; Aged; Aged, 80 and over; Child; Child, Preschool; *Comorbidity; Diabetes Complications; *Electronic Health Records; Female; Humans; Infant; Male; Middle Aged; *Natural Language Processing; Reference Standards; Sensitivity and Specificity; Young Adult; Comorbidity; Confounding Factors; Natural Language Processing",-2,,
370,Boeker,2013,BMC medical research methodology,Google Scholar as replacement for systematic literature searches: good relative recall and precision are not enough,"BACKGROUND: Recent research indicates a high recall in Google Scholar searches for systematic reviews. These reports raised high expectations of Google Scholar as a unified and easy to use search interface. However, studies on the coverage of Google Scholar rarely used the search interface in a realistic approach but instead merely checked for the existence of gold standard references. In addition, the severe limitations of the Google Search interface must be taken into consideration when comparing with professional literature retrieval tools.The objectives of this work are to measure the relative recall and precision of searches with Google Scholar under conditions which are derived from structured search procedures conventional in scientific literature retrieval; and to provide an overview of current advantages and disadvantages of the Google Scholar search interface in scientific literature retrieval. METHODS: General and MEDLINE-specific search strategies were retrieved from 14 Cochrane systematic reviews. Cochrane systematic review search strategies were translated to Google Scholar search expression as good as possible under consideration of the original search semantics. The references of the included studies from the Cochrane reviews were checked for their inclusion in the result sets of the Google Scholar searches. Relative recall and precision were calculated. RESULTS: We investigated Cochrane reviews with a number of included references between 11 and 70 with a total of 396 references. The Google Scholar searches resulted in sets between 4,320 and 67,800 and a total of 291,190 hits. The relative recall of the Google Scholar searches had a minimum of 76.2% and a maximum of 100% (7 searches). The precision of the Google Scholar searches had a minimum of 0.05% and a maximum of 0.92%. The overall relative recall for all searches was 92.9%, the overall precision was 0.13%. CONCLUSION: The reported relative recall must be interpreted with care. It is a quality indicator of Google Scholar confined to an experimental setting which is unavailable in systematic retrieval due to the severe limitations of the Google Scholar search interface. Currently, Google Scholar does not provide necessary elements for systematic scientific literature retrieval such as tools for incremental query optimization, export of a large number of references, a visual search builder or a history function. Google Scholar is not ready as a professional searching tool for tasks where structured retrieval methodology is necessary.", Abstracting and Indexing as Topic; Algorithms; Data Mining/methods; Humans; Internet; Medline; Review Literature as Topic; Search Engine/*standards,-2,,
371,Shemilt,2013,PloS one,Economic instruments for population diet and physical activity behaviour change: a systematic scoping review,"BACKGROUND: Unhealthy diet and low levels of physical activity are common behavioural factors in the aetiology of many non-communicable diseases. Recent years have witnessed an upsurge of policy and research interest in the use of taxes and other economic instruments to improve population health. OBJECTIVE: To assemble, configure and analyse empirical research studies available to inform the public health case for using economic instruments to promote dietary and physical activity behaviour change. METHODS: We conducted a systematic scoping review of evidence for the effects of specific interventions to change, or general exposure to variations in, prices or income on dietary and physical activity behaviours and corollary outcomes. Systematic electronic searches and parallel snowball searches retrieved >1 million study records. Text mining technologies were used to prioritise title-abstract records for screening. Eligible studies were selected, classified and analysed in terms of key characteristics and principal findings, using a narrative, configuring synthesis focused on implications for policy and further research. RESULTS: We identified 880 eligible studies, including 192 intervention studies and 768 studies that incorporated evidence for prices or income as correlates or determinants of target outcomes. Current evidence for the effects of economic instruments and exposures on diet and physical activity is limited in quality and equivocal in terms of its policy implications. Direct evidence for the effects of economic instruments is heavily skewed towards impacts on diet, with a relative lack of evidence for impacts on physical activity. CONCLUSIONS: The evidence-based case for using economic instruments to promote dietary and physical activity behaviour change may be less compelling than some proponents have claimed. Future research should include measurement of people's actual behavioural responses using study designs capable of generating reliable causal inferences regarding intervention effects. Policy implementation needs to be carefully aligned with evaluation planning and design.", *Behavior; Commerce; Diet/*economics; Humans; Income; *Motor Activity,-2,,
372,Lefebvre,2013,Systematic reviews,"Methodological developments in searching for studies for systematic reviews: past, present and future?","The Cochrane Collaboration was established in 1993, following the opening of the UK Cochrane Centre in 1992, at a time when searching for studies for inclusion in systematic reviews was not well-developed. Review authors largely conducted their own searches or depended on medical librarians, who often possessed limited awareness and experience of systematic reviews. Guidance on the conduct and reporting of searches was limited. When work began to identify reports of randomized controlled trials (RCTs) for inclusion in Cochrane Reviews in 1992, there were only approximately 20,000 reports indexed as RCTs in MEDLINE and none indexed as RCTs in Embase. No search filters had been developed with the aim of identifying all RCTs in MEDLINE or other major databases. This presented The Cochrane Collaboration with a considerable challenge in identifying relevant studies.Over time, the number of studies indexed as RCTs in the major databases has grown considerably and the Cochrane Central Register of Controlled Trials (CENTRAL) has become the best single source of published controlled trials, with approximately 700,000 records, including records identified by the Collaboration from Embase and MEDLINE. Search filters for various study types, including systematic reviews and the Cochrane Highly Sensitive Search Strategies for RCTs, have been developed. There have been considerable advances in the evidence base for methodological aspects of information retrieval. The Cochrane Handbook for Systematic Reviews of Interventions now provides detailed guidance on the conduct and reporting of searches. Initiatives across The Cochrane Collaboration to improve the quality inter alia of information retrieval include: the recently introduced Methodological Expectations for Cochrane Intervention Reviews (MECIR) programme, which stipulates 'mandatory' and 'highly desirable' standards for various aspects of review conduct and reporting including searching, the development of Standard Training Materials for Cochrane Reviews and work on peer review of electronic search strategies. Almost all Cochrane Review Groups and some Cochrane Centres and Fields now have a Trials Search Co-ordinator responsible for study identification and medical librarians and other information specialists are increasingly experienced in searching for studies for systematic reviews.Prospective registration of clinical trials is increasing and searching trials registers is now mandatory for Cochrane Reviews, where relevant. Portals such as the WHO International Clinical Trials Registry Platform (ICTRP) are likely to become increasingly attractive, given concerns about the number of trials which may not be registered and/or published. The importance of access to information from regulatory and reimbursement agencies is likely to increase. Cross-database searching, gateways or portals and improved access to full-text databases will impact on how searches are conducted and reported, as will services such as Google Scholar, Scopus and Web of Science. Technologies such as textual analysis, semantic analysis, text mining and data linkage will have a major impact on the search process but efficient and effective updating of reviews may remain a challenge.In twenty years' time, we envisage that the impact of universal social networking, as well as national and international legislation, will mean that all trials involving humans will be registered at inception and detailed trial results will be routinely available to all. Challenges will remain, however, to ensure the discoverability of relevant information in diverse and often complex sources and the availability of metadata to provide the most efficient access to information. We envisage an ongoing role for information professionals as experts in identifying new resources, researching efficient ways to link or mine them for relevant data and managing their content for the efficient production of systematic reviews."," Abstracting and Indexing as Topic; *Databases, Bibliographic; Evidence-Based Medicine; Humans; Information Storage and Retrieval/*methods/*standards/trends; Randomized Controlled Trials as Topic; Registries; *Review Literature as Topic",1,,
373,Nahar,2013,Studies in health technology and informatics,Issues of data governance associated with data mining in medical research: experiences from an empirical study,"This chapter is a review of data mining techniques used in medical research. It will cover the existing applications of these techniques in the identification of diseases, and also present the authors' research experiences in medical disease diagnosis and analysis. A computational diagnosis approach can have a significant impact on accurate diagnosis and result in time and cost effective solutions. The chapter will begin with an overview of computational intelligence concepts, followed by details on different classification algorithms. Use of association learning, a well recognised data mining procedure, will also be discussed. Many of the datasets considered in existing medical data mining research are imbalanced, and the chapter focuses on this issue as well. Lastly, the chapter outlines the need of data governance in this research domain."," Biomedical Research/*organization & administration; Data Mining/*methods; Database Management Systems/*organization & administration; Electronic Health Records/*organization & administration; Health Information Management/*organization & administration; Health Information Systems/*organization & administration; Medical Informatics/organization & administration; *Models, Organizational",-2,,
374,Stansfield,2013,Research synthesis methods,Clustering' documents automatically to support scoping reviews of research: a case study,"BACKGROUND: Scoping reviews of research help determine the feasibility and the resource requirements of conducting a systematic review, and the potential to generate a description of the literature quickly is attractive. AIMS: To test the utility and applicability of an automated clustering tool to describe and group research studies to improve the efficiency of scoping reviews. METHODS: A retrospective study of two completed scoping reviews was conducted. This compared the groups and descriptive categories obtained by automatically clustering titles and abstracts with those that had originally been derived using traditional researcher-driven techniques. RESULTS: The clustering tool rapidly categorised research into themes, which were useful in some instances, but not in others. This provided a dynamic means to view each dataset. Interpretation was challenging where there were potentially multiple meanings of terms. Where relevant clusters were unambiguous, there was a high precision of relevant studies, although recall varied widely. CONCLUSIONS: Policy-relevant scoping reviews are often undertaken rapidly, and this could potentially be enhanced by automation depending on the nature of the dataset and information sought. However, it is not a replacement for researcher-developed classification. The possibilities of further applications and potential for use in other types of review are discussed."," Cluster Analysis; Data Mining/*methods; Documentation/*classification; Machine Learning; *Natural Language Processing; Periodicals as Topic/classification; *Research Report; *Review Literature as Topic; *Vocabulary, Controlled; automatic clustering; automation; information storage and retrieval; methods, mapping; scoping reviews; text mining",2,,
375,Schild,2013,Research synthesis methods,Less is less: a systematic review of graph use in meta-analyses,"Graphs are an essential part of scientific communication. Complex datasets, of which meta-analyses are textbook examples, benefit the most from visualization. Although a number of graph options for meta-analyses exist, the extent to which these are used was hitherto unclear. A systematic review on graph use in meta-analyses in three disciplines (medicine, psychology, and business) and nine journals was conducted. Interdisciplinary differences, which are mirrored in the respective journals, were revealed, that is, graph use correlates with external factors rather than methodological considerations. There was only limited variation in graph types (with forest plots as the most important representatives), and diagnostic plots were very rare. Although an increase in graph use over time could be observed, it is unlikely that this phenomenon is specific to meta-analyses. There is a gaping discrepancy between available graphic methods and their application in meta-analyses. This may be rooted in a number of factors, namely, (i) insufficient dissemination of new developments, (ii) unsatisfactory implementation in software packages, and (iii) minor attention on graphics in meta-analysis reporting guidelines. Using visualization methods to their full capacity is a further step in using meta-analysis to its full potential."," *Algorithms; *Computer Graphics; *Data Interpretation, Statistical; Data Mining/*methods; *Meta-Analysis as Topic; Natural Language Processing; *User-Computer Interface; Vocabulary, Controlled; forest plot; graph; meta-analysis; research synthesis; systematic review",-2,,
376,DeLisle,2013,PloS one,Using the electronic medical record to identify community-acquired pneumonia: toward a replicable automated strategy,"BACKGROUND: Timely information about disease severity can be central to the detection and management of outbreaks of acute respiratory infections (ARI), including influenza. We asked if two resources: 1) free text, and 2) structured data from an electronic medical record (EMR) could complement each other to identify patients with pneumonia, an ARI severity landmark. METHODS: A manual EMR review of 2747 outpatient ARI visits with associated chest imaging identified x-ray reports that could support the diagnosis of pneumonia (kappa score = 0.88 (95% CI 0.82ratio0.93)), along with attendant cases with Possible Pneumonia (adds either cough, sputum, fever/chills/night sweats, dyspnea or pleuritic chest pain) or with Pneumonia-in-Plan (adds pneumonia stated as a likely diagnosis by the provider). The x-ray reports served as a reference to develop a text classifier using machine-learning software that did not require custom coding. To identify pneumonia cases, the classifier was combined with EMR-based structured data and with text analyses aimed at ARI symptoms in clinical notes. RESULTS: 370 reference cases with Possible Pneumonia and 250 with Pneumonia-in-Plan were identified. The x-ray report text classifier increased the positive predictive value of otherwise identical EMR-based case-detection algorithms by 20-70%, while retaining sensitivities of 58-75%. These performance gains were independent of the case definitions and of whether patients were admitted to the hospital or sent home. Text analyses seeking ARI symptoms in clinical notes did not add further value. CONCLUSION: Specialized software development is not required for automated text analyses to help identify pneumonia patients. These results begin to map an efficient, replicable strategy through which EMR data can be used to stratify ARI severity."," Adolescent; Adult; Aged; Aged, 80 and over; Algorithms; *Community-Acquired Infections/diagnosis/epidemiology; *Electronic Health Records; Female; Humans; Male; Middle Aged; Outpatients; *Pneumonia/diagnosis/epidemiology; Reproducibility of Results; Software; Young Adult",-2,,
377,Hanauer,2013,PloS one,Describing the relationship between cat bites and human depression using data from an electronic health record,"Data mining approaches have been increasingly applied to the electronic health record and have led to the discovery of numerous clinical associations. Recent data mining studies have suggested a potential association between cat bites and human depression. To explore this possible association in more detail we first used administrative diagnosis codes to identify patients with either depression or bites, drawn from a population of 1.3 million patients. We then conducted a manual chart review in the electronic health record of all patients with a code for a bite to accurately determine which were from cats or dogs. Overall there were 750 patients with cat bites, 1,108 with dog bites, and approximately 117,000 patients with depression. Depression was found in 41.3% of patients with cat bites and 28.7% of those with dog bites. Furthermore, 85.5% of those with both cat bites and depression were women, compared to 64.5% of those with dog bites and depression. The probability of a woman being diagnosed with depression at some point in her life if she presented to our health system with a cat bite was 47.0%, compared to 24.2% of men presenting with a similar bite. The high proportion of depression in patients who had cat bites, especially among women, suggests that screening for depression could be appropriate in patients who present to a clinical provider with a cat bite. Additionally, while no causative link is known to explain this association, there is growing evidence to suggest that the relationship between cats and human mental illness, such as depression, warrants further investigation."," Adult; Aged; Aged, 80 and over; Animals; Bites and Stings/*complications; Cats; *Data Mining; Depression/*complications; Dogs; *Electronic Health Records; Female; Humans; Male; Medical Informatics/*methods; Mice; Middle Aged; Rats; Young Adult",-2,,
378,Taira,2013,Studies in health technology and informatics,Hierarchical semantic structures for medical NLP,"We present a framework for building a medical natural language processing (NLP) system capable of deep understanding of clinical text reports. The framework helps developers understand how various NLP-related efforts and knowledge sources can be integrated. The aspects considered include: 1) computational issues dealing with defining layers of intermediate semantic structures to reduce the dimensionality of the NLP problem; 2) algorithmic issues in which we survey the NLP literature and discuss state-of-the-art procedures used to map between various levels of the hierarchy; and 3) implementation issues to software developers with available resources. The objective of this poster is to educate readers to the various levels of semantic representation (e.g., word level concepts, ontological concepts, logical relations, logical frames, discourse structures, etc.). The poster presents an architecture for which diverse efforts and resources in medical NLP can be integrated in a principled way."," *Algorithms; Documentation/methods; Electronic Health Records/*classification/*organization & administration; Information Storage and Retrieval/*methods; *Natural Language Processing; *Semantics; *Vocabulary, Controlled",1,,
379,Tusch,2013,Studies in health technology and informatics,Finding temporal gene expression patterns for translational research,"Translational research of time-series of gene-expression microarray datasets makes use on gene expression profiles that have been obtained at different points in time. Our web-based multi-user program helps a researcher find temporal patterns like peaks in large pre-selected microarray data sets that include data from different but related studies in publicly available databases. If all studies use the same platform, data can be combined for a meta-analysis type approach. For combination of data from different platforms we allow only Affymetrix GeneChips, for which a method for pooling of information exists. To search for time patterns, the data are transformed into an abstract layer that is independent from the particular selection of time point in the individual studies."," Algorithms; Artificial Intelligence; Data Mining/*methods; *Database Management Systems; *Databases, Genetic; Gene Expression Profiling/*methods; Gene Expression Regulation/genetics; Medical Record Linkage/methods; Meta-Analysis as Topic; Natural Language Processing; Oligonucleotide Array Sequence Analysis/*methods; *Software; Translational Medical Research/*methods",-2,,
380,Plazzotta,2013,Studies in health technology and informatics,Natural language processing and inference rules as strategies for updating problem list in an electronic health record,"UNLABELLED: Physicians do not always keep the problem list accurate, complete and updated. OBJECTIVE: To analyze natural language processing (NLP) techniques and inference rules as strategies to maintain completeness and accuracy of the problem list in EHRs. METHODS: Non systematic literature review in PubMed, in the last 10 years. Strategies to maintain the EHRs problem list were analyzed in two ways: inputting and removing problems from the problem list. RESULTS: NLP and inference rules have acceptable performance for inputting problems into the problem list. No studies using these techniques for removing problems were published Conclusion: Both tools, NLP and inference rules have had acceptable results as tools for maintain the completeness and accuracy of the problem list."," *Algorithms; Artificial Intelligence; Electronic Health Records/*organization & administration; Information Storage and Retrieval/*methods; *Medical Records, Problem-Oriented; *Natural Language Processing; Pattern Recognition, Automated/methods/standards; Quality Assurance, Health Care/*methods; User-Computer Interface; *Vocabulary, Controlled",-2,,
381,Pina,2013,Studies in health technology and informatics,Synonym-based word frequency analysis to support the development and presentation of a public health quality improvement taxonomy in an online exchange,"Word frequency analysis has not been fully explored as an input to public health taxonomy development. We used document analysis, expert review, and user-centered design to develop a taxonomy of public health quality improvement concepts for an online exchange of quality improvement work (www.phqix.org). Online entries were made searchable using a faceted search approach. To present the most relevant facets to users, we analyzed 334 published public health quality improvement documents using word frequency analysis to identify the most prevalent clusters of word meanings. We reviewed the highest-weighted concepts and identified their relationships to quality improvement details in our taxonomy. The meanings were mapped to items in our taxonomy, and presented in order of their weighted percentages in the data. Using this combination of methods, we developed and sorted concepts in the faceted search presentation so that relevant search criteria were accessible to users of the online exchange. Word frequency analysis may be a useful method to incorporate in other taxonomy development and presentationwhen relevant data is available."," Classification/*methods; *Dictionaries as Topic; Documentation/*standards; *Health Information Exchange; Meaningful Use/standards; Medical Record Linkage/standards; Natural Language Processing; Online Systems; Public Health Informatics/standards; Quality Improvement/*classification; *Terminology as Topic; United States; *Vocabulary, Controlled",1,,
382,Love,2013,Studies in health technology and informatics,Unifying acute stroke treatment guidelines for a Bayesian belief network,"With the large number of clinical practice guidelines available, there is an increasing need for a comprehensive unified model for acute ischemic stroke treatment to assist in clinical decision making. We present a unified treatment model derived through review of existing clinical practice guidelines, meta-analyses, and clinical trials. Using logic from the treatment model, a Bayesian belief network was defined and fitted to data from our institution's observational quality improvement database for acute stroke patients. The resulting network validates known relationships between variables, treatment decisions and outcomes, and enables the exploration of new correlative relationships not defined in current guidelines."," Algorithms; Artificial Intelligence; *Bayes Theorem; Decision Support Systems, Clinical/*standards; Humans; Natural Language Processing; Neurology/*standards; Outcome Assessment (Health Care)/*standards; Pattern Recognition, Automated/*methods; *Practice Guidelines as Topic; Stroke/*therapy",-2,,
383,Scarton,2013,Studies in health technology and informatics,"Completeness, accuracy, and presentation of information on interactions between prescription drugs and alternative medicines: an internet review","BACKGROUND: As the use of the Internet continues to increase across all age groups and education levels, with usage in the US around 78%, consumers are increasingly turning to the Internet for health related information. OBJECTIVE: To assess the completeness, accuracy, and consumer friendliness of information on the Internet pertaining to drug-Complementary and Alternative Medicine (CAM) interactions with cardiac drugs. METHODS: A review of online information was performed across three search engines and ten drug-CAM pairs. RESULTS: Overall, the quality of the drug-CAM interaction information available online to consumers is fairly poor. Only one site contained an interaction checker that provided interaction information for all ten pairs, but with an accuracy rate of 50%. Reading levels ranged from 10.5-23.5, with a mean of 16.7. A value greater than 22 indicates a graduate level reading skill. CONCLUSION: Web site developers should be cautious in presenting drug-CAM interaction information unless it is comprehensive and regularly maintained. Consumers should also know how to evaluate sites before trusting the content where the consequences are potentially severe.", Consumer Health Information/*classification/statistics & numerical data; Data Mining/methods; Drug-Related Side Effects and Adverse Reactions/*classification; *Herb-Drug Interactions; Humans; Internet/*statistics & numerical data; Meaningful Use/*statistics & numerical data; Phytotherapy/*classification; Prescription Drugs/*classification; Truth Disclosure,-2,,
384,Kilby,2013,Studies in health technology and informatics,Designing a mobile augmented reality tool for the locative visualisation of biomedical knowledge,"Mobile augmented reality (MAR) may offer new and engaging ways to support consumer participation in health. We report on design-based research into a MAR application for smartphones and tablets, intended to improve public engagement with biomedical research in a specific urban precinct. Following a review of technical capabilities and organizational and locative design considerations, we worked with staff of four research institutes to elicit their ideas about information and interaction functionalities of a shared MAR app. The results were promising, supporting the development of a prototype and initial field testing with these staff. Evidence from this project may point the way toward user-centred design of MAR services that will enable more widespread adoption of the technology in other healthcare and biomedical research contexts."," *Computers, Handheld; *Consumer Health Information; Data Mining/*methods; *Software; Software Design; Telemedicine/*methods; *User-Computer Interface",-2,,
385,Ferret,2013,Studies in health technology and informatics,Evaluation of a computerized tool allowing retrospective detection of potential vitamin K antagonist overdoses in complex contexts,"Management of vitamin K antagonists (VKA) is difficult, and overdoses can have dramatic hemorrhagic consequences. These works form part of a European computerized medical data processing project, which aims to develop IT tools for describing adverse drug events (ADEs). Materials and methods A tool enabling retrospective research of potential ADE cases was developed, using complex ADE detection rules taking into account chronological parameters: the ADE scorecards. The rules were applied on 14,748 medical records from a community hospital. We evaluated the predictive positive value of the rules related to INR elevation by an expert review of the detected cases. The severity of the clinical consequences was evaluated. Results 49 cases were detected, among which 11 cases were ADEs. The predictive positive value of the rules is 22.44%, mostly related to antibiotics and amiodarone introduction. The four cases of clinical damage related to a drug were properly designated by the rules. Discussion - Conclusion Our study shows the great potential of developing complex rules for the identification of adverse drug events in large medical databases."," Adverse Drug Reaction Reporting Systems/*statistics & numerical data; Algorithms; Anticoagulants/*adverse effects; Arrhythmias, Cardiac/classification/*etiology; Artificial Intelligence; Data Mining/methods; Decision Support Techniques; Electronic Health Records/*statistics & numerical data; France; Health Records, Personal; Hemorrhage/classification/*etiology; Humans; Retrospective Studies; *Software; Software Validation; Vitamin K/*antagonists & inhibitors",-2,,
386,Houser,2013,Studies in health technology and informatics,Telephone follow-up in primary care: can interactive voice response calls work?,"UNLABELLED: Follow-up calls after ambulatory visits are not routinely done, yet they can potentially detect and mitigate unresolved problems. Automated calls via an Interactive Voice Response System (IVRS) are an innovative way to conduct follow-up, but patients' attitudes toward follow-up calls are unknown. This study assessed 1) patient perceptions about follow-up calls after visits; 2) differences in perceptions between human and IVRS calls; and 3) association between follow-up calls and patient satisfaction with care. Post-visit follow-up calls in two ambulatory care setting were done in two phases. Phase 1 used a human caller and phase 2 used IVRS. Patient satisfaction questionnaires were completed after each phase. Results showed that 88% of patients favor the idea of the calls and those receiving them found them helpful. There were no differences in attitudes between patients receiving calls from clinic staff or from an IVRS. Patients receiving calls had higher patient satisfaction scores than those not called. CONCLUSION: Patients value follow-up calls and they are associated with patient satisfaction with care. IVRS is an innovative way to conduct post-visit follow-up.", Alabama; Ambulatory Care/*statistics & numerical data; Continuity of Patient Care/*statistics & numerical data; Health Care Surveys; Humans; Patient Participation/*statistics & numerical data; Patient Satisfaction/*statistics & numerical data; Primary Health Care/methods/*statistics & numerical data; Remote Consultation/*methods; Speech Recognition Software; *Telephone; User-Computer Interface,-2,,
387,Warner,2013,Studies in health technology and informatics,Automated synthesis and visualization of a chemotherapy treatment regimen network,"Cytotoxic treatments for cancer remain highly toxic, expensive, and variably efficacious. Many chemotherapy regimens are never directly compared in randomized clinical trials (RCTs); as a result, the vast majority of guideline recommendations are ultimately derived from human expert opinion. We introduce an automated network meta-analytic approach to this clinical problem, with nodes representing regimens and edges direct comparison via RCT(s). A chemotherapy regimen network is visualized for the primary treatment of chronic myelogenous leukemia (CML). Node and edge color, size, and opacity are all utilized to provide additional information about the quality and strength of the depicted evidence. Historical versions of the network are also created. With this approach, we were able to compactly compare the results of 17 CML regimens involving RCTs of 9700 patients, representing the accumulation of 45 years of evidence. Our results closely parallel the recommendations issued by a professional guidelines organization, the National Comprehensive Cancer Network (NCCN). This approach offers a novel method for interpreting complex clinical data, with potential implications for future objective guideline development."," *Algorithms; *Decision Support Systems, Clinical; Diagnosis, Computer-Assisted/*methods; Drug Therapy, Computer-Assisted/*methods; Humans; Leukemia, Myeloid/*drug therapy/*pathology; Pattern Recognition, Automated/*methods; Software; *User-Computer Interface",1,,
388,Muehlan,2013,Studies in health technology and informatics,Psycho-social aspects of personal health monitoring: a descriptive literature review,"We aimed at providing a short review on already published studies addressing psycho-social issues of personal health monitoring (PHM). Both core questions addressed within this review are: What is the impact of PHM on intended psycho-social and health-related outcomes? And which psycho-social issues affected by or related to PHM have already been investigated? This descriptive review based on a literature search using various databases (Psycinfo, Psyndex, Pubmed, SSCI). Resulting 428 abstracts were coded regarding their psycho-social content. Inspection of results was carried out along the relevance of the papers regarding psycho-social issues. Research in PHM focuses on telemonitoring and smart home applications: Tele-monitoring studies are directed to outcome-related questions, smart home studies to feasibility issues. Despite of technological matters, comparability of both systems in psycho-social issues is lacking. Tele-monitoring has been proven for impact on patient groups with chronic diseases, yet smart home still lacks evidence in health-related and psycho-social matters. Smart home applications have been investigated with respect to attitudes, perceptions and concerns of end-users, telemonitoring regarding acceptance and adherence."," Artificial Intelligence; *Biomedical Technology; Confidentiality/*psychology; *Diagnostic Self Evaluation; Humans; *Medical Informatics; Monitoring, Ambulatory/*psychology; Patient Acceptance of Health Care/*psychology; Psychology; *Telemedicine",-2,,
389,Nussbeck,2013,Studies in health technology and informatics,Technical literature review,"This review gives a comprehensive overview on the technical perspective of personal health monitoring. It is designed to build a mutual basis for the project partners of the PHM-Ethics project. A literature search was conducted to screen pertinent literature databases for relevant publications. All review papers that were retrieved were analyzed. The increasing number of publications that are published per year shows that the field of personal health monitoring is of growing interest in the research community. Most publications deal with telemonitoring, thus forming the core technology of personal health monitoring. Measured parameters, fields of application, participants and stakeholders are described. Moreover an outlook on information and communication technology that foster the integration possibilities of personal health monitoring into decision making and remote monitoring of individual people's health is provided. The removal of the technological barriers opens new perspectives in health and health care delivery using home monitoring applications."," *Artificial Intelligence; *Biomedical Technology; *Confidentiality; *Diagnostic Self Evaluation; *Medical Informatics; *Monitoring, Ambulatory; *Telemedicine",-2,,
390,Schmidt,2013,Studies in health technology and informatics,The PHM-Ethics methodology: interdisciplinary technology assessment of personal health monitoring,"The contribution briefly introduces the PHM Ethics project and the PHM methodology. Within the PHM-Ethics project, a set of tools and modules had been developed that may assist in the evaluation and assessment of new technologies for personal health monitoring, referred to as ""PHM methodology"" or ""PHM toolbox"". An overview on this interdisciplinary methodology and its comprising modules is provided, areas of application and intended target groups are indicated."," Artificial Intelligence/ethics; Biomedical Technology/*ethics; Confidentiality/ethics; Diagnostic Self Evaluation; Ethical Analysis/*methods; Interdisciplinary Studies; Medical Informatics/*ethics; Monitoring, Ambulatory/*ethics; *Software; Software Design; Technology Assessment, Biomedical/*methods; Telemedicine/*ethics",-2,,
391,Rienhoff,2013,Studies in health technology and informatics,From intensive care monitoring to personal health monitoring to ambient intelligence,"The historical roots of IT-based monitoring in health care are described. Since the 1970ies monitoring has been spreading to more and more domains of health care and public health. Today one can observe monitoring of persons in many environments and regarding widely different questions. While these monitoring applications have been introduced ethical questions have been raised to balance the possible positive and negative outcomes of the approaches. Today IT-technology is entering many parts of our life - IT eventually became what had been coined already in the last century by IBM as ""electronic dust"" which one can find in every part of our environment. As most of these ""dust-particles"" are able to observe something one can also understand this development as a development into ubiquitous monitoring of nearly everything at any time. The foreseen ambient intelligence worlds are also spaces of ambient monitoring. This article describes this historical development. It emphasizes why ethical and data protection questions are an absolute must in most IT activities today."," Artificial Intelligence/*ethics; Biomedical Technology/*ethics; Confidentiality/*ethics; Critical Care/*ethics; Diagnostic Self Evaluation; Medical Informatics/*ethics; Monitoring, Ambulatory/*ethics; Telemedicine/*ethics",-2,,
392,Bernardo,2013,Journal of medical Internet research,Scoping review on search queries and social media for disease surveillance: a chronology of innovation,"BACKGROUND: The threat of a global pandemic posed by outbreaks of influenza H5N1 (1997) and Severe Acute Respiratory Syndrome (SARS, 2002), both diseases of zoonotic origin, provoked interest in improving early warning systems and reinforced the need for combining data from different sources. It led to the use of search query data from search engines such as Google and Yahoo! as an indicator of when and where influenza was occurring. This methodology has subsequently been extended to other diseases and has led to experimentation with new types of social media for disease surveillance. OBJECTIVE: The objective of this scoping review was to formally assess the current state of knowledge regarding the use of search queries and social media for disease surveillance in order to inform future work on early detection and more effective mitigation of the effects of foodborne illness. METHODS: Structured scoping review methods were used to identify, characterize, and evaluate all published primary research, expert review, and commentary articles regarding the use of social media in surveillance of infectious diseases from 2002-2011. RESULTS: Thirty-two primary research articles and 19 reviews and case studies were identified as relevant. Most relevant citations were peer-reviewed journal articles (29/32, 91%) published in 2010-11 (28/32, 88%) and reported use of a Google program for surveillance of influenza. Only four primary research articles investigated social media in the context of foodborne disease or gastroenteritis. Most authors (21/32 articles, 66%) reported that social media-based surveillance had comparable performance when compared to an existing surveillance program. The most commonly reported strengths of social media surveillance programs included their effectiveness (21/32, 66%) and rapid detection of disease (21/32, 66%). The most commonly reported weaknesses were the potential for false positive (16/32, 50%) and false negative (11/32, 34%) results. Most authors (24/32, 75%) recommended that social media programs should primarily be used to support existing surveillance programs. CONCLUSIONS: The use of search queries and social media for disease surveillance are relatively recent phenomena (first reported in 2006). Both the tools themselves and the methodologies for exploiting them are evolving over time. While their accuracy, speed, and cost compare favorably with existing surveillance systems, the primary challenge is to refine the data signal by reducing surrounding noise. Further developments in digital disease surveillance have the potential to improve sensitivity and specificity, passively through advances in machine learning and actively through engagement of users. Adoption, even as supporting systems for existing surveillance, will entail a high level of familiarity with the tools and collaboration across jurisdictions.", *Disease Outbreaks; Humans; *Organizational Innovation; *Population Surveillance; *Social Media; disease; review; social media; surveillance,-2,,
393,Mutale,2013,BMC health services research,Improving health information systems for decision making across five sub-Saharan African countries: Implementation strategies from the African Health Initiative,"BACKGROUND: Weak health information systems (HIS) are a critical challenge to reaching the health-related Millennium Development Goals because health systems performance cannot be adequately assessed or monitored where HIS data are incomplete, inaccurate, or untimely. The Population Health Implementation and Training (PHIT) Partnerships were established in five sub-Saharan African countries (Ghana, Mozambique, Rwanda, Tanzania, and Zambia) to catalyze advances in strengthening district health systems. Interventions were tailored to the setting in which activities were planned. COMPARISONS ACROSS STRATEGIES: All five PHIT Partnerships share a common feature in their goal of enhancing HIS and linking data with improved decision-making, specific strategies varied. Mozambique, Ghana, and Tanzania all focus on improving the quality and use of the existing Ministry of Health HIS, while the Zambia and Rwanda partnerships have introduced new information and communication technology systems or tools. All partnerships have adopted a flexible, iterative approach in designing and refining the development of new tools and approaches for HIS enhancement (such as routine data quality audits and automated troubleshooting), as well as improving decision making through timely feedback on health system performance (such as through summary data dashboards or routine data review meetings). The most striking differences between partnership approaches can be found in the level of emphasis of data collection (patient versus health facility), and consequently the level of decision making enhancement (community, facility, district, or provincial leadership). DISCUSSION: Design differences across PHIT Partnerships reflect differing theories of change, particularly regarding what information is needed, who will use the information to affect change, and how this change is expected to manifest. The iterative process of data use to monitor and assess the health system has been heavily communication dependent, with challenges due to poor feedback loops. Implementation to date has highlighted the importance of engaging frontline staff and managers in improving data collection and its use for informing system improvement. Through rigorous process and impact evaluation, the experience of the PHIT teams hope to contribute to the evidence base in the areas of HIS strengthening, linking HIS with decision making, and its impact on measures of health system outputs and impact."," Africa South of the Sahara; Decision Support Systems, Clinical/*organization & administration; Health Information Systems/instrumentation/*standards; Quality Improvement/*organization & administration",-2,,
394,Chen,2013,Journal of the American Medical Informatics Association : JAMIA,Applying active learning to high-throughput phenotyping algorithms for electronic health records data,"OBJECTIVES: Generalizable, high-throughput phenotyping methods based on supervised machine learning (ML) algorithms could significantly accelerate the use of electronic health records data for clinical and translational research. However, they often require large numbers of annotated samples, which are costly and time-consuming to review. We investigated the use of active learning (AL) in ML-based phenotyping algorithms. METHODS: We integrated an uncertainty sampling AL approach with support vector machines-based phenotyping algorithms and evaluated its performance using three annotated disease cohorts including rheumatoid arthritis (RA), colorectal cancer (CRC), and venous thromboembolism (VTE). We investigated performance using two types of feature sets: unrefined features, which contained at least all clinical concepts extracted from notes and billing codes; and a smaller set of refined features selected by domain experts. The performance of the AL was compared with a passive learning (PL) approach based on random sampling. RESULTS: Our evaluation showed that AL outperformed PL on three phenotyping tasks. When unrefined features were used in the RA and CRC tasks, AL reduced the number of annotated samples required to achieve an area under the curve (AUC) score of 0.95 by 68% and 23%, respectively. AL also achieved a reduction of 68% for VTE with an optimal AUC of 0.70 using refined features. As expected, refined features improved the performance of phenotyping classifiers and required fewer annotated samples. CONCLUSIONS: This study demonstrated that AL can be useful in ML-based phenotyping methods. Moreover, AL and feature engineering based on domain knowledge could be combined to develop efficient and generalizable phenotyping methods.", *Algorithms; *Artificial Intelligence; *Electronic Health Records; Genetic Association Studies; Humans; *Phenotype; Support Vector Machine; Active Learning; Electronic Health Records; Machine Learning; Natural Language Processing; Phenotyping Algorithm,-2,,
395,Feess,2013,PloS one,Comparison of sensor selection mechanisms for an ERP-based brain-computer interface,"A major barrier for a broad applicability of brain-computer interfaces (BCIs) based on electroencephalography (EEG) is the large number of EEG sensor electrodes typically used. The necessity for this results from the fact that the relevant information for the BCI is often spread over the scalp in complex patterns that differ depending on subjects and application scenarios. Recently, a number of methods have been proposed to determine an individual optimal sensor selection. These methods have, however, rarely been compared against each other or against any type of baseline. In this paper, we review several selection approaches and propose one additional selection criterion based on the evaluation of the performance of a BCI system using a reduced set of sensors. We evaluate the methods in the context of a passive BCI system that is designed to detect a P300 event-related potential and compare the performance of the methods against randomly generated sensor constellations. For a realistic estimation of the reduced system's performance we transfer sensor constellations found on one experimental session to a different session for evaluation. We identified notable (and unanticipated) differences among the methods and could demonstrate that the best method in our setup is able to reduce the required number of sensors considerably. Though our application focuses on EEG data, all presented algorithms and evaluation schemes can be transferred to any binary classification task on sensor arrays."," *Algorithms; Brain/anatomy & histology/*physiology; Brain Mapping; *Brain-Computer Interfaces; Electrodes; Electroencephalography/instrumentation/*methods; Event-Related Potentials, P300/*physiology; Humans",-2,,
396,Chen,2014,International journal of medical informatics,An RFID solution for enhancing inpatient medication safety with real-time verifiable grouping-proof,"PURPOSE: The occurrence of a medication error can threaten patient safety. The medication administration process is complex and cumbersome, and nursing staffs are prone to error when they are tired. Proper Information Technology (IT) can assist the nurse in correct medication administration. METHOD: We review a recent proposal regarding a leading-edge solution to enhance inpatient medication safety by using RFID technology. The proof mechanism is the kernel concept in their design and worth studying to develop a well-designed grouping-proof scheme. Other RFID grouping-proof protocols could be similarly applied in administering physician orders. We improve on the weaknesses of previous works and develop a reading-order independent RFID grouping-proof scheme in this paper. RESULT: In our scheme, tags are queried and verified under the direct control of the authorized reader without connecting to the back-end database server. Immediate verification in our design makes this application more portable and efficient and critical security issues have been analyzed by the threat model. CONCLUSION: Our scheme is suitable for the safe drug administration scenario and the drug package scenario in a hospital environment to enhance inpatient medication safety. It automatically checks for correct drug unit-dose and appropriate inpatient treatments."," Algorithms; *Computer Systems; Humans; Medication Errors/*prevention & control; *Medication Systems, Hospital; *Patient Safety; Radio Frequency Identification Device/*methods; Authorization; Grouping-proof; Medication safety; Rfid; Security",-2,,
397,Oberbichler,2013,Studies in health technology and informatics,Functional requirements regarding medical registries--preliminary results,The term medical registry is used to reference tools and processes to support clinical or epidemiologic research or provide a data basis for decisions regarding health care policies. In spite of this wide range of applications the term registry and the functional requirements which a registry should support are not clearly defined. This work presents preliminary results of a literature review to discover functional requirements which form a registry. To extract these requirements a set of peer reviewed articles was collected. These set of articles was screened by using methods from qualitative research. Up to now most discovered functional requirements focus on data quality (e. g. prevent transcription error by conducting automatic domain checks).," Data Mining/*methods; *Databases, Factual; Information Storage and Retrieval/*methods; *Natural Language Processing; Pilot Projects; *Registries; *Terminology as Topic",-2,,
398,Lialiou,2013,Studies in health technology and informatics,Evaluation of health professionals in the use of internet information retrieval systems in health: a literature review,"This paper presents a literature review on how health professionals are seeking health information using internet retrieval systems, databases. Publications present many attitude scales which evaluate the behavior of the users and the barriers that they face through the information research. On the following review are mentioned the characteristics that health professionals encounter on the use of computing. Also, is mentioned a number of problems which are associated with the information recourses such as reliability that were elicited and reviewed."," *Attitude of Health Personnel; Data Mining/*statistics & numerical data; Databases, Factual/*statistics & numerical data; Health Literacy/*statistics & numerical data; Health Personnel/*statistics & numerical data; Information Storage and Retrieval/*statistics & numerical data; Internet/*statistics & numerical data; Medical Staff/statistics & numerical data",-2,,
399,Topaz,2014,Journal of the American Medical Informatics Association : JAMIA,The Omaha System: a systematic review of the recent literature,"BACKGROUND: The Omaha System (OS) is one of the oldest of the American Nurses Association recognized standardized terminologies describing and measuring the impact of healthcare services. This systematic review presents the state of science on the use of the OS in practice, research, and education. AIMS: (1) To identify, describe and evaluate the publications on the OS between 2004 and 2011, (2) to identify major trends in the use of the OS in research, practice, and education, and (3) to suggest areas for future research. METHODS: Systematic search in the largest online healthcare databases (PUBMED, CINAHL, Scopus, PsycINFO, Ovid) from 2004 to 2011. Methodological quality of the reviewed research studies was evaluated. RESULTS: 56 publications on the OS were identified and analyzed. The methodological quality of the reviewed research studies was relatively high. Over time, publications' focus shifted from describing clients' problems toward outcomes research. There was an increasing application of advanced statistical methods and a significant portion of authors focused on classification and interoperability research. There was an increasing body of international literature on the OS. Little research focused on the theoretical aspects of the OS, the effective use of the OS in education, or cultural adaptations of the OS outside the USA. CONCLUSIONS: The OS has a high potential to provide meaningful and high quality information about complex healthcare services. Further research on the OS should focus on its applicability in healthcare education, theoretical underpinnings and international validity. Researchers analyzing the OS data should address how they attempted to mitigate the effects of missing data in analyzing their results and clearly present the limitations of their studies."," *Bibliometrics; Clinical Coding; Databases, Bibliographic; Electronic Health Records; Humans; Nursing Informatics; *Nursing Research; *Outcome Assessment (Health Care); *Vocabulary, Controlled; Data Mining; Nursing Classification; Omaha System; Terminology as Topic",-2,,
400,Muller,2013,Systematic reviews,Defining publication bias: protocol for a systematic review of highly cited articles and proposal for a new framework,"BACKGROUND: Selective publication of studies, which is commonly called publication bias, is widely recognized. Over the years a new nomenclature for other types of bias related to non-publication or distortion related to the dissemination of research findings has been developed. However, several of these different biases are often still summarized by the term 'publication bias'. METHODS/DESIGN: As part of the OPEN Project (To Overcome failure to Publish nEgative fiNdings) we will conduct a systematic review with the following objectives:- To systematically review highly cited articles that focus on non-publication of studies and to present the various definitions of biases related to the dissemination of research findings contained in the articles identified.- To develop and discuss a new framework on nomenclature of various aspects of distortion in the dissemination process that leads to public availability of research findings in an international group of experts in the context of the OPEN Project.We will systematically search Web of Knowledge for highly cited articles that provide a definition of biases related to the dissemination of research findings. A specifically designed data extraction form will be developed and pilot-tested. Working in teams of two, we will independently extract relevant information from each eligible article.For the development of a new framework we will construct an initial table listing different levels and different hazards en route to making research findings public. An international group of experts will iteratively review the table and reflect on its content until no new insights emerge and consensus has been reached. DISCUSSION: Results are expected to be publicly available in mid-2013. This systematic review together with the results of other systematic reviews of the OPEN project will serve as a basis for the development of future policies and guidelines regarding the assessment and prevention of publication bias.", *Access to Information; *Bias; Consensus; Data Mining; Humans; Information Dissemination; *Meta-Analysis as Topic; *Publication Bias; *Research Design; *Systematic Reviews as Topic; *Terminology as Topic,-2,,
402,Sun,2013,Journal of the American Medical Informatics Association : JAMIA,Temporal reasoning over clinical text: the state of the art,"OBJECTIVES: To provide an overview of the problem of temporal reasoning over clinical text and to summarize the state of the art in clinical natural language processing for this task. TARGET AUDIENCE: This overview targets medical informatics researchers who are unfamiliar with the problems and applications of temporal reasoning over clinical text. SCOPE: We review the major applications of text-based temporal reasoning, describe the challenges for software systems handling temporal information in clinical text, and give an overview of the state of the art. Finally, we present some perspectives on future research directions that emerged during the recent community-wide challenge on text-based temporal reasoning in the clinical domain.", Artificial Intelligence; *Electronic Health Records; Humans; Information Storage and Retrieval/*methods; *Natural Language Processing; Software; Time; Medical language processing; Natural language processing; Temporal reasoning,1,,
401,Wolfe,2013,Systematic reviews,Strategies for obtaining unpublished drug trial data: a qualitative interview study,"BACKGROUND: Authors of systematic reviews have difficulty obtaining unpublished data for their reviews. This project aimed to provide an in-depth description of the experiences of authors in searching for and gaining access to unpublished data for their systematic reviews, and to give guidance on best practices for identifying, obtaining and using unpublished data. METHODS: This is a qualitative study analyzing in-depth interviews with authors of systematic reviews who have published Cochrane reviews or published systematic reviews outside of The Cochrane Library. We included participants who 1) were the first or senior author of a published systematic review of a drug intervention, 2) had expertise in conducting systematic reviews, searching for data, and assessing methodological biases, and 3) were able to participate in an interview in English. We used non-random sampling techniques to identify potential participants. Eighteen Cochrane authors were contacted and 16 agreed to be interviewed (89% response rate). Twenty-four non-Cochrane authors were contacted and 16 were interviewed (67% response rate). RESULTS: Respondents had different understandings of what was meant by unpublished data, including specific outcomes and methodological details. Contacting study authors was the most common method used to obtain unpublished data and the value of regulatory agencies as a data source was underappreciated. Using the data obtained was time consuming and labor intensive. Respondents described the collaboration with other colleagues and/or students required to organize, manage and use the data in their reviews, generally developing and using templates, spreadsheets and computer programs for data extraction and analysis. Respondents had a shared belief that data should be accessible but some had concerns about sharing their own data. Respondents believed that obtaining unpublished data for reviews has important public health implications. There was widespread support for government intervention to ensure open access to trial data. CONCLUSIONS: Respondents uniformly agreed that the benefit of identifying unpublished data was worth the effort and was necessary to identify the true harms and benefits of drugs. Recent actions by government, such as increased availability of trial data from the European Medicines Agency, may make it easier to acquire critical drug trial data.", *Access to Information; *Attitude; Comprehension; Cooperative Behavior; Data Collection; Data Mining; Humans; *Information Dissemination; Interviews as Topic; *Pharmaceutical Preparations; *Publishing; Qualitative Research; *Review Literature as Topic,-2,,
403,Boulesteix,2013,PloS one,A plea for neutral comparison studies in computational sciences,"In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of ""new methods"", while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a ""tidy neutral comparison study"". R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boule steix/plea2013.", *Artificial Intelligence; *Computational Biology/methods; Humans,-1,,
405,Kovacevic,2013,Journal of the American Medical Informatics Association : JAMIA,Combining rules and machine learning for extraction of temporal expressions and events from clinical narratives,"OBJECTIVE: Identification of clinical events (eg, problems, tests, treatments) and associated temporal expressions (eg, dates and times) are key tasks in extracting and managing data from electronic health records. As part of the i2b2 2012 Natural Language Processing for Clinical Data challenge, we developed and evaluated a system to automatically extract temporal expressions and events from clinical narratives. The extracted temporal expressions were additionally normalized by assigning type, value, and modifier. MATERIALS AND METHODS: The system combines rule-based and machine learning approaches that rely on morphological, lexical, syntactic, semantic, and domain-specific features. Rule-based components were designed to handle the recognition and normalization of temporal expressions, while conditional random fields models were trained for event and temporal recognition. RESULTS: The system achieved micro F scores of 90% for the extraction of temporal expressions and 87% for clinical event extraction. The normalization component for temporal expressions achieved accuracies of 84.73% (expression's type), 70.44% (value), and 82.75% (modifier). DISCUSSION: Compared to the initial agreement between human annotators (87-89%), the system provided comparable performance for both event and temporal expression mining. While (lenient) identification of such mentions is achievable, finding the exact boundaries proved challenging. CONCLUSIONS: The system provides a state-of-the-art method that can be used to support automated identification of mentions of clinical events and temporal expressions in narratives either to support the manual review process or as a part of a large-scale processing of electronic health databases.", *Artificial Intelligence; *Electronic Health Records; Humans; Information Storage and Retrieval/*methods; Natural Language Processing; Time; Translational Medical Research; clinical NLP; clinical text mining; event extraction; termporal expression extraction; termporal expression normalization,1,,
406,Wei,2013,Journal of the American Medical Informatics Association : JAMIA,Development and evaluation of an ensemble resource linking medications to their indications,"OBJECTIVE: To create a computable MEDication Indication resource (MEDI) to support primary and secondary use of electronic medical records (EMRs). MATERIALS AND METHODS: We processed four public medication resources, RxNorm, Side Effect Resource (SIDER) 2, MedlinePlus, and Wikipedia, to create MEDI. We applied natural language processing and ontology relationships to extract indications for prescribable, single-ingredient medication concepts and all ingredient concepts as defined by RxNorm. Indications were coded as Unified Medical Language System (UMLS) concepts and International Classification of Diseases, 9th edition (ICD9) codes. A total of 689 extracted indications were randomly selected for manual review for accuracy using dual-physician review. We identified a subset of medication-indication pairs that optimizes recall while maintaining high precision. RESULTS: MEDI contains 3112 medications and 63 343 medication-indication pairs. Wikipedia was the largest resource, with 2608 medications and 34 911 pairs. For each resource, estimated precision and recall, respectively, were 94% and 20% for RxNorm, 75% and 33% for MedlinePlus, 67% and 31% for SIDER 2, and 56% and 51% for Wikipedia. The MEDI high-precision subset (MEDI-HPS) includes indications found within either RxNorm or at least two of the three other resources. MEDI-HPS contains 13 304 unique indication pairs regarding 2136 medications. The mean+/-SD number of indications for each medication in MEDI-HPS is 6.22 +/- 6.09. The estimated precision of MEDI-HPS is 92%. CONCLUSIONS: MEDI is a publicly available, computable resource that links medications with their indications as represented by concepts and billing codes. MEDI may benefit clinical EMR applications and reuse of EMR data for research.", Dictionaries as Topic; *Drug Therapy; *Electronic Health Records; Internet; MedlinePlus; *Natural Language Processing; *Pharmaceutical Preparations; RxNorm; International Classification of Diseases; Ontology; Terminology; Unified Medical Language System; electronic medical records; medication indications,1,,
407,Wagholikar,2013,Journal of the American Medical Informatics Association : JAMIA,Formative evaluation of the accuracy of a clinical decision support system for cervical cancer screening,"OBJECTIVES: We previously developed and reported on a prototype clinical decision support system (CDSS) for cervical cancer screening. However, the system is complex as it is based on multiple guidelines and free-text processing. Therefore, the system is susceptible to failures. This report describes a formative evaluation of the system, which is a necessary step to ensure deployment readiness of the system. MATERIALS AND METHODS: Care providers who are potential end-users of the CDSS were invited to provide their recommendations for a random set of patients that represented diverse decision scenarios. The recommendations of the care providers and those generated by the CDSS were compared. Mismatched recommendations were reviewed by two independent experts. RESULTS: A total of 25 users participated in this study and provided recommendations for 175 cases. The CDSS had an accuracy of 87% and 12 types of CDSS errors were identified, which were mainly due to deficiencies in the system's guideline rules. When the deficiencies were rectified, the CDSS generated optimal recommendations for all failure cases, except one with incomplete documentation. DISCUSSION AND CONCLUSIONS: The crowd-sourcing approach for construction of the reference set, coupled with the expert review of mismatched recommendations, facilitated an effective evaluation and enhancement of the system, by identifying decision scenarios that were missed by the system's developers. The described methodology will be useful for other researchers who seek rapidly to evaluate and enhance the deployment readiness of complex decision support systems."," Data Mining; *Decision Support Systems, Clinical; Early Detection of Cancer; Electronic Health Records; Female; Humans; Natural Language Processing; Practice Guidelines as Topic; Uterine Cervical Neoplasms/*diagnosis; Crowdsourcing; Decision Support Systems, Clinical; Guideline Adherence; Uterine Cervical Neoplasms; Vaginal Smears; Validation Studies as Topic",-2,,
408,Sun,2013,Journal of the American Medical Informatics Association : JAMIA,Evaluating temporal relations in clinical text: 2012 i2b2 Challenge,"BACKGROUND: The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions. METHODS: The challenge evaluated systems on the information extraction tasks that targeted: (1) clinically significant events, including both clinical concepts such as problems, tests, treatments, and clinical departments, and events relevant to the patient's clinical timeline, such as admissions, transfers between departments, etc; (2) temporal expressions, referring to the dates, times, durations, or frequencies phrases in the clinical text. The values of the extracted temporal expressions had to be normalized to an ISO specification standard; and (3) temporal relations, between the clinical events and temporal expressions. Participants determined pairs of events and temporal expressions that exhibited a temporal relation, and identified the temporal relation between them. RESULTS: For event detection, statistical machine learning (ML) methods consistently showed superior performance. While ML and rule based methods seemed to detect temporal expressions equally well, the best systems overwhelmingly adopted a rule based approach for value normalization. For temporal relation classification, the systems using hybrid approaches that combined ML and heuristics based methods produced the best results.", *Artificial Intelligence; *Electronic Health Records; Humans; Natural Language Processing; *Patient Discharge Summaries; Time; *Translational Medical Research; clinical language processing; medical language processing; sharedtask challenges; temporal reasoning,2,,
404,Davis,2013,PloS one,Text mining effectively scores and ranks the literature for improving chemical-gene-disease curation at the comparative toxicogenomics database,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a public resource that curates interactions between environmental chemicals and gene products, and their relationships to diseases, as a means of understanding the effects of environmental chemicals on human health. CTD provides a triad of core information in the form of chemical-gene, chemical-disease, and gene-disease interactions that are manually curated from scientific articles. To increase the efficiency, productivity, and data coverage of manual curation, we have leveraged text mining to help rank and prioritize the triaged literature. Here, we describe our text-mining process that computes and assigns each article a document relevancy score (DRS), wherein a high DRS suggests that an article is more likely to be relevant for curation at CTD. We evaluated our process by first text mining a corpus of 14,904 articles triaged for seven heavy metals (cadmium, cobalt, copper, lead, manganese, mercury, and nickel). Based upon initial analysis, a representative subset corpus of 3,583 articles was then selected from the 14,094 articles and sent to five CTD biocurators for review. The resulting curation of these 3,583 articles was analyzed for a variety of parameters, including article relevancy, novel data content, interaction yield rate, mean average precision, and biological and toxicological interpretability. We show that for all measured parameters, the DRS is an effective indicator for scoring and improving the ranking of literature for the curation of chemical-gene-disease information at CTD. Here, we demonstrate how fully incorporating text mining-based DRS scoring into our curation pipeline enhances manual curation by prioritizing more relevant articles, thereby increasing data content, productivity, and efficiency."," Algorithms; Data Mining/*methods; *Databases, Factual; Disease/*genetics; Documentation; Humans; Metals, Heavy/toxicity; *Molecular Sequence Annotation; *Publications; Reproducibility of Results; *Toxicogenetics",2,,
409,Zarski,2013,PloS one,Contribution of the ELFG test in algorithms of non-invasive markers towards the diagnosis of significant fibrosis in chronic hepatitis C,"BACKGROUND AND AIMS: We aimed to determine the best algorithms for the diagnosis of significant fibrosis in chronic hepatitis C (CHC) patients using all available parameters and tests. PATIENTS AND METHODS: We used the database from our study of 507 patients with histologically proven CHC in which fibrosis was evaluated by liver biopsy (Metavir) and tests: Fibrometer(R), Fibrotest(R), Hepascore(R), Apri, ELFG, MP3, Forn's, hyaluronic acid, tissue inhibitor of metalloproteinase-1 (TIMP1), MMP1, collagen IV and when possible Fibroscan. For the first test we used 90% negative predictive value to exclude patients with F</=1, next an induction algorithm was applied giving the best tests with at least 80% positive predictive value for the diagnosis of F>/=2. The algorithms were computed using the R Software C4.5 program to select the best tests and cut-offs. The algorithm was automatically induced without premises on the part of the investigators. We also examined the inter-observer variations after independent review of liver biopsies by two pathologists. A medico-economic analysis compared the screening strategies with liver biopsy. RESULTS: In ""intention to diagnose"" the best algorithms for F>/=2 were Fibrometer (R), Fibrotest(R), or Hepascore(R) in first intention with the ELFG score in second intention for indeterminate cases. The percentage of avoided biopsies varied between 50% (Fibrotest(R) or Fibrometer(R)+ELFG) and 51% (Hepascore(R)+ELFG). In ""per-analysis"" Fibroscan+ELFG avoided liver biopsy in 55% of cases. The diagnostic performance of these screening strategies was statistically superior to the usual combinations (Fibrometer(R) or Fibrotest(R)+Fibroscan) and was cost effective. We note that the consensual review of liver biopsies between the two pathologists was mainly in favor of F1 (64-69%). CONCLUSION: The ELFG test could replace Fibroscan in most currently used algorithms for the diagnosis of significant fibrosis including for those patients for whom Fibroscan is unusable."," Algorithms; Biomarkers/blood; Biopsy; Elasticity Imaging Techniques; Hepatitis C, Chronic/*complications; Humans; Liver/pathology; Liver Cirrhosis/blood/*diagnosis/*etiology; Principal Component Analysis",-2,,
410,Wright,2013,Journal of the American Medical Informatics Association : JAMIA,Use of a support vector machine for categorizing free-text notes: assessment of accuracy across two institutions,"BACKGROUND: Electronic health record (EHR) users must regularly review large amounts of data in order to make informed clinical decisions, and such review is time-consuming and often overwhelming. Technologies like automated summarization tools, EHR search engines and natural language processing have been shown to help clinicians manage this information. OBJECTIVE: To develop a support vector machine (SVM)-based system for identifying EHR progress notes pertaining to diabetes, and to validate it at two institutions. MATERIALS AND METHODS: We retrieved 2000 EHR progress notes from patients with diabetes at the Brigham and Women's Hospital (1000 for training and 1000 for testing) and another 1000 notes from the University of Texas Physicians (for validation). We manually annotated all notes and trained a SVM using a bag of words approach. We then used the SVM on the testing and validation sets and evaluated its performance with the area under the curve (AUC) and F statistics. RESULTS: The model accurately identified diabetes-related notes in both the Brigham and Women's Hospital testing set (AUC=0.956, F=0.934) and the external University of Texas Faculty Physicians validation set (AUC=0.947, F=0.935). DISCUSSION: Overall, the model we developed was quite accurate. Furthermore, it generalized, without loss of accuracy, to another institution with a different EHR and a distinct patient and provider population. CONCLUSIONS: It is possible to use a SVM-based classifier to identify EHR progress notes pertaining to diabetes, and the model generalizes well.", Diabetes Mellitus; *Electronic Health Records; Humans; ROC Curve; Search Engine; *Support Vector Machine; electronic health record; natural language processing; search; support vector machine,1,,
411,Foran,2013,Studies in health technology and informatics,Automated image interpretation and computer-assisted diagnostics,"Much of the difficulty in reaching consistent evaluations of radiology and pathology imaging studies arises from subjective impressions of individual observers. Developing strategies that can reliably transform complex visual observations into well-defined algorithmic procedures is an active area of exploration which can advance clinical practice, investigative research and outcome studies. The literature shows that when characterizations are based upon computer-aided analysis, objectivity, reproducibility and sensitivity improve considerably. Advanced imaging and computational tools could potentially enable investigators to detect and track subtle changes in measurable parameters leading to the discovery of novel diagnostic and prognostic clues which are not apparent by human visual inspection alone. The overarching objective of this book chapter is to provide readers with a summary of the origin, evolution and future directions for the fields of automated image interpretation and computer-assisted diagnostics. The chapter begins with a high-level overview of the fields of image processing, pattern recognition, and computer vision followed by a description of how these disciplines relate to the more comprehensive fields of computer-assisted diagnostics and image guided decision support. Throughout the remainder of the chapter we have supplied multiple illustrative examples demonstrating how recent advances and innovations in each of these areas have impacted clinical and research activities throughout pathology and radiology including high-throughput tissue microarray analysis, multi-spectral imaging, and image co-registration."," *Algorithms; *Artificial Intelligence; Image Enhancement/*methods; Image Interpretation, Computer-Assisted/*methods; Microarray Analysis/*methods; Pattern Recognition, Automated/*methods",-2,,
412,Fung,2013,Journal of the American Medical Informatics Association : JAMIA,Extracting drug indication information from structured product labels using natural language processing,"OBJECTIVE: To extract drug indications from structured drug labels and represent the information using codes from standard medical terminologies. MATERIALS AND METHODS: We used MetaMap and other publicly available resources to extract information from the indications section of drug labels. Drugs and indications were encoded by RxNorm and UMLS identifiers respectively. A sample was manually reviewed. We also compared the results with two independent information sources: National Drug File-Reference Terminology and the Semantic Medline project. RESULTS: A total of 6797 drug labels were processed, resulting in 19 473 unique drug-indication pairs. Manual review of 298 most frequently prescribed drugs by seven physicians showed a recall of 0.95 and precision of 0.77. Inter-rater agreement (Fleiss kappa) was 0.713. The precision of the subset of results corroborated by Semantic Medline extractions increased to 0.93. DISCUSSION: Correlation of a patient's medical problems and drugs in an electronic health record has been used to improve data quality and reduce medication errors. Authoritative drug indication information is available from drug labels, but not in a format readily usable by computer applications. Our study shows that it is feasible to use publicly available natural language processing resources to extract drug indications from drug labels. The same method can be applied to other sections of the drug label-for example, adverse effects, contraindications. CONCLUSIONS: It is feasible to use publicly available natural language processing tools to extract indication information from freely available drug labels. Named entity recognition sources (eg, MetaMap) provide reasonable recall. Combination with other data sources provides higher precision.", *Drug Labeling; Humans; *Natural Language Processing; Pharmaceutical Preparations/classification; Prescription Drugs; RxNorm,1,,
413,Ewald,2013,PloS one,Expression microarray meta-analysis identifies genes associated with Ras/MAPK and related pathways in progression of muscle-invasive bladder transition cell carcinoma,"The effective detection and management of muscle-invasive bladder Transition Cell Carcinoma (TCC) continues to be an urgent clinical challenge. While some differences of gene expression and function in papillary (Ta), superficial (T1) and muscle-invasive (>/=T2) bladder cancers have been investigated, the understanding of mechanisms involved in the progression of bladder tumors remains incomplete. Statistical methods of pathway-enrichment, cluster analysis and text-mining can extract and help interpret functional information about gene expression patterns in large sets of genomic data. The public availability of patient-derived expression microarray data allows open access and analysis of large amounts of clinical data. Using these resources, we investigated gene expression differences associated with tumor progression and muscle-invasive TCC. Gene expression was calculated relative to Ta tumors to assess progression-associated differences, revealing a network of genes related to Ras/MAPK and PI3K signaling pathways with increased expression. Further, we identified genes within this network that are similarly expressed in superficial Ta and T1 stages but altered in muscle-invasive T2 tumors, finding 7 genes (COL3A1, COL5A1, COL11A1, FN1, ErbB3, MAPK10 and CDC25C) whose expression patterns in muscle-invasive tumors are consistent in 5 to 7 independent outside microarray studies. Further, we found increased expression of the fibrillar collagen proteins COL3A1 and COL5A1 in muscle-invasive tumor samples and metastatic T24 cells. Our results suggest that increased expression of genes involved in mitogenic signaling may support the progression of muscle-invasive bladder tumors that generally lack activating mutations in these pathways, while expression changes of fibrillar collagens, fibronectin and specific signaling proteins are associated with muscle-invasive disease. These results identify potential biomarkers and targets for TCC treatments, and provide an integrated systems-level perspective of TCC pathobiology to inform future studies."," Aged; Aged, 80 and over; Blotting, Western; Carcinoma, Transitional Cell/*genetics/pathology; Computational Biology; Fibrillar Collagens/metabolism; Gene Expression Regulation, Neoplastic/*genetics; Humans; Immunohistochemistry; Microarray Analysis; Middle Aged; Mitogen-Activated Protein Kinases/*metabolism; Muscle, Skeletal/*pathology; Neoplasm Invasiveness/pathology; Signal Transduction/*genetics; Urinary Bladder Neoplasms/*genetics/pathology; ras Proteins/*metabolism",-2,,
415,Toyabe,2012,BMC health services research,Detecting inpatient falls by using natural language processing of electronic medical records,"BACKGROUND: Incident reporting is the most common method for detecting adverse events in a hospital. However, under-reporting or non-reporting and delay in submission of reports are problems that prevent early detection of serious adverse events. The aim of this study was to determine whether it is possible to promptly detect serious injuries after inpatient falls by using a natural language processing method and to determine which data source is the most suitable for this purpose. METHODS: We tried to detect adverse events from narrative text data of electronic medical records by using a natural language processing method. We made syntactic category decision rules to detect inpatient falls from text data in electronic medical records. We compared how often the true fall events were recorded in various sources of data including progress notes, discharge summaries, image order entries and incident reports. We applied the rules to these data sources and compared F-measures to detect falls between these data sources with reference to the results of a manual chart review. The lag time between event occurrence and data submission and the degree of injury were compared. RESULTS: We made 170 syntactic rules to detect inpatient falls by using a natural language processing method. Information on true fall events was most frequently recorded in progress notes (100%), incident reports (65.0%) and image order entries (12.5%). However, F-measure to detect falls using the rules was poor when using progress notes (0.12) and discharge summaries (0.24) compared with that when using incident reports (1.00) and image order entries (0.91). Since the results suggested that incident reports and image order entries were possible data sources for prompt detection of serious falls, we focused on a comparison of falls found by incident reports and image order entries. Injury caused by falls found by image order entries was significantly more severe than falls detected by incident reports (p<0.001), and the lag time between falls and submission of data to the hospital information system was significantly shorter in image order entries than in incident reports (p<0.001). CONCLUSIONS: By using natural language processing of text data from image order entries, we could detect injurious falls within a shorter time than that by using incident reports. Concomitant use of this method might improve the shortcomings of an incident reporting system such as under-reporting or non-reporting and delayed submission of data on incidents.", Accidental Falls/*statistics & numerical data; Electronic Health Records/*statistics & numerical data; Humans; Injury Severity Score; Inpatients/*statistics & numerical data; *Natural Language Processing; Sensitivity and Specificity; Wounds and Injuries/epidemiology/etiology,-1,,
414,Sinnemaki,2013,Systematic reviews,Automated dose dispensing service for primary healthcare patients: a systematic review,"BACKGROUND: An automated dose dispensing (ADD) service has been implemented in primary healthcare in some European countries. In this service, regularly used medicines are machine-packed into unit-dose bags for each time of administration. The aim of this study is to review the evidence for ADD's influence on the appropriateness of medication use, medication safety, and costs in primary healthcare. METHODS: A literature search was performed in April 2012 in the most relevant databases (n = 10), including the Medline, Embase, and Cochrane Library. The reference lists of the studies selected were manually searched. A study was included in the review if the study was conducted in primary healthcare or nursing home settings and medicines were dispensed in unit-dose bags. RESULTS: Out of 328 abstracts, seven studies met the inclusion and reporting quality criteria, but none applied a randomized controlled study design. Of the four controlled studies, one was a national register-based study. It showed that the patient group in the ADD scheme more often used three or more psychotropic drugs and anticholinergics than patients using the standard dispensing procedure, while women in the ADD group used less long-acting benzodiazepines and both genders had fewer drug-drug interactions. In another, regional controlled study, the ADD group consisted of patients with higher risk of inappropriate drug use, according to all indicators applied. The third controlled study indicated that ADD user drug treatments were more likely to remain unchanged than in patients using a standard dispensing procedure. A controlled study from Norway showed that ADD reduced discrepancies in the documentation of patient medication records. Costs were not investigated in any of the studies. CONCLUSIONS: A very limited number of controlled studies have explored ADD in primary healthcare. Consequently, the evidence for ADD's influence on appropriateness and safety of medication use is limited and lacking in information on costs. The findings of this review suggest that patients using the ADD have more inappropriate drugs in their regimens, and that ADD may improve medication safety in terms of reducing the discrepancies in medication records. Further evidence is needed to draw sound conclusions on ADD's outcomes.", Europe; Female; Humans; Male; Medical Records; *Medication Systems; *Patient Safety; Pharmaceutical Preparations/*administration & dosage; *Primary Health Care,-2,,
416,van Valkenhoef,2012,Research synthesis methods,Automating network meta-analysis,"Mixed treatment comparison (MTC) (also called network meta-analysis) is an extension of traditional meta-analysis to allow the simultaneous pooling of data from clinical trials comparing more than two treatment options. Typically, MTCs are performed using general-purpose Markov chain Monte Carlo software such as WinBUGS, requiring a model and data to be specified using a specific syntax. It would be preferable if, for the most common cases, both could be derived from a well-structured data file that can be easily checked for errors. Automation is particularly valuable for simulation studies in which the large number of MTCs that have to be estimated may preclude manual model specification and analysis. Moreover, automated model generation raises issues that provide additional insight into the nature of MTC. We present a method for the automated generation of Bayesian homogeneous variance random effects consistency models, including the choice of basic parameters and trial baselines, priors, and starting values for the Markov chain(s). We validate our method against the results of five published MTCs. The method is implemented in freely available open source software. This means that performing an MTC no longer requires manually writing a statistical model. This reduces time and effort, and facilitates error checking of the dataset. Copyright (c) 2012 John Wiley & Sons, Ltd.", ,1,,
417,O'Hagan,2012,PloS one,Exploiting genomic knowledge in optimising molecular breeding programmes: algorithms from evolutionary computing,"Comparatively few studies have addressed directly the question of quantifying the benefits to be had from using molecular genetic markers in experimental breeding programmes (e.g. for improved crops and livestock), nor the question of which organisms should be mated with each other to best effect. We argue that this requires in silico modelling, an approach for which there is a large literature in the field of evolutionary computation (EC), but which has not really been applied in this way to experimental breeding programmes. EC seeks to optimise measurable outcomes (phenotypic fitnesses) by optimising in silico the mutation, recombination and selection regimes that are used. We review some of the approaches from EC, and compare experimentally, using a biologically relevant in silico landscape, some algorithms that have knowledge of where they are in the (genotypic) search space (G-algorithms) with some (albeit well-tuned ones) that do not (F-algorithms). For the present kinds of landscapes, F- and G-algorithms were broadly comparable in quality and effectiveness, although we recognise that the G-algorithms were not equipped with any 'prior knowledge' of epistatic pathway interactions. This use of algorithms based on machine learning has important implications for the optimisation of experimental breeding programmes in the post-genomic era when we shall potentially have access to the full genome sequence of every organism in a breeding population. The non-proprietary code that we have used is made freely available (via Supplementary information)."," *Algorithms; Computational Biology/*methods; DNA Shuffling/*methods; *Evolution, Molecular; Genome/*genetics; *Genomics; *Knowledge",-2,,
418,Alexandrov,2012,BMC bioinformatics,MALDI imaging mass spectrometry: statistical data analysis and current computational challenges,"Matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) imaging mass spectrometry, also called MALDI-imaging, is a label-free bioanalytical technique used for spatially-resolved chemical analysis of a sample. Usually, MALDI-imaging is exploited for analysis of a specially prepared tissue section thaw mounted onto glass slide. A tremendous development of the MALDI-imaging technique has been observed during the last decade. Currently, it is one of the most promising innovative measurement techniques in biochemistry and a powerful and versatile tool for spatially-resolved chemical analysis of diverse sample types ranging from biological and plant tissues to bio and polymer thin films. In this paper, we outline computational methods for analyzing MALDI-imaging data with the emphasis on multivariate statistical methods, discuss their pros and cons, and give recommendations on their application. The methods of unsupervised data mining as well as supervised classification methods for biomarker discovery are elucidated. We also present a high-throughput computational pipeline for interpretation of MALDI-imaging data using spatial segmentation. Finally, we discuss current challenges associated with the statistical analysis of MALDI-imaging data."," Biomarkers/analysis; Cluster Analysis; Computational Biology/*methods; Data Interpretation, Statistical; Spectrometry, Mass, Matrix-Assisted Laser; Desorption-Ionization/*methods/*statistics & numerical data",-2,,
419,Liaw,2013,International journal of medical informatics,Towards an ontology for data quality in integrated chronic disease management: a realist review of the literature,"PURPOSE: Effective use of routine data to support integrated chronic disease management (CDM) and population health is dependent on underlying data quality (DQ) and, for cross system use of data, semantic interoperability. An ontological approach to DQ is a potential solution but research in this area is limited and fragmented. OBJECTIVE: Identify mechanisms, including ontologies, to manage DQ in integrated CDM and whether improved DQ will better measure health outcomes. METHODS: A realist review of English language studies (January 2001-March 2011) which addressed data quality, used ontology-based approaches and is relevant to CDM. RESULTS: We screened 245 papers, excluded 26 duplicates, 135 on abstract review and 31 on full-text review; leaving 61 papers for critical appraisal. Of the 33 papers that examined ontologies in chronic disease management, 13 defined data quality and 15 used ontologies for DQ. Most saw DQ as a multidimensional construct, the most used dimensions being completeness, accuracy, correctness, consistency and timeliness. The majority of studies reported tool design and development (80%), implementation (23%), and descriptive evaluations (15%). Ontological approaches were used to address semantic interoperability, decision support, flexibility of information management and integration/linkage, and complexity of information models. CONCLUSION: DQ lacks a consensus conceptual framework and definition. DQ and ontological research is relatively immature with little rigorous evaluation studies published. Ontology-based applications could support automated processes to address DQ and semantic interoperability in repositories of routinely collected data to deliver integrated CDM. We advocate moving to ontology-based design of information systems to enable more reliable use of routine data to measure health mechanisms and impacts.", Chronic Disease; *Data Collection; *Disease Management; Humans; *Information Management; Medical Record Linkage; *Research Design,-2,,
420,Harpaz,2013,Journal of the American Medical Informatics Association : JAMIA,Combing signals from spontaneous reports and electronic health records for detection of adverse drug reactions,"OBJECTIVE: Data-mining algorithms that can produce accurate signals of potentially novel adverse drug reactions (ADRs) are a central component of pharmacovigilance. We propose a signal-detection strategy that combines the adverse event reporting system (AERS) of the Food and Drug Administration and electronic health records (EHRs) by requiring signaling in both sources. We claim that this approach leads to improved accuracy of signal detection when the goal is to produce a highly selective ranked set of candidate ADRs. MATERIALS AND METHODS: Our investigation was based on over 4 million AERS reports and information extracted from 1.2 million EHR narratives. Well-established methodologies were used to generate signals from each source. The study focused on ADRs related to three high-profile serious adverse reactions. A reference standard of over 600 established and plausible ADRs was created and used to evaluate the proposed approach against a comparator. RESULTS: The combined signaling system achieved a statistically significant large improvement over AERS (baseline) in the precision of top ranked signals. The average improvement ranged from 31% to almost threefold for different evaluation categories. Using this system, we identified a new association between the agent, rasburicase, and the adverse event, acute pancreatitis, which was supported by clinical review. CONCLUSIONS: The results provide promising initial evidence that combining AERS with EHRs via the framework of replicated signaling can improve the accuracy of signal detection for certain operating scenarios. The use of additional EHR data is required to further evaluate the capacity and limits of this system and to extend the generalizability of these results.", *Adverse Drug Reaction Reporting Systems; Drug-Related Side Effects and Adverse Reactions/*diagnosis; *Electronic Health Records; Humans; Pharmacovigilance,-2,,
421,Torrieri,2012,PloS one,Automatic assignment of prokaryotic genes to functional categories using literature profiling,"In the last years, there was an exponential increase in the number of publicly available genomes. Once finished, most genome projects lack financial support to review annotations. A few of these gene annotations are based on a combination of bioinformatics evidence, however, in most cases, annotations are based solely on sequence similarity to a previously known gene, which was most probably annotated in the same way. As a result, a large number of predicted genes remain unassigned to any functional category despite the fact that there is enough evidence in the literature to predict their function. We developed a classifier trained with term-frequency vectors automatically disclosed from text corpora of an ensemble of genes representative of each functional category of the J. Craig Venter Institute Comprehensive Microbial Resource (JCVI-CMR) ontology. The classifier achieved up to 84% precision with 68% recall (for confidence>/=0.4), F-measure 0.76 (recall and precision equally weighted) in an independent set of 2,220 genes, from 13 bacterial species, previously classified by JCVI-CMR into unambiguous categories of its ontology. Finally, the classifier assigned (confidence>/=0.7) to functional categories a total of 5,235 out of the approximately 24 thousand genes previously in categories ""Unknown function"" or ""Unclassified"" for which there is literature in MEDLINE. Two biologists reviewed the literature of 100 of these genes, randomly picket, and assigned them to the same functional categories predicted by the automatic classifier. Our results confirmed the hypothesis that it is possible to confidently assign genes of a real world repository to functional categories, based exclusively on the automatic profiling of its associated literature. The LitProf--Gene Classifier web server is accessible at: www.cebio.org/litprofGC."," *Computational Biology; *Databases, Genetic; Humans; Internet; *Medline; *Molecular Sequence Annotation/classification/methods",-2,,
423,Korotkov,2012,Artificial intelligence in medicine,Computerized analysis of pigmented skin lesions: a review,"OBJECTIVE: Computerized analysis of pigmented skin lesions (PSLs) is an active area of research that dates back over 25years. One of its main goals is to develop reliable automatic instruments for recognizing skin cancer from images acquired in vivo. This paper presents a review of this research applied to microscopic (dermoscopic) and macroscopic (clinical) images of PSLs. The review aims to: (1) provide an extensive introduction to and clarify ambiguities in the terminology used in the literature and (2) categorize and group together relevant references so as to simplify literature searches on a specific sub-topic. METHODS AND MATERIAL: The existing literature was classified according to the nature of publication (clinical or computer vision articles) and differentiating between individual and multiple PSL image analysis. We also emphasize the importance of the difference in content between dermoscopic and clinical images. RESULTS: Various approaches for implementing PSL computer-aided diagnosis systems and their standard workflow components are reviewed and summary tables provided. An extended categorization of PSL feature descriptors is also proposed, associating them with the specific methods for diagnosing melanoma, separating images of the two modalities and discriminating references according to our classification of the literature. CONCLUSIONS: There is a large discrepancy in the number of articles published on individual and multiple PSL image analysis and a scarcity of reported material on the automation of lesion change detection. At present, computer-aided diagnosis systems based on individual PSL image analysis cannot yet be used to provide the best diagnostic results. Furthermore, the absence of benchmark datasets for standardized algorithm evaluation is a barrier to a more dynamic development of this research area."," Diagnosis, Computer-Assisted/*methods; Diagnosis, Differential; Humans; Image Processing, Computer-Assisted/methods; Melanoma/diagnosis/pathology; Skin/pathology; Skin Neoplasms/diagnosis/pathology; *Skin Pigmentation",-2,,
422,Larson,2012,PloS one,Validation of an automated cough detection algorithm for tracking recovery of pulmonary tuberculosis patients,"BACKGROUND: A laboratory-free test for assessing recovery from pulmonary tuberculosis (TB) would be extremely beneficial in regions of the world where laboratory facilities are lacking. Our hypothesis is that analysis of cough sound recordings may provide such a test. In the current paper, we present validation of a cough analysis tool. METHODOLOGY/PRINCIPAL FINDINGS: Cough data was collected from a cohort of TB patients in Lima, Peru and 25.5 hours of recordings were manually annotated by clinical staff. Analysis software was developed and validated by comparison to manual scoring. Because many patients cough in bursts, coughing was characterized in terms of cough epochs. Our software correctly detects 75.5% of cough episodes with a specificity of 99.6% (comparable to past results using the same definition) and a median false positive rate of 4 false positives/hour, due to the noisy, real-world nature of our dataset. We then manually review detected coughs to eliminate false positives, in effect using the algorithm as a pre-screening tool that reduces reviewing time to roughly 5% of the recording length. This cough analysis approach provides a foundation to support larger-scale studies of coughing rates over time for TB patients undergoing treatment."," *Algorithms; *Automation; Cohort Studies; Cough/*physiopathology; Humans; Peru; Tuberculosis, Pulmonary/drug therapy/*physiopathology",-2,,
424,Gottesman,2012,PloS one,Can genetic pleiotropy replicate common clinical constellations of cardiovascular disease and risk?,"The relationship between obesity, diabetes, hyperlipidemia, hypertension, kidney disease and cardiovascular disease (CVD) is established when looked at from a clinical, epidemiological or pathophysiological perspective. Yet, when viewed from a genetic perspective, there is comparatively little data synthesis that these conditions have an underlying relationship. We sought to investigate the overlap of genetic variants independently associated with each of these commonly co-existing conditions from the NHGRI genome-wide association study (GWAS) catalog, in an attempt to replicate the established notion of shared pathophysiology and risk. We used pathway-based analyses to detect subsets of pleiotropic genes involved in similar biological processes. We identified 107 eligible GWAS studies related to CVD and its established comorbidities and risk factors and assigned genes that correspond to the associated signals based on their position. We found 44 positional genes shared across at least two CVD-related phenotypes that independently recreated the established relationship between the six phenotypes, but only if studies representing non-European populations were included. Seven genes revealed pleiotropy across three or more phenotypes, mostly related to lipid transport and metabolism. Yet, many genes had no relationship to each other or to genes with established functional connection. Whilst we successfully reproduced established relationships between CVD risk factors using GWAS findings, interpretation of biological pathways involved in the observed pleiotropy was limited. Further studies linking genetic variation to gene expression, as well as describing novel biological pathways will be needed to take full advantage of GWAS results."," African Continental Ancestry Group; Coronary Artery Disease/epidemiology/ethnology/*genetics; Data Mining; Diabetes Mellitus/ethnology/*genetics; European Continental Ancestry Group; *Genetic Pleiotropy; Genetic Predisposition to Disease; Genome-Wide Association Study; Humans; Hyperlipidemias/ethnology/*genetics; Hypertension/ethnology/*genetics; Kidney Diseases/ethnology/*genetics; Obesity/ethnology/*genetics; Polymorphism, Single Nucleotide; Risk Factors; United States/epidemiology",-2,,
425,Wyld,2012,PLoS medicine,A systematic review and meta-analysis of utility-based quality of life in chronic kidney disease treatments,"BACKGROUND: Chronic kidney disease (CKD) is a common and costly condition to treat. Economic evaluations of health care often incorporate patient preferences for health outcomes using utilities. The objective of this study was to determine pooled utility-based quality of life (the numerical value attached to the strength of an individual's preference for a specific health outcome) by CKD treatment modality. METHODS AND FINDINGS: We conducted a systematic review, meta-analysis, and meta-regression of peer-reviewed published articles and of PhD dissertations published through 1 December 2010 that reported utility-based quality of life (utility) for adults with late-stage CKD. Studies reporting utilities by proxy (e.g., reported by a patient's doctor or family member) were excluded. In total, 190 studies reporting 326 utilities from over 56,000 patients were analysed. There were 25 utilities from pre-treatment CKD patients, 226 from dialysis patients (haemodialysis, n = 163; peritoneal dialysis, n = 44), 66 from kidney transplant patients, and three from patients treated with non-dialytic conservative care. Using time tradeoff as a referent instrument, kidney transplant recipients had a mean utility of 0.82 (95% CI: 0.74, 0.90). The mean utility was comparable in pre-treatment CKD patients (difference = -0.02; 95% CI: -0.09, 0.04), 0.11 lower in dialysis patients (95% CI: -0.15, -0.08), and 0.2 lower in conservative care patients (95% CI: -0.38, -0.01). Patients treated with automated peritoneal dialysis had a significantly higher mean utility (0.80) than those on continuous ambulatory peritoneal dialysis (0.72; p = 0.02). The mean utility of transplant patients increased over time, from 0.66 in the 1980s to 0.85 in the 2000s, an increase of 0.19 (95% CI: 0.11, 0.26). Utility varied by elicitation instrument, with standard gamble producing the highest estimates, and the SF-6D by Brazier et al., University of Sheffield, producing the lowest estimates. The main limitations of this study were that treatment assignments were not random, that only transplant had longitudinal data available, and that we calculated EuroQol Group EQ-5D scores from SF-36 and SF-12 health survey data, and therefore the algorithms may not reflect EQ-5D scores measured directly. CONCLUSIONS: For patients with late-stage CKD, treatment with dialysis is associated with a significant decrement in quality of life compared to treatment with kidney transplantation. These findings provide evidence-based utility estimates to inform economic evaluations of kidney therapies, useful for policy makers and in individual treatment discussions with CKD patients."," Humans; Kidney Transplantation; *Quality of Life; Renal Dialysis; Renal Insufficiency, Chronic/*physiopathology/*therapy",-2,,
426,Overholser,2012,Studies in health technology and informatics,Adapting computerized treatments into traditional psychotherapy for depression,"UNLABELLED: Recent developments in technology have helped to improve the process of psychotherapy. Unfortunately, many therapists lack the computer skills or financial resources needed for the newest technology. Nonetheless, even basic advances in technology may help to improve the treatment of depression. METHOD: The literature is reviewed for journal articles on the treatment of depression published during the past seven years in which treatments have been guided by technology. RESULTS: Six novel findings are summarized that may be helpful even when the therapist lacks skill or resources for advanced technology. 1) The efficient assessment of depression can be facilitated by technology, whether using standardized measures or simple daily ratings of mood. 2) Technology tools can be used to send semi-automated daily reminders to help clients develop more adaptive habits in thoughts or actions. 3) Depressed clients can begin to confront their negative view of self, often triggered by some form of loss, failure, or rejection, whether real, imagined, or anticipated. 4) Clients can confront their problems through therapeutic dialogue, whether conducted in person, over the telephone, or via video conference. 5) Clients can use writing assignments to identify, label, explore and express their thoughts and feelings. These writing assignments can be conducted via paper, email, or internet forms. 6) Clients value rapport with a therapist, and this bond seems important to ensure participation and adherence with treatment. CONCLUSION: Even low-tech therapists can strengthen the treatment of depression using basic technology tools to replace, extend, or supplement traditional sessions. However, it is important to protect the rapport needed for sustained participation in therapy."," Adaptation, Psychological; Depression/*diagnosis/*therapy; Electronic Mail; Humans; Internet; Professional-Patient Relations; Psychotherapy/*trends; Reminder Systems; Remote Consultation; Self Concept; Therapy, Computer-Assisted/*trends; Writing",-2,,
427,Vakili,2012,Studies in health technology and informatics,Lessons learned from the development of technological support for PTSD prevention: a review,"This review describes the state-of-the-art technologies that support mental resilience training for PTSD prevention. It characterizes four current systems across training approaches; seeks insights via interviews with the system developers; and extracts from these a set of essential guidelines for future developers. The guidelines include four distinct project-limiting factors, which were found to constrain the reviewed developments. These were Culture, Effectiveness, Engineering, and Resource constraints.This research is novel in reviewing technologies for PTSD prevention as opposed to treatment, and in analyzing from the perspective of system development and design issues."," Adaptation, Psychological; Artificial Intelligence; Attention; Biofeedback, Psychology; Cognitive Behavioral Therapy; Game Theory; Humans; Psychological Theory; Stress Disorders, Post-Traumatic/*prevention & control/psychology; User-Computer Interface",-2,,
428,Rashidian,2012,PloS one,No evidence of the effect of the interventions to combat health care fraud and abuse: a systematic review of literature,"BACKGROUND: Despite the importance of health care fraud and the political, legislative and administrative attentions paid to it, combating fraud remains a challenge to the health systems. We aimed to identify, categorize and assess the effectiveness of the interventions to combat health care fraud and abuse. METHODS: The interventions to combat health care fraud can be categorized as the interventions for 'prevention' and 'detection' of fraud, and 'response' to fraud. We conducted sensitive search strategies on Embase, CINAHL, and PsycINFO from 1975 to 2008, and Medline from 1975-2010, and on relevant professional and organizational websites. Articles assessing the effectiveness of any intervention to combat health care fraud were eligible for inclusion in our review. We considered including the interventional studies with or without a concurrent control group. Two authors assessed the studies for inclusion, and appraised the quality of the included studies. As a limited number of studies were found, we analyzed the data using narrative synthesis. FINDINGS: The searches retrieved 2229 titles, of which 221 full-text studies were assessed. We found no studies using an RCT design. Only four original articles (from the US and Taiwan) were included: two studies within the detection category, one in the response category, one under the detection and response categories, and no studies under the prevention category. The findings suggest that data-mining may improve fraud detection, and legal interventions as well as investment in anti-fraud activities may reduce fraud. DISCUSSION: Our analysis shows a lack of evidence of effect of the interventions to combat health care fraud. Further studies using robust research methodologies are required in all aspects of dealing with health care fraud and abuse, assessing the effectiveness and cost-effectiveness of methods to prevent, detect, and respond to fraud in health care.", Cost-Benefit Analysis; *Fraud,-2,,
429,Welch,2013,Journal of the American Medical Informatics Association : JAMIA,Clinical decision support for genetically guided personalized medicine: a systematic review,"OBJECTIVE: To review the literature on clinical decision support (CDS) for genetically guided personalized medicine (GPM). MATERIALS AND METHODS: MEDLINE and Embase were searched from 1990 to 2011. The manuscripts included were summarized, and notable themes and trends were identified. RESULTS: Following a screening of 3416 articles, 38 primary research articles were identified. Focal areas of research included family history-driven CDS, cancer management, and pharmacogenomics. Nine randomized controlled trials of CDS interventions for GPM were identified, seven of which reported positive results. The majority of manuscripts were published on or after 2007, with increased recent focus on genotype-driven CDS and the integration of CDS within primary clinical information systems. DISCUSSION: Substantial research has been conducted to date on the use of CDS to enable GPM. In a previous analysis of CDS intervention trials, the automatic provision of CDS as a part of routine clinical workflow had been identified as being critical for CDS effectiveness. There was some indication that CDS for GPM could potentially be effective without the CDS being provided automatically, but we did not find conclusive evidence to support this hypothesis. CONCLUSION: To maximize the clinical benefits arising from ongoing discoveries in genetics and genomics, additional research and development is recommended for identifying how best to leverage CDS to bridge the gap between the promise and realization of GPM."," Bibliometrics; Biomedical Research/trends; *Decision Support Systems, Clinical/trends; Humans; Neoplasms/therapy; Pharmacogenetics; *Precision Medicine/trends",-2,,
430,Botsis,2012,Journal of the American Medical Informatics Association : JAMIA,Vaccine adverse event text mining system for extracting features from vaccine safety reports,"OBJECTIVE: To develop and evaluate a text mining system for extracting key clinical features from vaccine adverse event reporting system (VAERS) narratives to aid in the automated review of adverse event reports. DESIGN: Based upon clinical significance to VAERS reviewing physicians, we defined the primary (diagnosis and cause of death) and secondary features (eg, symptoms) for extraction. We built a novel vaccine adverse event text mining (VaeTM) system based on a semantic text mining strategy. The performance of VaeTM was evaluated using a total of 300 VAERS reports in three sequential evaluations of 100 reports each. Moreover, we evaluated the VaeTM contribution to case classification; an information retrieval-based approach was used for the identification of anaphylaxis cases in a set of reports and was compared with two other methods: a dedicated text classifier and an online tool. MEASUREMENTS: The performance metrics of VaeTM were text mining metrics: recall, precision and F-measure. We also conducted a qualitative difference analysis and calculated sensitivity and specificity for classification of anaphylaxis cases based on the above three approaches. RESULTS: VaeTM performed best in extracting diagnosis, second level diagnosis, drug, vaccine, and lot number features (lenient F-measure in the third evaluation: 0.897, 0.817, 0.858, 0.874, and 0.914, respectively). In terms of case classification, high sensitivity was achieved (83.1%); this was equal and better compared to the text classifier (83.1%) and the online tool (40.7%), respectively. CONCLUSION: Our VaeTM implementation of a semantic text mining strategy shows promise in providing accurate and efficient extraction of key features from VAERS narratives.", *Adverse Drug Reaction Reporting Systems; Data Mining/*methods; Humans; *Natural Language Processing; Semantics; United States; Vaccines/*adverse effects,-1,,
431,Tusch,2012,Studies in health technology and informatics,Translational meta-analysis tool for temporal gene expression profiles,"Widespread use of microarray technology that led to highly complex datasets often is addressing similar or related biological questions. In translational medicine research is often based on measurements that have been obtained at different points in time. However, the researcher looks at them as a progression over time. If a biological stimulus shows an effect on a particular gene that is reversed over time, this would show, for instance, as a peak in the gene's temporal expression profile. Our program SPOT helps researchers find these patterns in large sets of microarray data. We created the software tool using open-source platforms and the Semantic Web tool Protege-OWL."," Artificial Intelligence; Data Mining/*methods; Database Management Systems; *Databases, Genetic; Gene Expression Profiling/*methods; Medical Record Linkage/*methods; Meta-Analysis as Topic; Oligonucleotide Array Sequence Analysis/*methods; *Software; Translational Medical Research/*methods",-2,,
432,Doupi,2012,Studies in health technology and informatics,Using EHR data for monitoring and promoting patient safety: reviewing the evidence on trigger tools,"Trigger tools, both paper and automated, have been viewed as a promising technology for patient record content analysis and identification of patient safety adverse events. The requirements and potential barriers for implementation of each line of tools have been explored by means of a literature review focusing on two interconnected subject areas: the Institute of Healthcare Improvement's paper-based Global Trigger Tool, which is currently taken up by several national level patient safety programs, and automated trigger tools, because of their increased feasibility as electronic health record (EHR) adoption grows. This paper provides an overview of the existing evidence on the strengths and weaknesses of each approach, and discusses the implications of the findings from the perspectives of healthcare organizations' management and staff, and from the viewpoint of demands on EHR systems."," Data Mining/*methods; *Electronic Health Records; *Evidence-Based Medicine; Health Promotion/methods; *Health Records, Personal; Medical Errors/*prevention & control; *Patient Safety; Safety Management/*methods",-2,,
433,Karakulah,2012,Studies in health technology and informatics,A data-driven living review for pharmacogenomic decision support in cancer treatment,"With drastically decreasing costs of genetic sequencing, it has become feasible to use individual genetic markers to optimize treatment selection in cancer therapy. However, it is still difficult for medical practitioners to integrate these new kinds of data into clinical routine, since available information is growing rapidly. We demonstrate how a blend of manual curation and automated data extraction and evidence synthesis can be used to generate a 'living review', a summarization of current evidence on cancer classification, corresponding genetic markers, genetic tests and treatment options that can be used by clinicians to refine treatment choices. In contrast to a classical review, this automated 'living review' offers the opportunity of automatically updating core content when available data changes, making it easier to keep an overview of the best current evidence. We discuss some of the findings we made while creating a prototype of a 'living review' for colorectal cancer pharmacotherapy."," Data Mining/*methods; *Decision Support Systems, Clinical; Drug Therapy, Computer-Assisted/methods; *Electronic Health Records; Genetic Testing/methods; *Health Records, Personal; Humans; Neoplasms/diagnosis/*drug therapy/*genetics; Pharmacogenetics/*methods",2,,
434,Mets,2012,PloS one,Visual versus automated evaluation of chest computed tomography for the presence of chronic obstructive pulmonary disease,"BACKGROUND: Incidental CT findings may provide an opportunity for early detection of chronic obstructive pulmonary disease (COPD), which may prove important in CT-based lung cancer screening setting. We aimed to determine the diagnostic performance of human observers to visually evaluate COPD presence on CT images, in comparison to automated evaluation using quantitative CT measures. METHODS: This study was approved by the Dutch Ministry of Health and the institutional review board. All participants provided written informed consent. We studied 266 heavy smokers enrolled in a lung cancer screening trial. All subjects underwent volumetric inspiratory and expiratory chest computed tomography (CT). Pulmonary function testing was used as the reference standard for COPD. We evaluated the diagnostic performance of eight observers and one automated model based on quantitative CT measures. RESULTS: The prevalence of COPD in the study population was 44% (118/266), of whom 62% (73/118) had mild disease. The diagnostic accuracy was 74.1% in the automated evaluation, and ranged between 58.3% and 74.3% for the visual evaluation of CT images. The positive predictive value was 74.3% in the automated evaluation, and ranged between 52.9% and 74.7% for the visual evaluation. Interobserver variation was substantial, even within the subgroup of experienced observers. Agreement within observers yielded kappa values between 0.28 and 0.68, regardless of the level of expertise. The agreement between the observers and the automated CT model showed kappa values of 0.12-0.35. CONCLUSIONS: Visual evaluation of COPD presence on chest CT images provides at best modest accuracy and is associated with substantial interobserver variation. Automated evaluation of COPD subjects using quantitative CT measures appears superior to visual evaluation by human observers."," Aged; Automation; Humans; Image Interpretation, Computer-Assisted/*methods; Male; Middle Aged; Observer Variation; Pulmonary Disease, Chronic Obstructive/*diagnostic imaging; Radiography, Thoracic/*methods; Tomography, X-Ray Computed/*methods",-2,,
435,Strauss,2013,Journal of the American Medical Informatics Association : JAMIA,Identifying primary and recurrent cancers using a SAS-based natural language processing algorithm,"OBJECTIVE: Significant limitations exist in the timely and complete identification of primary and recurrent cancers for clinical and epidemiologic research. A SAS-based coding, extraction, and nomenclature tool (SCENT) was developed to address this problem. MATERIALS AND METHODS: SCENT employs hierarchical classification rules to identify and extract information from electronic pathology reports. Reports are analyzed and coded using a dictionary of clinical concepts and associated SNOMED codes. To assess the accuracy of SCENT, validation was conducted using manual review of pathology reports from a random sample of 400 breast and 400 prostate cancer patients diagnosed at Kaiser Permanente Southern California. Trained abstractors classified the malignancy status of each report. RESULTS: Classifications of SCENT were highly concordant with those of abstractors, achieving kappa of 0.96 and 0.95 in the breast and prostate cancer groups, respectively. SCENT identified 51 of 54 new primary and 60 of 61 recurrent cancer cases across both groups, with only three false positives in 792 true benign cases. Measures of sensitivity, specificity, positive predictive value, and negative predictive value exceeded 94% in both cancer groups. DISCUSSION: Favorable validation results suggest that SCENT can be used to identify, extract, and code information from pathology report text. Consequently, SCENT has wide applicability in research and clinical care. Further assessment will be needed to validate performance with other clinical text sources, particularly those with greater linguistic variability. CONCLUSION: SCENT is proof of concept for SAS-based natural language processing applications that can be easily shared between institutions and used to support clinical and epidemiologic research.", Algorithms; Biomedical Research; Breast Neoplasms/epidemiology/pathology; California/epidemiology; *Data Mining; *Electronic Health Records; Female; Humans; Information Dissemination; Male; *Natural Language Processing; Neoplasms/*epidemiology/pathology; Prostatic Neoplasms/epidemiology/pathology; Recurrence; Reproducibility of Results; Sensitivity and Specificity; Systematized Nomenclature of Medicine,-2,,
436,Joffe,2012,Journal of the American Medical Informatics Association : JAMIA,Collaborative knowledge acquisition for the design of context-aware alert systems,"OBJECTIVE: To present a framework for combining implicit knowledge acquisition from multiple experts with machine learning and to evaluate this framework in the context of anemia alerts. MATERIALS AND METHODS: Five internal medicine residents reviewed 18 anemia alerts, while 'talking aloud'. They identified features that were reviewed by two or more physicians to determine appropriate alert level, etiology and treatment recommendation. Based on these features, data were extracted from 100 randomly-selected anemia cases for a training set and an additional 82 cases for a test set. Two staff internists assigned an alert level, etiology and treatment recommendation before and after reviewing the entire electronic medical record. The training set of 118 cases (100 plus 18) and the test set of 82 cases were explored using RIDOR and JRip algorithms. RESULTS: The feature set was sufficient to assess 93% of anemia cases (intraclass correlation for alert level before and after review of the records by internists 1 and 2 were 0.92 and 0.95, respectively). High-precision classifiers were constructed to identify low-level alerts (precision p=0.87, recall R=0.4), iron deficiency (p=1.0, R=0.73), and anemia associated with kidney disease (p=0.87, R=0.77). DISCUSSION: It was possible to identify low-level alerts and several conditions commonly associated with chronic anemia. This approach may reduce the number of clinically unimportant alerts. The study was limited to anemia alerts. Furthermore, clinicians were aware of the study hypotheses potentially biasing their evaluation. CONCLUSION: Implicit knowledge acquisition, collaborative filtering and machine learning were combined automatically to induce clinically meaningful and precise decision rules."," Anemia/*prevention & control; *Artificial Intelligence; *Decision Support Systems, Clinical; *Diagnosis, Computer-Assisted; Electronic Health Records; Humans; Internal Medicine; Israel; Practice Patterns, Physicians'",-2,,
437,Kennedy,2012,Journal of medical Internet research,Active assistance technology for health-related behavior change: an interdisciplinary review,"BACKGROUND: Information technology can help individuals to change their health behaviors. This is due to its potential for dynamic and unbiased information processing enabling users to monitor their own progress and be informed about risks and opportunities specific to evolving contexts and motivations. However, in many behavior change interventions, information technology is underused by treating it as a passive medium focused on efficient transmission of information and a positive user experience. OBJECTIVE: To conduct an interdisciplinary literature review to determine the extent to which the active technological capabilities of dynamic and adaptive information processing are being applied in behavior change interventions and to identify their role in these interventions. METHODS: We defined key categories of active technology such as semantic information processing, pattern recognition, and adaptation. We conducted the literature search using keywords derived from the categories and included studies that indicated a significant role for an active technology in health-related behavior change. In the data extraction, we looked specifically for the following technology roles: (1) dynamic adaptive tailoring of messages depending on context, (2) interactive education, (3) support for client self-monitoring of behavior change progress, and (4) novel ways in which interventions are grounded in behavior change theories using active technology. RESULTS: The search returned 228 potentially relevant articles, of which 41 satisfied the inclusion criteria. We found that significant research was focused on dialog systems, embodied conversational agents, and activity recognition. The most covered health topic was physical activity. The majority of the studies were early-stage research. Only 6 were randomized controlled trials, of which 4 were positive for behavior change and 5 were positive for acceptability. Empathy and relational behavior were significant research themes in dialog systems for behavior change, with many pilot studies showing a preference for those features. We found few studies that focused on interactive education (3 studies) and self-monitoring (2 studies). Some recent research is emerging in dynamic tailoring (15 studies) and theoretically grounded ontologies for automated semantic processing (4 studies). CONCLUSIONS: The potential capabilities and risks of active assistance technologies are not being fully explored in most current behavior change research. Designers of health behavior interventions need to consider the relevant informatics methods and algorithms more fully. There is also a need to analyze the possibilities that can result from interaction between different technology components. This requires deep interdisciplinary collaboration, for example, between health psychology, computer science, health informatics, cognitive science, and educational methodology.", *Behavior Therapy; *Health Behavior; Humans; *Self-Help Devices,-2,,
438,Reeves,2013,International journal of medical informatics,Detecting temporal expressions in medical narratives,"BACKGROUND: Clinical practice and epidemiological information aggregation require knowing when, how long, and in what sequence medically relevant events occur. The Temporal Awareness and Reasoning Systems for Question Interpretation (TARSQI) Toolkit (TTK) is a complete, open source software package for the temporal ordering of events within narrative text documents. TTK was developed on newspaper articles. We extended TTK to support medical notes using veterans' affairs (VA) clinical notes and compared it to TTK. METHODS: We used a development set consisting of 200 VA clinical notes to modify and append rules to TTK's time tagger, creating Med-TTK. We then evaluated the performances of TTK and Med-TTK on an independent random selection of 100 clinical notes. Evaluation tasks were to identify and classify time-referring expressions as one of four temporal classes (DATE, TIME, DURATION, and SET). The reference standard for this test set was generated by dual human manual review with disagreements resolved by a third reviewer. Outcome measures included recall and precision for each class, and inter-rater agreement scores. RESULTS: There were 3146 temporal expressions in the reference standard. TTK identified 1595 temporal expressions. Recall was 0.15 (95% confidence interval [CI] 0.12-0.15) and precision was 0.27 (95% CI 0.25-0.29) for TTK. Med-TTK identified 3174 expressions. Recall was 0.86 (95% CI 0.84-0.87) and precision was 0.85 (95% CI 0.84-0.86) for Med-TTK. CONCLUSION: The algorithms for identifying and classifying temporal expressions in medical narratives developed within Med-TTK significantly improved performance compared to TTK. Natural language processing applications such as Med-TTK provide a foundation for meaningful longitudinal mapping of patient history events among electronic health records. The tool can be accessed at the following site: http://code.google.com/p/med-ttk/."," Electronic Health Records/*statistics & numerical data; *Health Records, Personal; *Narration; *Natural Language Processing; Pattern Recognition, Automated/*methods; Software; *Time Factors; United States; *Vocabulary, Controlled",-2,,
439,McCoy,2012,Journal of the American Medical Informatics Association : JAMIA,Development and evaluation of a crowdsourcing methodology for knowledge base construction: identifying relationships between clinical problems and medications,"OBJECTIVE: We describe a novel, crowdsourcing method for generating a knowledge base of problem-medication pairs that takes advantage of manually asserted links between medications and problems. METHODS: Through iterative review, we developed metrics to estimate the appropriateness of manually entered problem-medication links for inclusion in a knowledge base that can be used to infer previously unasserted links between problems and medications. RESULTS: Clinicians manually linked 231,223 medications (55.30% of prescribed medications) to problems within the electronic health record, generating 41,203 distinct problem-medication pairs, although not all were accurate. We developed methods to evaluate the accuracy of the pairs, and after limiting the pairs to those meeting an estimated 95% appropriateness threshold, 11,166 pairs remained. The pairs in the knowledge base accounted for 183,127 total links asserted (76.47% of all links). Retrospective application of the knowledge base linked 68,316 medications not previously linked by a clinician to an indicated problem (36.53% of unlinked medications). Expert review of the combined knowledge base, including inferred and manually linked problem-medication pairs, found a sensitivity of 65.8% and a specificity of 97.9%. CONCLUSION: Crowdsourcing is an effective, inexpensive method for generating a knowledge base of problem-medication pairs that is automatically mapped to local terminologies, up-to-date, and reflective of local prescribing practices and trends."," Adolescent; Adult; Child; *Crowdsourcing; *Drug Therapy, Computer-Assisted; Electronic Health Records; Humans; *Knowledge Bases; *Medical Records, Problem-Oriented; Retrospective Studies; Sensitivity and Specificity; Texas",-2,,
440,Walia,2012,BMC bioinformatics,Protein-RNA interface residue prediction using machine learning: an assessment of the state of the art,"BACKGROUND: RNA molecules play diverse functional and structural roles in cells. They function as messengers for transferring genetic information from DNA to proteins, as the primary genetic material in many viruses, as catalysts (ribozymes) important for protein synthesis and RNA processing, and as essential and ubiquitous regulators of gene expression in living organisms. Many of these functions depend on precisely orchestrated interactions between RNA molecules and specific proteins in cells. Understanding the molecular mechanisms by which proteins recognize and bind RNA is essential for comprehending the functional implications of these interactions, but the recognition 'code' that mediates interactions between proteins and RNA is not yet understood. Success in deciphering this code would dramatically impact the development of new therapeutic strategies for intervening in devastating diseases such as AIDS and cancer. Because of the high cost of experimental determination of protein-RNA interfaces, there is an increasing reliance on statistical machine learning methods for training predictors of RNA-binding residues in proteins. However, because of differences in the choice of datasets, performance measures, and data representations used, it has been difficult to obtain an accurate assessment of the current state of the art in protein-RNA interface prediction. RESULTS: We provide a review of published approaches for predicting RNA-binding residues in proteins and a systematic comparison and critical assessment of protein-RNA interface residue predictors trained using these approaches on three carefully curated non-redundant datasets. We directly compare two widely used machine learning algorithms (Naive Bayes (NB) and Support Vector Machine (SVM)) using three different data representations in which features are encoded using either sequence- or structure-based windows. Our results show that (i) Sequence-based classifiers that use a position-specific scoring matrix (PSSM)-based representation (PSSMSeq) outperform those that use an amino acid identity based representation (IDSeq) or a smoothed PSSM (SmoPSSMSeq); (ii) Structure-based classifiers that use smoothed PSSM representation (SmoPSSMStr) outperform those that use PSSM (PSSMStr) as well as sequence identity based representation (IDStr). PSSMSeq classifiers, when tested on an independent test set of 44 proteins, achieve performance that is comparable to that of three state-of-the-art structure-based predictors (including those that exploit geometric features) in terms of Matthews Correlation Coefficient (MCC), although the structure-based methods achieve substantially higher Specificity (albeit at the expense of Sensitivity) compared to sequence-based methods. We also find that the expected performance of the classifiers on a residue level can be markedly different from that on a protein level. Our experiments show that the classifiers trained on three different non-redundant protein-RNA interface datasets achieve comparable cross-validation performance. However, we find that the results are significantly affected by differences in the distance threshold used to define interface residues. CONCLUSIONS: Our results demonstrate that protein-RNA interface residue predictors that use a PSSM-based encoding of sequence windows outperform classifiers that use other encodings of sequence windows. While structure-based methods that exploit geometric features can yield significant increases in the Specificity of protein-RNA interface residue predictions, such increases are offset by decreases in Sensitivity. These results underscore the importance of comparing alternative methods using rigorous statistical procedures, multiple performance measures, and datasets that are constructed based on several alternative definitions of interface residues and redundancy cutoffs as well as including evaluations on independent test sets into the comparisons."," Algorithms; Amino Acids/chemistry; *Artificial Intelligence; Bayes Theorem; Humans; Position-Specific Scoring Matrices; Protein Conformation; RNA/*chemistry/metabolism; RNA-Binding Proteins/*chemistry/metabolism; Sequence Analysis, Protein; Support Vector Machine",-2,,
441,Vervloet,2012,Journal of the American Medical Informatics Association : JAMIA,The effectiveness of interventions using electronic reminders to improve adherence to chronic medication: a systematic review of the literature,"BACKGROUND: Many patients experience difficulties in adhering to long-term treatment. Although patients' reasons for not being adherent are diverse, one of the most commonly reported barriers is forgetfulness. Reminding patients to take their medication may provide a solution. Electronic reminders (automatically sent reminders without personal contact between the healthcare provider and patient) are now increasingly being used in the effort to improve adherence. OBJECTIVE: To examine the effectiveness of interventions using electronic reminders in improving patients' adherence to chronic medication. METHODS: A comprehensive literature search was conducted in PubMed, Embase, PsycINFO, CINAHL and Cochrane Central Register of Controlled Trials. Electronic searches were supplemented by manual searching of reference lists and reviews. Two reviewers independently screened all citations. Full text was obtained from selected citations and screened for final inclusion. The methodological quality of studies was assessed. RESULTS: Thirteen studies met the inclusion criteria. Four studies evaluated short message service (SMS) reminders, seven audiovisual reminders from electronic reminder devices (ERD), and two pager messages. Best evidence synthesis revealed evidence for the effectiveness of electronic reminders, provided by eight (four high, four low quality) studies showing significant effects on patients' adherence, seven of which measured short-term effects (follow-up period <6 months). Improved adherence was found in all but one study using SMS reminders, four studies using ERD and one pager intervention. In addition, one high quality study using an ERD found subgroup effects. CONCLUSION: This review provides evidence for the short-term effectiveness of electronic reminders, especially SMS reminders. However, long-term effects remain unclear.", Audiovisual Aids; Automation; Cell Phone; Humans; *Medication Adherence; *Reminder Systems; Telemedicine; *Text Messaging,-2,,
442,Korhonen,2012,PloS one,Text mining for literature review and knowledge discovery in cancer risk assessment and research,"Research in biomedical text mining is starting to produce technology which can make information in biomedical literature more accessible for bio-scientists. One of the current challenges is to integrate and refine this technology to support real-life scientific tasks in biomedicine, and to evaluate its usefulness in the context of such tasks. We describe CRAB - a fully integrated text mining tool designed to support chemical health risk assessment. This task is complex and time-consuming, requiring a thorough review of existing scientific data on a particular chemical. Covering human, animal, cellular and other mechanistic data from various fields of biomedicine, this is highly varied and therefore difficult to harvest from literature databases via manual means. Our tool automates the process by extracting relevant scientific data in published literature and classifying it according to multiple qualitative dimensions. Developed in close collaboration with risk assessors, the tool allows navigating the classified dataset in various ways and sharing the data with other users. We present a direct and user-based evaluation which shows that the technology integrated in the tool is highly accurate, and report a number of case studies which demonstrate how the tool can be used to support scientific discovery in cancer risk assessment and research. Our work demonstrates the usefulness of a text mining pipeline in facilitating complex research tasks in biomedicine. We discuss further development and application of our technology to other types of chemical risk assessment in the future.", Animals; Biomedical Research/*methods; Carcinogens/*chemistry; Data Mining/*methods; Humans; Neoplasms/*chemically induced; Risk Assessment; *Software,1,,
443,Shublaq,2012,Studies in health technology and informatics,Merging genomic and phenomic data for research and clinical impact,"Driven primarily by advances in genomics, pharmacogenomics and systems biology technologies, large amounts of genomic and phenomic data are today being collected on individuals worldwide. Integrative analysis, mining, and computer modeling of these data, facilitated by information technology, have led to the development of predictive, preventive, and personalized medicine. This transformative approach holds the potential inter alia to enable future general practitioners and physicians to prescribe the right drug to the right patient at the right dosage. For such patient-specific medicine to be adopted as standard clinical practice, publicly accumulated knowledge of genes, proteins, molecular functional annotations, and interactions need to be unified and with electronic health records including phenotypic information, most of which still reside as paper-based records in hospitals. We review the state-of-the-art in terms of electronic data capture and medical data standards. Some of these activities are drawn from research projects currently being performed within the European Virtual Physiological Human (VPH) initiative; all are being monitored by the VPH INBIOMEDvision Consortium. Various ethical, legal and societal issues linked with privacy will increasingly arise in the post-genomic era. This will require a closer interaction between the bioinformatics/systems biology and medical informatics/healthcare communities. Planning for how individuals will own their personal health records is urgently needed, as the cost of sequencing a whole human genome will soon be less than U.S. $100. We discuss some of the issues that will need to be addressed by society as a result of this revolution in healthcare."," Computational Biology/*organization & administration; Data Mining/methods; Electronic Health Records/organization & administration; Genome, Human; Genomics/*organization & administration; Humans; *Medical Informatics Applications; Phenotype; Precision Medicine/methods",-2,,
444,Garvin,2012,Journal of the American Medical Informatics Association : JAMIA,Automated extraction of ejection fraction for quality measurement using regular expressions in Unstructured Information Management Architecture (UIMA) for heart failure,"OBJECTIVES: Left ventricular ejection fraction (EF) is a key component of heart failure quality measures used within the Department of Veteran Affairs (VA). Our goals were to build a natural language processing system to extract the EF from free-text echocardiogram reports to automate measurement reporting and to validate the accuracy of the system using a comparison reference standard developed through human review. This project was a Translational Use Case Project within the VA Consortium for Healthcare Informatics. MATERIALS AND METHODS: We created a set of regular expressions and rules to capture the EF using a random sample of 765 echocardiograms from seven VA medical centers. The documents were randomly assigned to two sets: a set of 275 used for training and a second set of 490 used for testing and validation. To establish the reference standard, two independent reviewers annotated all documents in both sets; a third reviewer adjudicated disagreements. RESULTS: System test results for document-level classification of EF of <40% had a sensitivity (recall) of 98.41%, a specificity of 100%, a positive predictive value (precision) of 100%, and an F measure of 99.2%. System test results at the concept level had a sensitivity of 88.9% (95% CI 87.7% to 90.0%), a positive predictive value of 95% (95% CI 94.2% to 95.9%), and an F measure of 91.9% (95% CI 91.2% to 92.7%). DISCUSSION: An EF value of <40% can be accurately identified in VA echocardiogram reports. CONCLUSIONS: An automated information extraction system can be used to accurately extract EF for quality measurement."," Data Mining/*methods; Echocardiography; *Heart Failure/diagnostic imaging/therapy; Humans; *Medical Records Systems, Computerized; *Natural Language Processing; *Quality Indicators, Health Care; Reference Standards; Software Validation; *Stroke Volume; United States; United States Department of Veterans Affairs",-2,,
445,Uzuner,2012,Journal of the American Medical Informatics Association : JAMIA,Evaluating the state of the art in coreference resolution for electronic medical records,"BACKGROUND: The fifth i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records conducted a systematic review on resolution of noun phrase coreference in medical records. Informatics for Integrating Biology and the Bedside (i2b2) and the Veterans Affair (VA) Consortium for Healthcare Informatics Research (CHIR) partnered to organize the coreference challenge. They provided the research community with two corpora of medical records for the development and evaluation of the coreference resolution systems. These corpora contained various record types (ie, discharge summaries, pathology reports) from multiple institutions. METHODS: The coreference challenge provided the community with two annotated ground truth corpora and evaluated systems on coreference resolution in two ways: first, it evaluated systems for their ability to identify mentions of concepts and to link together those mentions. Second, it evaluated the ability of the systems to link together ground truth mentions that refer to the same entity. Twenty teams representing 29 organizations and nine countries participated in the coreference challenge. RESULTS: The teams' system submissions showed that machine-learning and rule-based approaches worked best when augmented with external knowledge sources and coreference clues extracted from document structure. The systems performed better in coreference resolution when provided with ground truth mentions. Overall, the systems struggled in solving coreference resolution for cases that required domain knowledge.", *Electronic Health Records; Humans; Information Storage and Retrieval/*methods; *Natural Language Processing,-2,,
446,Peissig,2012,Journal of the American Medical Informatics Association : JAMIA,Importance of multi-modal approaches to effectively identify cataract cases from electronic health records,"OBJECTIVE: There is increasing interest in using electronic health records (EHRs) to identify subjects for genomic association studies, due in part to the availability of large amounts of clinical data and the expected cost efficiencies of subject identification. We describe the construction and validation of an EHR-based algorithm to identify subjects with age-related cataracts. MATERIALS AND METHODS: We used a multi-modal strategy consisting of structured database querying, natural language processing on free-text documents, and optical character recognition on scanned clinical images to identify cataract subjects and related cataract attributes. Extensive validation on 3657 subjects compared the multi-modal results to manual chart review. The algorithm was also implemented at participating electronic MEdical Records and GEnomics (eMERGE) institutions. RESULTS: An EHR-based cataract phenotyping algorithm was successfully developed and validated, resulting in positive predictive values (PPVs) >95%. The multi-modal approach increased the identification of cataract subject attributes by a factor of three compared to single-mode approaches while maintaining high PPV. Components of the cataract algorithm were successfully deployed at three other institutions with similar accuracy. DISCUSSION: A multi-modal strategy incorporating optical character recognition and natural language processing may increase the number of cases identified while maintaining similar PPVs. Such algorithms, however, require that the needed information be embedded within clinical documents. CONCLUSION: We have demonstrated that algorithms to identify and characterize cataracts can be developed utilizing data collected via the EHR. These algorithms provide a high level of accuracy even when implemented across multiple EHRs and institutional boundaries."," Adult; *Cataract; Databases, Factual; *Electronic Health Records; Humans; Information Storage and Retrieval/*methods; *Natural Language Processing; Phenotype",-2,,
447,Wang,2012,PloS one,Extracting diagnoses and investigation results from unstructured text in electronic health records by semi-supervised machine learning,"BACKGROUND: Electronic health records are invaluable for medical research, but much of the information is recorded as unstructured free text which is time-consuming to review manually. AIM: To develop an algorithm to identify relevant free texts automatically based on labelled examples. METHODS: We developed a novel machine learning algorithm, the 'Semi-supervised Set Covering Machine' (S3CM), and tested its ability to detect the presence of coronary angiogram results and ovarian cancer diagnoses in free text in the General Practice Research Database. For training the algorithm, we used texts classified as positive and negative according to their associated Read diagnostic codes, rather than by manual annotation. We evaluated the precision (positive predictive value) and recall (sensitivity) of S3CM in classifying unlabelled texts against the gold standard of manual review. We compared the performance of S3CM with the Transductive Vector Support Machine (TVSM), the original fully-supervised Set Covering Machine (SCM) and our 'Freetext Matching Algorithm' natural language processor. RESULTS: Only 60% of texts with Read codes for angiogram actually contained angiogram results. However, the S3CM algorithm achieved 87% recall with 64% precision on detecting coronary angiogram results, outperforming the fully-supervised SCM (recall 78%, precision 60%) and TSVM (recall 2%, precision 3%). For ovarian cancer diagnoses, S3CM had higher recall than the other algorithms tested (86%). The Freetext Matching Algorithm had better precision than S3CM (85% versus 74%) but lower recall (62%). CONCLUSIONS: Our novel S3CM machine learning algorithm effectively detected free texts in primary care records associated with angiogram results and ovarian cancer diagnoses, after training on pre-classified test sets. It should be easy to adapt to other disease areas as it does not rely on linguistic rules, but needs further testing in other electronic health record datasets.", *Algorithms; *Artificial Intelligence; *Electronic Health Records; Female; Humans; Male; Ovarian Neoplasms/diagnosis,-1,,
448,Parsons,2012,Journal of the American Medical Informatics Association : JAMIA,Validity of electronic health record-derived quality measurement for performance monitoring,"BACKGROUND: Since 2007, New York City's primary care information project has assisted over 3000 providers to adopt and use a prevention-oriented electronic health record (EHR). Participating practices were taught to re-adjust their workflows to use the EHR built-in population health monitoring tools, including automated quality measures, patient registries and a clinical decision support system. Practices received a comprehensive suite of technical assistance, which included quality improvement, EHR customization and configuration, privacy and security training, and revenue cycle optimization. These services were aimed at helping providers understand how to use their EHR to track and improve the quality of care delivered to patients. MATERIALS AND METHODS: Retrospective electronic chart reviews of 4081 patient records across 57 practices were analyzed to determine the validity of EHR-derived quality measures and documented preventive services. RESULTS: Results from this study show that workflow and documentation habits have a profound impact on EHR-derived quality measures. Compared with the manual review of electronic charts, EHR-derived measures can undercount practice performance, with a disproportionately negative impact on the number of patients captured as receiving a clinical preventive service or meeting a recommended treatment goal. CONCLUSION: This study provides a cautionary note in using EHR-derived measurement for public reporting of provider performance or use for payment."," Adult; Benchmarking; Documentation; *Electronic Health Records; Female; Humans; *Information Storage and Retrieval; Male; Medical Audit; Middle Aged; New York City; Primary Health Care; Quality Assurance, Health Care/*methods/*statistics & numerical data; Reimbursement, Incentive; Reproducibility of Results; Retrospective Studies; Workflow",-2,,
449,AlGhamdi,2012,International journal of medical informatics,Internet use by the public to search for health-related information,"BACKGROUND: The use of the Internet to search for health-related information (HRI) has become a common practice worldwide. Our literature review failed to find any evidence of previous studies on this topic from Saudi Arabia. OBJECTIVE: To determine the public use of the Internet in Saudi Arabia to search for HRI and to evaluate patients' perceptions of the quality of the information available on the Internet compared to that provided by their health care providers. METHODS: A self-administered questionnaire about Internet use to search for HRI was distributed randomly to male and female outpatients and visitors attending a public University Hospital in Riyadh, Saudi Arabia from January to May 2010. A Chi-squared test was used to assess the association between different categorical variables. Multiple logistic regression was used to relate the use of the Internet to search for HRI with various socio-demographic variables. RESULTS: The questionnaire response was 80.1%, with completion of 801 of the 1000 distributed questionnaires; 50% (400/801) of respondents were males. The mean age of respondents was 32+/-11 years. The majority of respondents used the Internet in general (87.8%), and 58.4% of them (363/622) used the Internet to search for HRI. The majority stated a doctor was their primary source of HRI (89.3%, 654/732). This practice was considered useful by 84.2%, and the main reason behind it was sheer curiosity (92.7%, 418/451). Other reasons included not getting enough information from their doctor (58.5%, 227/413) and not trusting the information given by their doctor (28.2%, 101/443). Forty-four percent (205/466) searched for HRI before coming to the clinic; 72.5% of those discussed the information with their doctors and 71.7% (119/166) of those who did so believed that this positively affected their relationship with their doctor. Searching the Internet for health information was observed more frequently among the 30-39 year age group (OR=2.0, 95% CI 1.1-3.7), females (OR=3.8, 95% CI 2.3-6.4), individuals with university or higher education (OR=1.7, 95% CI 1.1-2.8), employed individuals (OR=2.7, 95% CI 1.4-4.9) and high income groups (OR=2.8, 95% CI 1.5-5.1). CONCLUSIONS: A proportion of the public searches the Internet to obtain HRI for various reasons, which could have consequences on their health and relationship with their doctors. Therefore, doctors should be aware of the health information available online to help guide patients to reliable websites. Health authorities should also be aware of the issue to offer regulations and solutions.", Adult; Age Distribution; Aged; Data Mining/*statistics & numerical data; Educational Status; Female; Health Education/*statistics & numerical data; Humans; Internet/*statistics & numerical data; Middle Aged; Saudi Arabia; Sex Distribution; *Surveys and Questionnaires; Young Adult,-2,,
450,Chubak,2012,Journal of clinical epidemiology,Tradeoffs between accuracy measures for electronic health care data algorithms,"OBJECTIVE: We review the uses of electronic health care data algorithms, measures of their accuracy, and reasons for prioritizing one measure of accuracy over another. STUDY DESIGN AND SETTING: We use real studies to illustrate the variety of uses of automated health care data in epidemiologic and health services research. Hypothetical examples show the impact of different types of misclassification when algorithms are used to ascertain exposure and outcome. RESULTS: High algorithm sensitivity is important for reducing the costs and burdens associated with the use of a more accurate measurement tool, for enhancing study inclusiveness, and for ascertaining common exposures. High specificity is important for classifying outcomes. High positive predictive value is important for identifying a cohort of persons with a condition of interest but that need not be representative of or include everyone with that condition. Finally, a high negative predictive value is important for reducing the likelihood that study subjects have an exclusionary condition. CONCLUSION: Epidemiologists must often prioritize one measure of accuracy over another when generating an algorithm for use in their study. We recommend researchers publish all tested algorithms-including those without acceptable accuracy levels-to help future studies refine and apply algorithms that are well suited to their objectives."," Aged; *Algorithms; Bias; Clinical Coding; Databases, Factual/standards/*statistics & numerical data; *Electronic Health Records; *Epidemiologic Methods; Female; Health Services Research/*statistics & numerical data; Humans; Sensitivity and Specificity",-2,,
451,Dozmorov,2011,BMC bioinformatics,High-throughput processing and normalization of one-color microarrays for transcriptional meta-analyses,"BACKGROUND: Microarray experiments are becoming increasingly common in biomedical research, as is their deposition in publicly accessible repositories, such as Gene Expression Omnibus (GEO). As such, there has been a surge in interest to use this microarray data for meta-analytic approaches, whether to increase sample size for a more powerful analysis of a specific disease (e.g. lung cancer) or to re-examine experiments for reasons different than those examined in the initial, publishing study that generated them. For the average biomedical researcher, there are a number of practical barriers to conducting such meta-analyses such as manually aggregating, filtering and formatting the data. Methods to automatically process large repositories of microarray data into a standardized, directly comparable format will enable easier and more reliable access to microarray data to conduct meta-analyses. METHODS: We present a straightforward, simple but robust against potential outliers method for automatic quality control and pre-processing of tens of thousands of single-channel microarray data files. GEO GDS files are quality checked by comparing parametric distributions and quantile normalized to enable direct comparison of expression level for subsequent meta-analyses. RESULTS: 13,000 human 1-color experiments were processed to create a single gene expression matrix that subsets can be extracted from to conduct meta-analyses. Interestingly, we found that when conducting a global meta-analysis of gene-gene co-expression patterns across all 13,000 experiments to predict gene function, normalization had minimal improvement over using the raw data. CONCLUSIONS: Normalization of microarray data appears to be of minimal importance on analyses based on co-expression patterns when the sample size is on the order of thousands microarray datasets. Smaller subsets, however, are more prone to aberrations and artefacts, and effective means of automating normalization procedures not only empowers meta-analytic approaches, but aids in reproducibility by providing a standard way of approaching the problem.Data availability: matrix containing normalized expression of 20,813 genes across 13,000 experiments is available for download at . Source code for GDS files pre-processing is available from the authors upon request."," Databases, Genetic; Gene Expression Profiling/*methods; Humans; Meta-Analysis as Topic; Oligonucleotide Array Sequence Analysis/*methods; Quality Control; Reproducibility of Results; Sample Size; *Software",-2,,
452,Arighi,2011,BMC bioinformatics,BioCreative III interactive task: an overview,"BACKGROUND: The BioCreative challenge evaluation is a community-wide effort for evaluating text mining and information extraction systems applied to the biological domain. The biocurator community, as an active user of biomedical literature, provides a diverse and engaged end user group for text mining tools. Earlier BioCreative challenges involved many text mining teams in developing basic capabilities relevant to biological curation, but they did not address the issues of system usage, insertion into the workflow and adoption by curators. Thus in BioCreative III (BC-III), the InterActive Task (IAT) was introduced to address the utility and usability of text mining tools for real-life biocuration tasks. To support the aims of the IAT in BC-III, involvement of both developers and end users was solicited, and the development of a user interface to address the tasks interactively was requested. RESULTS: A User Advisory Group (UAG) actively participated in the IAT design and assessment. The task focused on gene normalization (identifying gene mentions in the article and linking these genes to standard database identifiers), gene ranking based on the overall importance of each gene mentioned in the article, and gene-oriented document retrieval (identifying full text papers relevant to a selected gene). Six systems participated and all processed and displayed the same set of articles. The articles were selected based on content known to be problematic for curation, such as ambiguity of gene names, coverage of multiple genes and species, or introduction of a new gene name. Members of the UAG curated three articles for training and assessment purposes, and each member was assigned a system to review. A questionnaire related to the interface usability and task performance (as measured by precision and recall) was answered after systems were used to curate articles. Although the limited number of articles analyzed and users involved in the IAT experiment precluded rigorous quantitative analysis of the results, a qualitative analysis provided valuable insight into some of the problems encountered by users when using the systems. The overall assessment indicates that the system usability features appealed to most users, but the system performance was suboptimal (mainly due to low accuracy in gene normalization). Some of the issues included failure of species identification and gene name ambiguity in the gene normalization task leading to an extensive list of gene identifiers to review, which, in some cases, did not contain the relevant genes. The document retrieval suffered from the same shortfalls. The UAG favored achieving high performance (measured by precision and recall), but strongly recommended the addition of features that facilitate the identification of correct gene and its identifier, such as contextual information to assist in disambiguation. DISCUSSION: The IAT was an informative exercise that advanced the dialog between curators and developers and increased the appreciation of challenges faced by each group. A major conclusion was that the intended users should be actively involved in every phase of software development, and this will be strongly encouraged in future tasks. The IAT Task provides the first steps toward the definition of metrics and functional requirements that are necessary for designing a formal evaluation of interactive curation systems in the BioCreative IV challenge.", Animals; Computational Biology/methods; Data Mining/*methods; *Genes; Periodicals as Topic; Plants/genetics/metabolism,-2,,
453,Bontempi,2011,BMC bioinformatics,Multiple-input multiple-output causal strategies for gene selection,"BACKGROUND: Traditional strategies for selecting variables in high dimensional classification problems aim to find sets of maximally relevant variables able to explain the target variations. If these techniques may be effective in generalization accuracy they often do not reveal direct causes. The latter is essentially related to the fact that high correlation (or relevance) does not imply causation. In this study, we show how to efficiently incorporate causal information into gene selection by moving from a single-input single-output to a multiple-input multiple-output setting. RESULTS: We show in synthetic case study that a better prioritization of causal variables can be obtained by considering a relevance score which incorporates a causal term. In addition we show, in a meta-analysis study of six publicly available breast cancer microarray datasets, that the improvement occurs also in terms of accuracy. The biological interpretation of the results confirms the potential of a causal approach to gene selection. CONCLUSIONS: Integrating causal information into gene selection algorithms is effective both in terms of prediction accuracy and biological interpretation.", Algorithms; Bayes Theorem; Breast Neoplasms/genetics; Female; *Gene Expression Profiling; Humans; *Oligonucleotide Array Sequence Analysis; *Software,-2,,
454,Bobo,2011,BMC medical research methodology,Positive predictive value of automated database records for diabetic ketoacidosis (DKA) in children and youth exposed to antipsychotic drugs or control medications: a Tennessee Medicaid Study,"BACKGROUND: Diabetic ketoacidosis (DKA) is a potentially life-threatening complication of treatment with some atypical antipsychotic drugs in children and youth. Because drug-associated DKA is rare, large automated health outcomes databases may be a valuable data source for conducting pharmacoepidemiologic studies of DKA associated with exposure to individual antipsychotic drugs. However, no validated computer case definition of DKA exists. We sought to assess the positive predictive value (PPV) of a computer case definition to detect incident cases of DKA, using automated records of Tennessee Medicaid as the data source and medical record confirmation as a ""gold standard."" METHODS: The computer case definition of DKA was developed from a retrospective cohort study of antipsychotic-related type 2 diabetes mellitus (1996-2007) in Tennessee Medicaid enrollees, aged 6-24 years. Thirty potential cases with any DKA diagnosis (ICD-9 250.1, ICD-10 E1x.1) were identified from inpatient encounter claims. Medical records were reviewed to determine if they met the clinical definition of DKA. RESULTS: Of 30 potential cases, 27 (90%) were successfully abstracted and adjudicated. Of these, 24 cases were confirmed by medical record review (PPV 88.9%, 95% CI 71.9 to 96.1%). Three non-confirmed cases presented acutely with severe hyperglycemia, but had no evidence of acidosis. CONCLUSIONS: Diabetic ketoacidosis in children and youth can be identified in a computerized Medicaid database using our case definition, which could be useful for automated database studies in which drug-associated DKA is the outcome of interest."," Adolescent; Antipsychotic Agents/*adverse effects; Child; Diabetes Mellitus, Type 2/chemically induced/diagnosis; Diabetic Ketoacidosis/*chemically induced/diagnosis; Female; Humans; Male; Medicaid/organization & administration/standards/statistics & numerical data; Medical Records Systems, Computerized/organization &; administration/standards/*statistics & numerical data; Predictive Value of Tests; Reproducibility of Results; Retrospective Studies; Tennessee; United States; Young Adult",-2,,
455,Wang,2011,PloS one,Semantic processing disturbance in patients with schizophrenia: a meta-analysis of the N400 component,"BACKGROUND: Theoretically semantic processing can be separated into early automatic semantic activation and late contextualization. Semantic processing deficits have been suggested in patients with schizophrenia, however it is not clear which stage of semantic processing is impaired. We attempted to clarify this issue by conducting a meta-analysis of the N400 component. METHODS: Twenty-one studies met the inclusion criteria for the meta-analysis procedure. The Comprehensive Meta-Analysis software package was used to compute pooled effect sizes and homogeneity. RESULTS: Studies favoring early automatic activation produced a significant effect size of -0.41 for the N400 effect. Studies favoring late contextualization generated a significant effect size of -0.36 for the N400 effect, a significant effect size of -0.52 for N400 for congruent/related target words, and a significant effect size of 0.82 for the N400 peak latency. CONCLUSION: These findings suggest the automatic spreading activation process in patients with schizophrenia is very similar for closely related concepts and weakly or remotely related concepts, while late contextualization may be associated with impairments in processing semantically congruent context accompanied by slow processing speed.", Antipsychotic Agents/therapeutic use; Case-Control Studies; Evoked Potentials/*physiology; Humans; Reaction Time/physiology; Regression Analysis; Schizophrenia/drug therapy/*physiopathology; *Semantics,-2,,
456,Lluch,2011,International journal of medical informatics,Healthcare professionals' organisational barriers to health information technologies-a literature review,"OBJECTIVES: This literature review identifies and categorises, from an organisational management perspective, barriers to the use of HIT or ICT for health. Based on the review, it offers policy interventions. METHODS: This systematic literature review was carried out during December 2009 and January 2010. Additional on-going reviews of updates through automated system alerts took place up until this paper was submitted. A total of thirty-one sources were searched including nine software platforms/databases, fifteen specialised websites/targeted databases, Google Scholar, ISI Science Citation Index and five journals hand-searched. RESULTS: The study covers seventy-nine articles on organisational barriers to ICT adoption by healthcare professionals. These are categorised under five main headings - (I) Structure of healthcare organisations; (II) Tasks; (III) People policies; (IV) Incentives; and (V) Information and decision processes. A total of ten subcategories are also identified. By adopting an organisational management approach, some recommendations to remove organisational management barriers are made. CONCLUSIONS: Despite their apparent promise, health information technologies (HIT) have proved difficult to implement. This systematic review reveals the implementation barriers associated to organisational management and their interrelations. Several important future directions in the field are also suggested: (1) there is a need for further research providing evidence of HIT cost-effectiveness as well as the development of optimal HIT applications; (2) more information is needed regarding organisational change, incentives, liability issues, end-users HIT competences and skills, structure and work process issues involved in realising the benefits from HIT. Future policy interventions should consider the five dimensions identified when addressing the impact of HIT in healthcare organisational systems, and how the impact of an intervention aimed at a particular dimension would interrelate with others."," Computer Communication Networks/*organization & administration; Decision Support Systems, Clinical/*organization & administration; Health Personnel/*organization & administration/statistics & numerical data; Humans; Medical Informatics; Professional Practice/*organization & administration/statistics & numerical data",-2,,
457,Murphy,2011,Journal of the American Medical Informatics Association : JAMIA,Strategies for maintaining patient privacy in i2b2,"BACKGROUND: The re-use of patient data from electronic healthcare record systems can provide tremendous benefits for clinical research, but measures to protect patient privacy while utilizing these records have many challenges. Some of these challenges arise from a misperception that the problem should be solved technically when actually the problem needs a holistic solution. OBJECTIVE: The authors' experience with informatics for integrating biology and the bedside (i2b2) use cases indicates that the privacy of the patient should be considered on three fronts: technical de-identification of the data, trust in the researcher and the research, and the security of the underlying technical platforms. METHODS: The security structure of i2b2 is implemented based on consideration of all three fronts. It has been supported with several use cases across the USA, resulting in five privacy categories of users that serve to protect the data while supporting the use cases. RESULTS: The i2b2 architecture is designed to provide consistency and faithfully implement these user privacy categories. These privacy categories help reflect the policy of both the Health Insurance Portability and Accountability Act and the provisions of the National Research Act of 1974, as embodied by current institutional review boards. CONCLUSION: By implementing a holistic approach to patient privacy solutions, i2b2 is able to help close the gap between principle and practice.", Algorithms; *Artificial Intelligence; Computer Systems; *Confidentiality; Health Insurance Portability and Accountability Act; Humans; Information Storage and Retrieval; Systems Integration; Translational Medical Research/*organization & administration; United States,-2,,
458,Sewell,2012,Artificial intelligence in medicine,Static and dynamic pressure prediction for prosthetic socket fitting assessment utilising an inverse problem approach,"OBJECTIVE: It has been recognised in a review of the developments of lower-limb prosthetic socket fitting processes that the future demands new tools to aid in socket fitting. This paper presents the results of research to design and clinically test an artificial intelligence approach, specifically inverse problem analysis, for the determination of the pressures at the limb/prosthetic socket interface during stance and ambulation. METHODS: Inverse problem analysis is based on accurately calculating the external loads or boundary conditions that can generate a known amount of strain, stresses or displacements at pre-determined locations on a structure. In this study a backpropagation artificial neural network (ANN) is designed and validated to predict the interfacial pressures at the residual limb/socket interface from strain data collected from the socket surface. The subject of this investigation was a 45-year-old male unilateral trans-tibial (below-knee) traumatic amputee who had been using a prosthesis for 22 years. RESULTS: When comparing the ANN predicted interfacial pressure on 16 patches within the socket with actual pressures applied to the socket there is shown to be 8.7% difference, validating the methodology. Investigation of varying axial load through the subject's prosthesis, alignment of the subject's prosthesis, and pressure at the limb/socket interface during walking demonstrates that the validated ANN is able to give an accurate full-field study of the static and dynamic interfacial pressure distribution. CONCLUSIONS: To conclude, a methodology has been developed that enables a prosthetist to quantitatively analyse the distribution of pressures within the prosthetic socket in a clinical environment. This will aid in facilitating the ""right first time"" approach to socket fitting which will benefit both the patient in terms of comfort and the prosthetist, by reducing the time and associated costs of providing a high level of socket fit."," *Artificial Limbs; Biomechanical Phenomena; Computer Simulation; Humans; Male; Middle Aged; *Neural Networks (Computer); Pressure; Prosthesis Fitting/*methods; Stress, Mechanical; Therapy, Computer-Assisted/*methods; Tibia/surgery",-2,,
459,Buteneers,2011,Artificial intelligence in medicine,Automatic detection of epileptic seizures on the intra-cranial electroencephalogram of rats using reservoir computing,"INTRODUCTION: In this paper we propose a technique based on reservoir computing (RC) to mark epileptic seizures on the intra-cranial electroencephalogram (EEG) of rats. RC is a recurrent neural networks training technique which has been shown to possess good generalization properties with limited training. MATERIALS: The system is evaluated on data containing two different seizure types: absence seizures from genetic absence epilepsy rats from Strasbourg (GAERS) and tonic-clonic seizures from kainate-induced temporal-lobe epilepsy rats. The dataset consists of 452hours from 23 GAERS and 982hours from 15 kainate-induced temporal-lobe epilepsy rats. METHODS: During the preprocessing stage, several features are extracted from the EEG. A feature selection algorithm selects the best features, which are then presented as input to the RC-based classification algorithm. To classify the output of this algorithm a two-threshold technique is used. This technique is compared with other state-of-the-art techniques. RESULTS: A balanced error rate (BER) of 3.7% and 3.5% was achieved on the data from GAERS and kainate rats, respectively. This resulted in a sensitivity of 96% and 94% and a specificity of 96% and 99% respectively. The state-of-the-art technique for GAERS achieved a BER of 4%, whereas the best technique to detect tonic-clonic seizures achieved a BER of 16%. CONCLUSION: Our method outperforms up-to-date techniques and only a few parameters need to be optimized on a limited training set. It is therefore suited as an automatic aid for epilepsy researchers and is able to eliminate the tedious manual review and annotation of EEG."," Algorithms; Animals; Automation; Brain/*physiopathology; *Brain Waves; Disease Models, Animal; *Electroencephalography; Epilepsy, Absence/*diagnosis/genetics/physiopathology; Epilepsy, Tonic-Clonic/chemically induced/*diagnosis/physiopathology; Kainic Acid; Male; *Neural Networks (Computer); Pattern Recognition, Automated; Predictive Value of Tests; Rats; Rats, Wistar; Reproducibility of Results; Sensitivity and Specificity; *Signal Processing, Computer-Assisted; Time Factors",-2,,
460,Harkema,2011,Journal of the American Medical Informatics Association : JAMIA,Developing a natural language processing application for measuring the quality of colonoscopy procedures,"OBJECTIVE: The quality of colonoscopy procedures for colorectal cancer screening is often inadequate and varies widely among physicians. Routine measurement of quality is limited by the costs of manual review of free-text patient charts. Our goal was to develop a natural language processing (NLP) application to measure colonoscopy quality. MATERIALS AND METHODS: Using a set of quality measures published by physician specialty societies, we implemented an NLP engine that extracts 21 variables for 19 quality measures from free-text colonoscopy and pathology reports. We evaluated the performance of the NLP engine on a test set of 453 colonoscopy reports and 226 pathology reports, considering accuracy in extracting the values of the target variables from text, and the reliability of the outcomes of the quality measures as computed from the NLP-extracted information. RESULTS: The average accuracy of the NLP engine over all variables was 0.89 (range: 0.62-1.0) and the average F measure over all variables was 0.74 (range: 0.49-0.89). The average agreement score, measured as Cohen's kappa, between the manually established and NLP-derived outcomes of the quality measures was 0.62 (range: 0.09-0.86). DISCUSSION: For nine of the 19 colonoscopy quality measures, the agreement score was 0.70 or above, which we consider a sufficient score for the NLP-derived outcomes of these measures to be practically useful for quality measurement. CONCLUSION: The use of NLP for information extraction from free-text colonoscopy and pathology reports creates opportunities for large scale, routine quality measurement, which can support quality improvement in colonoscopy care.", Colonoscopy/*standards; Computer Systems; Data Mining; Humans; *Natural Language Processing; *Quality of Health Care,-1,,
461,Nguyen,2011,Studies in health technology and informatics,Automatic extraction of cancer characteristics from free-text pathology reports for cancer notifications,"OBJECTIVE: To develop a system for the automatic classification of Cancer Registry notifications data from free-text pathology reports. METHOD: The underlying technology used for the extraction of cancer notification items is based on the symbolic rule-based classification methodology, whereby formal semantics are used to reason with the systematised nomenclature of medicine - clinical terms (SNOMED CT) concepts identified in the free text. Business rules for cancer notifications used by Cancer Registry coding staff were also incorporated with the aim to mimic Cancer Registry processes. RESULTS: The system was developed on a corpus of 239 histology and cytology reports (with 60% notifiable reports), and then evaluated on an independent set of 300 reports (with 20% notifiable reports). Results show that the system can reliably classify notifiable reports with 96% and 100% specificity, and achieve an overall accuracy of 82% and 74% for classifying notification items from notifiable reports at a unit record level from the development and evaluation set, respectively. CONCLUSION: Cancer Registries collect a multitude of data that requires manual review, slowing down the flow of information. Extracting and providing an automatically coded cancer pathology notification for review can lessen the reliance on expert clinical staff, improving the efficiency and availability of cancer information.", Data Mining/*methods; *Disease Notification; Humans; Neoplasms/*pathology; Registries; Systematized Nomenclature of Medicine,-1,,
462,Grabar,2011,Studies in health technology and informatics,Terminology for the description of the diagnostic studies in the field of EBM,"Diagnostic systematic reviews is a relatively new area within the Evidence-Based Medicine (EBM). Their indexing in Pubmed is not precise, which complicates their detection when a systematic review is to be realized. In order to provide an assistance in the selection of relevant studies, we propose to develop a terminology describing this area and the organization of its terms. The terminology is built with a bottom-up approach. It contains 255 terms organized into five hierarchical levels. Only a small proportion of these terms (13%) are already registered in MeSH. This terminology will be exploited in a dedicated web service as a main tool for the detection of relevant diagnostic studies.", Abstracting and Indexing as Topic; Algorithms; Clinical Trials as Topic; Data Mining/methods; Evidence-Based Medicine/*methods; Humans; Language; Medical Subject Headings; Natural Language Processing; Neurosurgery/methods; PubMed; ROC Curve; Reproducibility of Results; *Terminology as Topic,1,,
463,Nadkarni,2011,Journal of the American Medical Informatics Association : JAMIA,Natural language processing: an introduction,"OBJECTIVES: To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design. TARGET AUDIENCE: This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the current state of the art. SCOPE: We describe the historical evolution of NLP, and summarize common NLP sub-problems in this extensive field. We then provide a synopsis of selected highlights of medical NLP efforts. After providing a brief description of common machine-learning approaches that are being used for diverse NLP sub-problems, we discuss how modern NLP architectures are designed, with a summary of the Apache Foundation's Unstructured Information Management Architecture. We finally consider possible future directions for NLP, and reflect on the possible impact of IBM Watson on the medical field."," Humans; Information Management; Information Storage and Retrieval; Medical Informatics/*trends; Models, Theoretical; *Natural Language Processing; Pattern Recognition, Automated; User-Computer Interface",-2,,
464,van Mourik,2011,PloS one,Automated detection of external ventricular and lumbar drain-related meningitis using laboratory and microbiology results and medication data,"OBJECTIVE: Monitoring of healthcare-associated infection rates is important for infection control and hospital benchmarking. However, manual surveillance is time-consuming and susceptible to error. The aim was, therefore, to develop a prediction model to retrospectively detect drain-related meningitis (DRM), a frequently occurring nosocomial infection, using routinely collected data from a clinical data warehouse. METHODS: As part of the hospital infection control program, all patients receiving an external ventricular (EVD) or lumbar drain (ELD) (2004 to 2009; n = 742) had been evaluated for the development of DRM through chart review and standardized diagnostic criteria by infection control staff; this was the reference standard. Children, patients dying <24 hours after drain insertion or with <1 day follow-up and patients with infection at the time of insertion or multiple simultaneous drains were excluded. Logistic regression was used to develop a model predicting the occurrence of DRM. Missing data were imputed using multiple imputation. Bootstrapping was applied to increase generalizability. RESULTS: 537 patients remained after application of exclusion criteria, of which 82 developed DRM (13.5/1000 days at risk). The automated model to detect DRM included the number of drains placed, drain type, blood leukocyte count, C-reactive protein, cerebrospinal fluid leukocyte count and culture result, number of antibiotics started during admission, and empiric antibiotic therapy. Discriminatory power of this model was excellent (area under the ROC curve 0.97). The model achieved 98.8% sensitivity (95% CI 88.0% to 99.9%) and specificity of 87.9% (84.6% to 90.8%). Positive and negative predictive values were 56.9% (50.8% to 67.9%) and 99.9% (98.6% to 99.9%), respectively. Predicted yearly infection rates concurred with observed infection rates. CONCLUSION: A prediction model based on multi-source data stored in a clinical data warehouse could accurately quantify rates of DRM. Automated detection using this statistical approach is feasible and could be applied to other nosocomial infections.", Aged; *Automation; *Cerebral Ventricles; Cross Infection/*diagnosis; Female; Hospital Administration; Humans; *Infection Control; *Lumbar Vertebrae; Male; Meningitis/*diagnosis/etiology; Middle Aged,-2,,
465,Augestad,2012,Journal of the American Medical Informatics Association : JAMIA,Standards for reporting randomized controlled trials in medical informatics: a systematic review of CONSORT adherence in RCTs on clinical decision support,"INTRODUCTION: The Consolidated Standards for Reporting Trials (CONSORT) were published to standardize reporting and improve the quality of clinical trials. The objective of this study is to assess CONSORT adherence in randomized clinical trials (RCT) of disease specific clinical decision support (CDS). METHODS: A systematic search was conducted of the Medline, EMBASE, and Cochrane databases. RCTs on CDS were assessed against CONSORT guidelines and the Jadad score. RESULT: 32 of 3784 papers identified in the primary search were included in the final review. 181 702 patients and 7315 physicians participated in the selected trials. Most trials were performed in primary care (22), including 897 general practitioner offices. RCTs assessing CDS for asthma (4), diabetes (4), and hyperlipidemia (3) were the most common. Thirteen CDS systems (40%) were implemented in electronic medical records, and 14 (43%) provided automatic alerts. CONSORT and Jadad scores were generally low; the mean CONSORT score was 30.75 (95% CI 27.0 to 34.5), median score 32, range 21-38. Fourteen trials (43%) did not clearly define the study objective, and 11 studies (34%) did not include a sample size calculation. Outcome measures were adequately identified and defined in 23 (71%) trials; adverse events or side effects were not reported in 20 trials (62%). Thirteen trials (40%) were of superior quality according to the Jadad score (>/=3 points). Six trials (18%) reported on long-term implementation of CDS. CONCLUSION: The overall quality of reporting RCTs was low. There is a need to develop standards for reporting RCTs in medical informatics."," *Decision Support Systems, Clinical; Evidence-Based Medicine; *Guideline Adherence; Guidelines as Topic; Humans; Medical Informatics/*standards; Publishing/*standards; Quality Control; Randomized Controlled Trials as Topic/*standards; Research Design/standards",-2,,
466,Botsis,2011,Journal of the American Medical Informatics Association : JAMIA,Text mining for the Vaccine Adverse Event Reporting System: medical text classification using informative feature selection,"OBJECTIVE: The US Vaccine Adverse Event Reporting System (VAERS) collects spontaneous reports of adverse events following vaccination. Medical officers review the reports and often apply standardized case definitions, such as those developed by the Brighton Collaboration. Our objective was to demonstrate a multi-level text mining approach for automated text classification of VAERS reports that could potentially reduce human workload. DESIGN: We selected 6034 VAERS reports for H1N1 vaccine that were classified by medical officers as potentially positive (N(pos)=237) or negative for anaphylaxis. We created a categorized corpus of text files that included the class label and the symptom text field of each report. A validation set of 1100 labeled text files was also used. Text mining techniques were applied to extract three feature sets for important keywords, low- and high-level patterns. A rule-based classifier processed the high-level feature representation, while several machine learning classifiers were trained for the remaining two feature representations. MEASUREMENTS: Classifiers' performance was evaluated by macro-averaging recall, precision, and F-measure, and Friedman's test; misclassification error rate analysis was also performed. RESULTS: Rule-based classifier, boosted trees, and weighted support vector machines performed well in terms of macro-recall, however at the expense of a higher mean misclassification error rate. The rule-based classifier performed very well in terms of average sensitivity and specificity (79.05% and 94.80%, respectively). CONCLUSION: Our validated results showed the possibility of developing effective medical text classifiers for VAERS reports by combining text mining with informative feature selection; this strategy has the potential to reduce reviewer workload considerably."," *Adverse Drug Reaction Reporting Systems/classification; Anaphylaxis/*epidemiology/etiology; Artificial Intelligence; Data Mining/*methods; Humans; Influenza A Virus, H1N1 Subtype; *Natural Language Processing; Reproducibility of Results; Sensitivity and Specificity; Support Vector Machine; United States/epidemiology; Viral Vaccines/*adverse effects",-1,,
467,Chazard,2011,Studies in health technology and informatics,The ADE scorecards: a tool for adverse drug event detection in electronic health records,"Although several methods exist for Adverse Drug events (ADE) detection due to past hospitalizations, a tool that could display those ADEs to the physicians does not exist yet. This article presents the ADE Scorecards, a Web tool that enables to screen past hospitalizations extracted from Electronic Health Records (EHR), using a set of ADE detection rules, presently rules discovered by data mining. The tool enables the physicians to (1) get contextualized statistics about the ADEs that happen in their medical department, (2) see the rules that are useful in their department, i.e. the rules that could have enabled to prevent those ADEs and (3) review in detail the ADE cases, through a comprehensive interface displaying the diagnoses, procedures, lab results, administered drugs and anonymized records. The article shows a demonstration of the tool through a use case."," Data Mining/*methods; Drug-Related Side Effects and Adverse Reactions/*prevention & control; Humans; Information Systems/organization & administration; *Internet; Medical Records Systems, Computerized/*organization & administration",-1,,
468,Goddard,2012,Journal of the American Medical Informatics Association : JAMIA,"Automation bias: a systematic review of frequency, effect mediators, and mitigators","Automation bias (AB)--the tendency to over-rely on automation--has been studied in various academic fields. Clinical decision support systems (CDSS) aim to benefit the clinical decision-making process. Although most research shows overall improved performance with use, there is often a failure to recognize the new errors that CDSS can introduce. With a focus on healthcare, a systematic review of the literature from a variety of research fields has been carried out, assessing the frequency and severity of AB, the effect mediators, and interventions potentially mitigating this effect. This is discussed alongside automation-induced complacency, or insufficient monitoring of automation output. A mix of subject specific and freetext terms around the themes of automation, human-automation interaction, and task performance and error were used to search article databases. Of 13 821 retrieved papers, 74 met the inclusion criteria. User factors such as cognitive style, decision support systems (DSS), and task specific experience mediated AB, as did attitudinal driving factors such as trust and confidence. Environmental mediators included workload, task complexity, and time constraint, which pressurized cognitive resources. Mitigators of AB included implementation factors such as training and emphasizing user accountability, and DSS design factors such as the position of advice on the screen, updated confidence levels attached to DSS output, and the provision of information versus recommendation. By uncovering the mechanisms by which AB operates, this review aims to help optimize the clinical decision-making process for CDSS developers and healthcare practitioners."," *Attitude to Computers; *Automation; Aviation; *Decision Support Systems, Clinical; Humans; Task Performance and Analysis",-2,,
469,Tinoco,2011,Journal of the American Medical Informatics Association : JAMIA,Comparison of computerized surveillance and manual chart review for adverse events,"OBJECTIVE: To understand how the source of information affects different adverse event (AE) surveillance methods. DESIGN: Retrospective analysis of inpatient adverse drug events (ADEs) and hospital-associated infections (HAIs) detected by either a computerized surveillance system (CSS) or manual chart review (MCR). MEASUREMENT: Descriptive analysis of events detected using the two methods by type of AE, type of information about the AE, and sources of the information. RESULTS: CSS detected more HAIs than MCR (92% vs 34%); however, a similar number of ADEs was detected by both systems (52% vs 51%). The agreement between systems was greater for HAIs than ADEs (26% vs 3%). The CSS missed events that did not have information in coded format or that were described only in physician narratives. The MCR detected events missed by CSS using information in physician narratives. Discharge summaries were more likely to contain information about AEs than any other type of physician narrative, followed by emergency department reports for HAIs and general consult notes for ADEs. Some ADEs found by MCR were detected by CSS but not verified by a clinician. LIMITATIONS: Inability to distinguish between CSS false positives and suspected AEs for cases in which the clinician did not document their assessment in the CSS. CONCLUSION: The effect that information source has on different surveillance methods depends on the type of AE. Integrating information from physician narratives with CSS using natural language processing would improve the detection of ADEs more than HAIs.", Cross Infection/*prevention & control; Humans; Medical Audit/*methods; Medication Errors/*prevention & control; *Natural Language Processing; Population Surveillance/*methods; Retrospective Studies; Risk Management/*methods; Sensitivity and Specificity; Utah,-2,,
470,Geifman,2011,BMC bioinformatics,Towards an Age-Phenome Knowledge-base,"BACKGROUND: Currently, data about age-phenotype associations are not systematically organized and cannot be studied methodically. Searching for scientific articles describing phenotypic changes reported as occurring at a given age is not possible for most ages. RESULTS: Here we present the Age-Phenome Knowledge-base (APK), in which knowledge about age-related phenotypic patterns and events can be modeled and stored for retrieval. The APK contains evidence connecting specific ages or age groups with phenotypes, such as disease and clinical traits. Using a simple text mining tool developed for this purpose, we extracted instances of age-phenotype associations from journal abstracts related to non-insulin-dependent Diabetes Mellitus. In addition, links between age and phenotype were extracted from clinical data obtained from the NHANES III survey. The knowledge stored in the APK is made available for the relevant research community in the form of 'Age-Cards', each card holds the collection of all the information stored in the APK about a particular age. These Age-Cards are presented in a wiki, allowing community review, amendment and contribution of additional information. In addition to the wiki interaction, complex searches can also be conducted which require the user to have some knowledge of database query construction. CONCLUSIONS: The combination of a knowledge model based repository with community participation in the evolution and refinement of the knowledge-base makes the APK a useful and valuable environment for collecting and curating existing knowledge of the connections between age and phenotypes.", *Age Factors; *Data Mining; Humans; Knowledge; *Phenotype,-1,,
471,Wright,2011,Journal of the American Medical Informatics Association : JAMIA,A method and knowledge base for automated inference of patient problems from structured data in an electronic medical record,"BACKGROUND: Accurate knowledge of a patient's medical problems is critical for clinical decision making, quality measurement, research, billing and clinical decision support. Common structured sources of problem information include the patient problem list and billing data; however, these sources are often inaccurate or incomplete. OBJECTIVE: To develop and validate methods of automatically inferring patient problems from clinical and billing data, and to provide a knowledge base for inferring problems. STUDY DESIGN AND METHODS: We identified 17 target conditions and designed and validated a set of rules for identifying patient problems based on medications, laboratory results, billing codes, and vital signs. A panel of physicians provided input on a preliminary set of rules. Based on this input, we tested candidate rules on a sample of 100,000 patient records to assess their performance compared to gold standard manual chart review. The physician panel selected a final rule for each condition, which was validated on an independent sample of 100,000 records to assess its accuracy. RESULTS: Seventeen rules were developed for inferring patient problems. Analysis using a validation set of 100,000 randomly selected patients showed high sensitivity (range: 62.8-100.0%) and positive predictive value (range: 79.8-99.6%) for most rules. Overall, the inference rules performed better than using either the problem list or billing data alone. CONCLUSION: We developed and validated a set of rules for inferring patient problems. These rules have a variety of applications, including clinical decision support, care improvement, augmentation of the problem list, and identification of patients for research cohorts."," Algorithms; *Electronic Health Records; Humans; *Knowledge Bases; *Medical Records, Problem-Oriented; *Patient Care Management",-2,,
472,Carnevale,2011,Journal of the American Medical Informatics Association : JAMIA,Evaluating the utility of syndromic surveillance algorithms for screening to detect potentially clonal hospital infection outbreaks,"OBJECTIVE: The authors evaluated algorithms commonly used in syndromic surveillance for use as screening tools to detect potentially clonal outbreaks for review by infection control practitioners. DESIGN: Study phase 1 applied four aberrancy detection algorithms (CUSUM, EWMA, space-time scan statistic, and WSARE) to retrospective microbiologic culture data, producing a list of past candidate outbreak clusters. In phase 2, four infectious disease physicians categorized the phase 1 algorithm-identified clusters to ascertain algorithm performance. In phase 3, project members combined the algorithms to create a unified screening system and conducted a retrospective pilot evaluation. MEASUREMENTS: The study calculated recall and precision for each algorithm, and created precision-recall curves for various methods of combining the algorithms into a unified screening tool. RESULTS: Individual algorithm recall and precision ranged from 0.21 to 0.31 and from 0.053 to 0.29, respectively. Few candidate outbreak clusters were identified by more than one algorithm. The best method of combining the algorithms yielded an area under the precision-recall curve of 0.553. The phase 3 combined system detected all infection control-confirmed outbreaks during the retrospective evaluation period. LIMITATIONS: Lack of phase 2 reviewers' agreement indicates that subjective expert review was an imperfect gold standard. Less conservative filtering of culture results and alternate parameter selection for each algorithm might have improved algorithm performance. CONCLUSION: Hospital outbreak detection presents different challenges than traditional syndromic surveillance. Nevertheless, algorithms developed for syndromic surveillance have potential to form the basis of a combined system that might perform clinically useful hospital outbreak screening."," *Algorithms; Bacterial Infections/epidemiology/prevention & control; Cells, Cultured/microbiology; Cross Infection/epidemiology/*prevention & control; Disease Outbreaks/*prevention & control; Humans; *Infection Control; *Pattern Recognition, Automated; Population Surveillance/*methods; Reproducibility of Results; Retrospective Studies; Sensitivity and Specificity",-2,,
473,Doing-Harris,2011,Journal of medical Internet research,Computer-assisted update of a consumer health vocabulary through mining of social network data,"BACKGROUND: Consumer health vocabularies (CHVs) have been developed to aid consumer health informatics applications. This purpose is best served if the vocabulary evolves with consumers' language. OBJECTIVE: Our objective was to create a computer assisted update (CAU) system that works with live corpora to identify new candidate terms for inclusion in the open access and collaborative (OAC) CHV. METHODS: The CAU system consisted of three main parts: a Web crawler and an HTML parser, a candidate term filter that utilizes natural language processing tools including term recognition methods, and a human review interface. In evaluation, the CAU system was applied to the health-related social network website PatientsLikeMe.com. The system's utility was assessed by comparing the candidate term list it generated to a list of valid terms hand extracted from the text of the crawled webpages. RESULTS: The CAU system identified 88,994 unique terms 1- to 7-grams (""n-grams"" are n consecutive words within a sentence) in 300 crawled PatientsLikeMe.com webpages. The manual review of the crawled webpages identified 651 valid terms not yet included in the OAC CHV or the Unified Medical Language System (UMLS) Metathesaurus, a collection of vocabularies amalgamated to form an ontology of medical terms, (ie, 1 valid term per 136.7 candidate n-grams). The term filter selected 774 candidate terms, of which 237 were valid terms, that is, 1 valid term among every 3 or 4 candidates reviewed. CONCLUSION: The CAU system is effective for generating a list of candidate terms for human review during CHV development."," Biomedical Research; *Computers; *Consumer Health Information; *Data Mining; Humans; Natural Language Processing; *Social Support; *Vocabulary; Vocabulary, Controlled",-1,,
474,Rocha,2013,International journal of medical informatics,Innovations in health care services: the CAALYX system,"PURPOSE: This paper describes proposed health care services innovations, provided by a system called CAALYX (Complete Ambient Assisted Living eXperiment). CAALYX aimed to provide healthcare innovation by extending the state-of-the-art in tele-healthcare, by focusing on increasing the confidence of elderly people living autonomously, by building on the knowledge base of the most common disorders and respective characteristic vital sign changes for this age group. METHODS: A review of the state-of-the-art on health care services was carried out. Then, extensive research was conducted on the particular needs of the elderly in relation to home health services that, if offered to them, could improve their day life by giving them greater confidence and autonomy. To achieve this, we addressed issues associated with the gathering of clinical data and interpretation of these data, as well as possibilities of automatically triggering appropriate clinical measures. Considering this initial work we started the identification of initiatives, ongoing works and technologies that could be used for the development of the system. After that, the implementation of CAALYX was done. FINDINGS: The innovation in CAALYX system considers three main areas of contribution: (i) The Roaming Monitoring System that is used to collect information on the well-being of the elderly users; (ii) The Home Monitoring System that is aimed at helping the elders independently living at home being implemented by a device (a personal computer or a set top box) that supports the connection of sensors and video cameras that may be used for monitoring and for interaction with the elder; (iii) The Central Care Service and Monitoring System that is implemented by a Caretaker System where attention and care services are provided to elders, where actors as Caretakers, Doctors and Relatives are logically linked to elders. Innovations in each of these areas are presented here. CONCLUSIONS: The ageing European society is placing an added burden on future generations, as the 'elderly-to-working-age-people' ratio is set to steadily increase in the future. Nowadays, quality of life and fitness allows for most older persons to have an active life well into their eighties. Furthermore, many older persons prefer to live in their own house and choose their own lifestyle. The CAALYX system can have a clear impact in increasing older persons' autonomy, by ensuring that they do not need to leave their preferred environment in order to be properly monitored and taken care of.", Accidental Falls; Aged; Assisted Living Facilities/*organization & administration; Computer Security; Electronic Health Records; Humans; *Organizational Innovation; Surveys and Questionnaires; Systems Integration; Ambient Assisted Living; Caalyx; Caretaker site; Elderly healthcare; Fall and mobility sensor; Home System; Location-based system; Observation patterns; Personal area network; Sensor network; eHealth; eInclusion,-2,,
475,Bowden,2011,BMC medical research methodology,"Quantifying, displaying and accounting for heterogeneity in the meta-analysis of RCTs using standard and generalised Q statistics","BACKGROUND: Clinical researchers have often preferred to use a fixed effects model for the primary interpretation of a meta-analysis. Heterogeneity is usually assessed via the well known Q and I2 statistics, along with the random effects estimate they imply. In recent years, alternative methods for quantifying heterogeneity have been proposed, that are based on a 'generalised' Q statistic. METHODS: We review 18 IPD meta-analyses of RCTs into treatments for cancer, in order to quantify the amount of heterogeneity present and also to discuss practical methods for explaining heterogeneity. RESULTS: Differing results were obtained when the standard Q and I2 statistics were used to test for the presence of heterogeneity. The two meta-analyses with the largest amount of heterogeneity were investigated further, and on inspection the straightforward application of a random effects model was not deemed appropriate. Compared to the standard Q statistic, the generalised Q statistic provided a more accurate platform for estimating the amount of heterogeneity in the 18 meta-analyses. CONCLUSIONS: Explaining heterogeneity via the pre-specification of trial subgroups, graphical diagnostic tools and sensitivity analyses produced a more desirable outcome than an automatic application of the random effects model. Generalised Q statistic methods for quantifying and adjusting for heterogeneity should be incorporated as standard into statistical software. Software is provided to help achieve this aim."," Analysis of Variance; Clinical Trials as Topic; *Data Interpretation, Statistical; Humans; *Meta-Analysis as Topic; Neoplasms/therapy; *Randomized Controlled Trials as Topic; Statistics as Topic",-2,,
476,Cuggia,2011,International journal of medical informatics,Comparing semi-automatic systems for recruitment of patients to clinical trials,"OBJECTIVES: (i) To review contributions and limitations of decision support systems for automatic recruitment of patients to clinical trials (Clinical Trial Recruitment Support Systems, CTRSS). (ii) To characterize the important features of this domain, the main classes of approach that have been used, and their advantages and disadvantages. (iii) To assess the effectiveness and potential of such systems in improving trial recruitment rates. DATA SOURCES: A systematic MESH keyword-based search of Pubmed, Embase, and Scholar Google for relevant CTRSS publications from January 1st 1998 to August 31st 2009 yielded 73 references, from which 33 relevant papers describing 28 distinct studies were chosen for review, based on their report of a novel decision support system for trial recruitment which reused already available patient data. METHOD: The reviewed papers were classified using a modified version of an existing taxonomy for clinical decision support systems, using 10 axes relevant to the trial recruitment domain. RESULTS: It proved possible and useful to characterize CTRSS on a relatively small number of dimensions and a number of clear trends emerge from the study. Only nine papers reported a useful evaluation of the effectiveness of the system in terms of trial pre-inclusion or enrolment rate. While all the systems reviewed re-use structured and coded patient data none attempts the more difficult task of using unstructured patient notes to pre-screen for trial inclusion. Few studies address acceptance of systems by clinicians, or integration into clinical workflow, and there is little evidence of use of interoperability standards. CONCLUSIONS: System design, scope, and assessment methodology vary significantly between papers, making it difficult to establish the impact of different approaches on recruitment rate. It is clear, however, that the pre-screening phase of trial recruitment is the most effective part of the process to address with CTRSS, that clinical workflow integration and clinician acceptance are critical for this class of decision support, and that the current trends in this field are towards generalization and scalability."," Automation; *Clinical Trials as Topic; *Decision Support Systems, Clinical; Humans; *Patient Selection",-2,,
477,Mpindi,2011,PloS one,GTI: a novel algorithm for identifying outlier gene expression profiles from integrated microarray datasets,"BACKGROUND: Meta-analysis of gene expression microarray datasets presents significant challenges for statistical analysis. We developed and validated a new bioinformatic method for the identification of genes upregulated in subsets of samples of a given tumour type ('outlier genes'), a hallmark of potential oncogenes. METHODOLOGY: A new statistical method (the gene tissue index, GTI) was developed by modifying and adapting algorithms originally developed for statistical problems in economics. We compared the potential of the GTI to detect outlier genes in meta-datasets with four previously defined statistical methods, COPA, the OS statistic, the t-test and ORT, using simulated data. We demonstrated that the GTI performed equally well to existing methods in a single study simulation. Next, we evaluated the performance of the GTI in the analysis of combined Affymetrix gene expression data from several published studies covering 392 normal samples of tissue from the central nervous system, 74 astrocytomas, and 353 glioblastomas. According to the results, the GTI was better able than most of the previous methods to identify known oncogenic outlier genes. In addition, the GTI identified 29 novel outlier genes in glioblastomas, including TYMS and CDKN2A. The over-expression of these genes was validated in vivo by immunohistochemical staining data from clinical glioblastoma samples. Immunohistochemical data were available for 65% (19 of 29) of these genes, and 17 of these 19 genes (90%) showed a typical outlier staining pattern. Furthermore, raltitrexed, a specific inhibitor of TYMS used in the therapy of tumour types other than glioblastoma, also effectively blocked cell proliferation in glioblastoma cell lines, thus highlighting this outlier gene candidate as a potential therapeutic target. CONCLUSIONS/SIGNIFICANCE: Taken together, these results support the GTI as a novel approach to identify potential oncogene outliers and drug targets. The algorithm is implemented in an R package (Text S1)."," *Algorithms; Astrocytoma/genetics; Brain Neoplasms/genetics; Computational Biology/*methods; Data Interpretation, Statistical; Gene Expression Profiling/methods/*statistics & numerical data; Genetic Association Studies/methods; Glioblastoma/genetics; Humans; Microarray Analysis/methods/*statistics & numerical data; Models, Theoretical; Pattern Recognition, Automated/methods; Software; Tumor Cells, Cultured",-1,,
478,Thomas,2011,Research synthesis methods,Applications of text mining within systematic reviews,"Systematic reviews are a widely accepted research method. However, it is increasingly difficult to conduct them to fit with policy and practice timescales, particularly in areas which do not have well indexed, comprehensive bibliographic databases. Text mining technologies offer one possible way forward in reducing the amount of time systematic reviews take to conduct. They can facilitate the identification of relevant literature, its rapid description or categorization, and its summarization. In this paper, we describe the application of four text mining technologies, namely, automatic term recognition, document clustering, classification and summarization, which support the identification of relevant studies in systematic reviews. The contributions of text mining technologies to improve reviewing efficiency are considered and their strengths and weaknesses explored. We conclude that these technologies do have the potential to assist at various stages of the review process. However, they are relatively unknown in the systematic reviewing community, and substantial evaluation and methods development are required before their possible impact can be fully assessed. Copyright (c) 2011 John Wiley & Sons, Ltd.", automatic summarization; document classification; document clustering; research synthesis; screening; searching; systematic review; term recognition; text mining,2,,
479,White,2011,Studies in health technology and informatics,Use of ontologies for monitoring electronic health records for compliance with clinical practice guidelines,"Ontologies can assist with translating information from an electronic health record to a clinical practice guideline and reformatting it into a compliance report. A 2009 literature search reviews publications on the use of ontologies to support automated reporting of compliance with clinical practice guidelines via electronic health records. Research stage, data-pulling capabilities, ontologies used, and issues raised are some of the comparative data pulled from 13 articles from the literature review results. Suggestions for further research are given."," Electronic Health Records/*organization & administration; *Guideline Adherence; *Practice Guidelines as Topic; *Vocabulary, Controlled",-2,,
480,Goddard,2011,Studies in health technology and informatics,Automation bias - a hidden issue for clinical decision support system use,"Automation bias - the tendency to over-rely on automation - has been studied in a variety of academic fields. Clinical Decision Support Systems aim to benefit the clinical decision making process. Although most research shows overall improved performance with use, there is often a failure to recognize the new errors that CDSS can introduce, and the healthcare field has a gap in this research. This paper outlines some of the most compelling theoretical factors in the literature involved in automation bias, and builds a simple model to be tested empirically. Ultimately, this will uncover the mechanisms by which this bias operates and help CDSS producers and healthcare practitioners optimize the medical decision making process."," *Attitude to Computers; Automation; Cognition; Decision Support Systems, Clinical/*statistics & numerical data; Health Personnel/psychology; Humans; *Medical Errors",-2,,
481,Goddard,2011,Studies in health technology and informatics,Decision support and automation bias: methodology and preliminary results of a systematic review,"Automation bias - or a tendency to over-rely on automation - is a subject which has been studied in a variety of academic fields. Clinical Decision Support Systems (CDSS) aim to benefit the clinical decision making process. Although most research shows overall improved performance with use, there is often a failure to recognize the new errors that CDSS can introduce, and as such the healthcare field has a gap in this research. This paper summarizes the methodology and preliminary results of a systematic review over a broad range of fields into the effects of over-reliance on automation. Results indicate that though automation bias is a significant phenomenon, it is not well defined, and there is a gap in the research which must be addressed to optimize the use of decision support."," *Attitude to Computers; Automation; Decision Support Systems, Clinical/*statistics & numerical data; Health Personnel/*psychology; Humans",-2,,
482,Fernandez-Luque,2011,Journal of medical Internet research,Review of extracting information from the Social Web for health personalization,"In recent years the Web has come into its own as a social platform where health consumers are actively creating and consuming Web content. Moreover, as the Web matures, consumers are gaining access to personalized applications adapted to their health needs and interests. The creation of personalized Web applications relies on extracted information about the users and the content to personalize. The Social Web itself provides many sources of information that can be used to extract information for personalization apart from traditional Web forms and questionnaires. This paper provides a review of different approaches for extracting information from the Social Web for health personalization. We reviewed research literature across different fields addressing the disclosure of health information in the Social Web, techniques to extract that information, and examples of personalized health applications. In addition, the paper includes a discussion of technical and socioethical challenges related to the extraction of information for health personalization.", Community Networks; Data Mining; Electronic Health Records; *Health Communication; *Health Education; Humans; *Information Storage and Retrieval; *Internet; *Medical Informatics; Natural Language Processing; Online Systems; *Social Support,-2,,
483,Rosenbloom,2011,Journal of the American Medical Informatics Association : JAMIA,Data from clinical notes: a perspective on the tension between structure and flexible documentation,"Clinical documentation is central to patient care. The success of electronic health record system adoption may depend on how well such systems support clinical documentation. A major goal of integrating clinical documentation into electronic heath record systems is to generate reusable data. As a result, there has been an emphasis on deploying computer-based documentation systems that prioritize direct structured documentation. Research has demonstrated that healthcare providers value different factors when writing clinical notes, such as narrative expressivity, amenability to the existing workflow, and usability. The authors explore the tension between expressivity and structured clinical documentation, review methods for obtaining reusable data from clinical notes, and recommend that healthcare providers be able to choose how to document patient care based on workflow and note content needs. When reusable data are needed from notes, providers can use structured documentation or rely on post-hoc text processing to produce structured data, as appropriate."," Data Mining; *Documentation; Efficiency, Organizational; *Electronic Health Records; *Forms and Records Control; Humans; Medical Records, Problem-Oriented; Natural Language Processing; Reference Standards; Systems Integration; Workflow",-1,,
484,Kreiner,2010,Studies in health technology and informatics,Feasibility of a mobile anticoagulation telemedicine system using automated decision support,"Anticoagulation is necessary for many patients after implantation of mechanical heart valves. In order to reduce the number of visits to a physician for the review of INR values, we propose a telemedicine system based on mobile phones for patients and physicians. Physicians are furthermore supported by a decision support system offering recommendations for therapy adjustments."," Anticoagulants/*therapeutic use; Cell Phone/*instrumentation; *Decision Support Systems, Clinical; Feasibility Studies; Humans; Telemedicine/*methods",-2,,
485,Ahmadian,2011,International journal of medical informatics,The role of standardized data and terminological systems in computerized clinical decision support systems: literature review and survey,"INTRODUCTION: Clinical decision support systems (CDSSs) should be seamlessly integrated with existing clinical information systems to enable automatic provision of advice at the time and place where decisions are made. It has been suggested that a lack of agreed data standards frequently hampers this integration. We performed a literature review to investigate whether CDSSs used standardized (i.e. coded or numerical) data and which terminological systems have been used to code data. We also investigated whether a lack of standardized data was considered an impediment for CDSS implementation. METHODS: Articles reporting an evaluation of a CDSS that provided a computerized advice based on patient-specific data items were identified based on a former literature review on CDSS and on CDSS studies identified in AMIA's 'Year in Review'. Authors of these articles were contacted to check and complete the extracted data. A questionnaire among the authors of included studies was used to determine the obstacles in CDSS implementation. RESULTS: We identified 77 articles published between 1995 and 2008. Twenty-two percent of the evaluated CDSSs used only numerical data. Fifty one percent of the CDSSs that used coded data applied an international terminology. The most frequently used international terminology were the ICD (International Classification of Diseases), used in 68% of the cases and LOINC (Logical Observation Identifiers Names and Codes) in 12% of the cases. More than half of the authors experienced barriers in CDSS implementation. In most cases these barriers were related to the lack of electronically available standardized data required to invoke or activate the CDSS. CONCLUSION: Many CDSSs applied different terminological systems to code data. This diversity hampers the possibility of sharing and reasoning with data within different systems. The results of the survey confirm the hypothesis that data standardization is a critical success factor for CDSS development."," Clinical Trials as Topic; *Decision Making, Computer-Assisted; Decision Support Systems, Clinical/*standards; Humans; Review Literature as Topic; *Terminology as Topic",-2,,
486,Frunza,2011,Artificial intelligence in medicine,Exploiting the systematic review protocol for classification of medical abstracts,"OBJECTIVE: To determine whether the automatic classification of documents can be useful in systematic reviews on medical topics, and specifically if the performance of the automatic classification can be enhanced by using the particular protocol of questions employed by the human reviewers to create multiple classifiers. METHODS AND MATERIALS: The test collection is the data used in large-scale systematic review on the topic of the dissemination strategy of health care services for elderly people. From a group of 47,274 abstracts marked by human reviewers to be included in or excluded from further screening, we randomly selected 20,000 as a training set, with the remaining 27,274 becoming a separate test set. As a machine learning algorithm we used complement naive Bayes. We tested both a global classification method, where a single classifier is trained on instances of abstracts and their classification (i.e., included or excluded), and a novel per-question classification method that trains multiple classifiers for each abstract, exploiting the specific protocol (questions) of the systematic review. For the per-question method we tested four ways of combining the results of the classifiers trained for the individual questions. As evaluation measures, we calculated precision and recall for several settings of the two methods. It is most important not to exclude any relevant documents (i.e., to attain high recall for the class of interest) but also desirable to exclude most of the non-relevant documents (i.e., to attain high precision on the class of interest) in order to reduce human workload. RESULTS: For the global method, the highest recall was 67.8% and the highest precision was 37.9%. For the per-question method, the highest recall was 99.2%, and the highest precision was 63%. The human-machine workflow proposed in this paper achieved a recall value of 99.6%, and a precision value of 17.8%. CONCLUSION: The per-question method that combines classifiers following the specific protocol of the review leads to better results than the global method in terms of recall. Because neither method is efficient enough to classify abstracts reliably by itself, the technology should be applied in a semi-automatic way, with a human expert still involved. When the workflow includes one human expert and the trained automatic classifier, recall improves to an acceptable level, showing that automatic classification techniques can reduce the human workload in the process of building a systematic review."," *Abstracting and Indexing as Topic; Aged; Aged, 80 and over; Algorithms; *Artificial Intelligence; *Bibliometrics; *Data Mining; *Databases, Bibliographic; Evidence-Based Medicine; Health Services for the Aged; Humans; Pattern Recognition, Automated; Publications/*classification; *Systematic Reviews as Topic; Workflow",2,,
487,Shamliyan,2011,Journal of clinical epidemiology,"Development quality criteria to evaluate nontherapeutic studies of incidence, prevalence, or risk factors of chronic diseases: pilot study of new checklists","OBJECTIVE: To develop two checklists for the quality of observational studies of incidence or risk factors of diseases. STUDY DESIGN AND SETTING: Initial development of the checklists was based on a systematic literature review. The checklists were refined after pilot trials of validity and reliability were conducted by seven experts, who tested the checklists on 10 articles. RESULTS: The checklist for studies of incidence or prevalence of chronic disease had six criteria for external validity and five for internal validity. The checklist for risk factor studies had six criteria for external validity, 13 criteria for internal validity, and two aspects of causality. A Microsoft Access database produced automated standardized reports about external and internal validities. Pilot testing demonstrated face and content validities and discrimination of reporting vs. methodological qualities. Interrater agreement was poor. The experts suggested future reliability testing of the checklists in systematic reviews with preplanned protocols, a priori consensus about research-specific quality criteria, and training of the reviewers. CONCLUSION: We propose transparent and standardized quality assessment criteria of observational studies using the developed checklists. Future testing of the checklists in systematic reviews is necessary to develop reliable tools that can be used with confidence.", Bias; Checklist/methods/*standards; Chronic Disease/*epidemiology; Female; Humans; Incidence; Male; Pilot Projects; Prevalence; Quality Control; Reproducibility of Results; Research Design/standards; Risk Factors,-2,,
488,Ma,2010,BMC bioinformatics,"Classifying and scoring of molecules with the NGN: new datasets, significance tests, and generalization","UNLABELLED: This paper demonstrates how a Neural Grammar Network learns to classify and score molecules for a variety of tasks in chemistry and toxicology. In addition to a more detailed analysis on datasets previously studied, we introduce three new datasets (BBB, FXa, and toxicology) to show the generality of the approach. A new experimental methodology is developed and applied to both the new datasets as well as previously studied datasets. This methodology is rigorous and statistically grounded, and ultimately culminates in a Wilcoxon significance test that proves the effectiveness of the system. We further include a complete generalization of the specific technique to arbitrary grammars and datasets using a mathematical abstraction that allows researchers in different domains to apply the method to their own work. BACKGROUND: Our work can be viewed as an alternative to existing methods to solve the quantitative structure-activity relationship (QSAR) problem. To this end, we review a number approaches both from a methodological and also a performance perspective. In addition to these approaches, we also examined a number of chemical properties that can be used by generic classifier systems, such as feed-forward artificial neural networks. In studying these approaches, we identified a set of interesting benchmark problem sets to which many of the above approaches had been applied. These included: ACE, AChE, AR, BBB, BZR, Cox2, DHFR, ER, FXa, GPB, Therm, and Thr. Finally, we developed our own benchmark set by collecting data on toxicology. RESULTS: Our results show that our system performs better than, or comparatively to, the existing methods over a broad range of problem types. Our method does not require the expert knowledge that is necessary to apply the other methods to novel problems. CONCLUSIONS: We conclude that our success is due to the ability of our system to: 1) encode molecules losslessly before presentation to the learning system, and 2) leverage the design of molecular description languages to facilitate the identification of relevant structural attributes of the molecules over different problem domains."," Algorithms; Alkaloids; Animals; *Artificial Intelligence; Computational Biology/*methods; *Databases, Factual; Mice; Pattern Recognition, Automated/*methods; Proteins/classification; Quantitative Structure-Activity Relationship; Rats; Regression Analysis; Reproducibility of Results; Software",-2,,
489,Stanfill,2010,Journal of the American Medical Informatics Association : JAMIA,A systematic literature review of automated clinical coding and classification systems,"Clinical coding and classification processes transform natural language descriptions in clinical text into data that can subsequently be used for clinical care, research, and other purposes. This systematic literature review examined studies that evaluated all types of automated coding and classification systems to determine the performance of such systems. Studies indexed in Medline or other relevant databases prior to March 2009 were considered. The 113 studies included in this review show that automated tools exist for a variety of coding and classification purposes, focus on various healthcare specialties, and handle a wide variety of clinical document types. Automated coding and classification systems themselves are not generalizable, nor are the results of the studies evaluating them. Published research shows these systems hold promise, but these data must be considered in context, with performance relative to the complexity of the task and the desired outcome."," *Automation; *Classification; *Clinical Coding; Humans; *Vocabulary, Controlled",-2,,
490,Kupershmidt,2010,PloS one,Ontology-based meta-analysis of global collections of high-throughput public data,"BACKGROUND: The investigation of the interconnections between the molecular and genetic events that govern biological systems is essential if we are to understand the development of disease and design effective novel treatments. Microarray and next-generation sequencing technologies have the potential to provide this information. However, taking full advantage of these approaches requires that biological connections be made across large quantities of highly heterogeneous genomic datasets. Leveraging the increasingly huge quantities of genomic data in the public domain is fast becoming one of the key challenges in the research community today. METHODOLOGY/RESULTS: We have developed a novel data mining framework that enables researchers to use this growing collection of public high-throughput data to investigate any set of genes or proteins. The connectivity between molecular states across thousands of heterogeneous datasets from microarrays and other genomic platforms is determined through a combination of rank-based enrichment statistics, meta-analyses, and biomedical ontologies. We address data quality concerns through dataset replication and meta-analysis and ensure that the majority of the findings are derived using multiple lines of evidence. As an example of our strategy and the utility of this framework, we apply our data mining approach to explore the biology of brown fat within the context of the thousands of publicly available gene expression datasets. CONCLUSIONS: Our work presents a practical strategy for organizing, mining, and correlating global collections of large-scale genomic data to explore normal and disease biology. Using a hypothesis-free approach, we demonstrate how a data-driven analysis across very large collections of genomic data can reveal novel discoveries and evidence to support existing hypothesis."," Animals; *Data Mining; Database Management Systems; *Databases, Genetic; Gene Expression Profiling; Humans; Meta-Analysis as Topic",-1,,
491,Salvador,2010,Studies in health technology and informatics,Evaluation methodology for automatic radiology reporting transcription systems,"This article describes a usability evaluation methodology for automatic transcription system used for radiology reporting. In order to assess this class of system's limitations and strengths, a review of the concepts involved in this kind of system is done in a critical way. Specific requirements, for this category of application, that are forgotten when a product is launched in the market, are listed and a methodology for their evaluation is presented."," *Evaluation Studies as Topic; *Medical Records; Medical Records Systems, Computerized; Radiology Information Systems/*standards",-2,,
492,Michalakidis,2010,Studies in health technology and informatics,A system for solution-orientated reporting of errors associated with the extraction of routinely collected clinical data for research and quality improvement,"BACKGROUND: We have used routinely collected clinical data in epidemiological and quality improvement research for over 10 years. We extract, pseudonymise and link data from heterogeneous distributed databases; inevitably encountering errors and problems. OBJECTIVE: To develop a solution-orientated system of error reporting which enables appropriate corrective action. METHOD: Review of the 94 errors, which occurred in 2008/9. Previously we had described failures in terms of the data missing from our response files; however this provided little information about causation. We therefore developed a taxonomy based on the IT component limiting data extraction. RESULTS: Our final taxonomy categorised errors as: (A) Data extraction Method and Process; (B) Translation Layer and Proxy Specification; (C) Shape and Complexity of the Original Schema; (D) Communication and System (mainly Software-based) Faults; (E) Hardware and Infrastructure; (F) Generic/Uncategorised and/or Human Errors. We found 79 distinct errors among the 94 reported; and the categories were generally predictive of the time needed to develop fixes. CONCLUSIONS: A systematic approach to errors and linking them to problem solving has improved project efficiency and enabled us to better predict any associated delays."," Biomedical Research/*statistics & numerical data; Data Mining/*methods; Medical Errors/*classification/prevention & control/*statistics & numerical data; Medical Records Systems, Computerized/*statistics & numerical data; Missouri; Quality Assurance, Health Care/*standards; Risk Management/*organization & administration",-2,,
493,Fiszman,2010,Studies in health technology and informatics,Combining relevance assignment with quality of the evidence to support guideline development,"Clinical practice guidelines are used to disseminate best practice to clinicians. Successful guidelines depend on literature that is both relevant to the questions posed and based on high quality research in accordance with evidence-based medicine. Meeting these standards requires extensive manual review. We describe a system that combines symbolic semantic processing with a statistical method for selecting both relevant and high quality studies. We focused on a cardiovascular risk factor guideline, and the overall performance of the system was 56% recall, 91% precision (F0.5-score 0.81). If quality of the evidence is not taken into account, performance drops to 62% recall, 79% precision (F0.5-score 0.75). We suggest that this system can potentially improve the efficiency of the literature review process in guideline development."," Canada; Data Mining/*methods; *Databases, Bibliographic; Evidence-Based Medicine/*standards; *Natural Language Processing; *Practice Guidelines as Topic; Quality Assurance, Health Care/*methods",2,,
494,Delamarre,2010,Studies in health technology and informatics,Documentation in pharmacovigilance: using an ontology to extend and normalize Pubmed queries,"OBJECTIVES: To assess and understand adverse drug reactions (ADRs), a systematic review of reference databases like Pubmed is a necessary and mandatory step in Pharmacovigilance. In order to assist pharmacovigilance team with a computerized tool, we performed a comparative study of 4 different approaches to query Pubmed through ADR-drug terms. The aim of this study is to assess how an ontology of adverse effects, used to normalize and extend queries, could improve this search. MATERIAL AND METHOD: The ontological resource OntoEIM contains 58,000 classes and integrates MedDRA terminology. The entry point is a ADR-Drug term and the four methods are (i) a direct search on Pubmed (ii) a search with a normalized query enhanced with domain-specific Mesh Heading criteria, (iii) a search with the same elaborated query extended to the MeSH sub-hierarchy of the adverse effect entry and (iv) a search with a set of MedDRA terms grouped by subsomption in the OntoEIM ontology. For each of the 16 queries performed and analysed, relevant publications are selected ""manually"" by two pharmacovigilant experts. RESULTS: The recall is respectively of 63%, 50%, 67% and 74%, the precision of 13%, 26%, 29% and 4%. The best recall is provided by the ontology-based method, for 4 cases out of 16 this method returns relevant publications when the others return no results. CONCLUSION: Results show that an ontology-based search tool improves the recall performance, but other tools and methods are needed to raise the precision."," Data Mining/*methods; *Database Management Systems; Documentation/*methods; Drug-Related Side Effects and Adverse Reactions/*epidemiology; Humans; *Natural Language Processing; *PubMed; *Terminology as Topic; Vocabulary, Controlled",-1,,
495,Koller,2010,Studies in health technology and informatics,Electronic surveillance of healthcare-associated infections with MONI-ICU--a clinical breakthrough compared to conventional surveillance systems,"Surveillance of clinical entities such as healthcare-associated infections (HCAI) by conventional techniques is a time-consuming task for highly trained experts. Such are neither available nor affordable in sufficient numbers on a permanent basis. Nevertheless, expert surveillance is a key parameter for good clinical practice, especially in intensive care medicine. MONI-ICU (monitoring of nosocomial infections in intensive care units) has been developed methodically and practically in a stepwise manner over the last 20 years and is now a reliable tool for clinical experts. It provides an almost real-time view of clinical indicators for HCAI--at the cost of almost no additional time on the part of surveillance staff or clinicians. We describe the use of this system in clinical routine and compare the results generated automatically by MONI-ICU with those generated in parallel by trained surveillance staff using patient chart reviews and other available information (""gold standard""). A total of 99 ICU patient admissions representing 1007 patient days were analyzed. MONI-ICU identified correctly the presence of an HCAI condition in 28/31 cases (sensitivity, 90.3%) and their absence in 68/68 of the non-HCAI cases (specificity, 100%), the latter meaning that MONI-ICU produced no ""false alarms"". The time taken for conventional surveillance at the 52 ward visits was 82.5 hours. MONI-ICU analysis of the same patient cases, including careful review of the generated results required only 12.5 hours (15.2%).", Austria/epidemiology; Cross Infection/*diagnosis/*embryology; Database Management Systems/*organization & administration; Disease Notification/*methods; Electronic Health Records/*organization & administration; Humans; Information Storage and Retrieval/methods; *Sentinel Surveillance; *Software,-2,,
496,Bekhuis,2010,Studies in health technology and informatics,Towards automating the initial screening phase of a systematic review,"Systematic review authors synthesize research to guide clinicians in their practice of evidence-based medicine. Teammates independently identify provisionally eligible studies by reading the same set of hundreds and sometimes thousands of citations during an initial screening phase. We investigated whether supervised machine learning methods can potentially reduce their workload. We also extended earlier research by including observational studies of a rare condition. To build training and test sets, we used annotated citations from a search conducted for an in-progress Cochrane systematic review. We extracted features from titles, abstracts, and metadata, then trained, optimized, and tested several classifiers with respect to mean performance based on 10-fold cross-validations. In the training condition, the evolutionary support vector machine (EvoSVM) with an Epanechnikov or radial kernel is the best classifier: mean recall=100%; mean precision=48% and 41%, respectively. In the test condition, EvoSVM performance degrades: mean recall=77%, mean precision ranges from 26% to 37%. Because near-perfect recall is essential in this context, we conclude that supervised machine learning methods may be useful for reducing workload under certain conditions."," Abstracting and Indexing as Topic/*methods; *Artificial Intelligence; Database Management Systems; *Databases, Bibliographic; Information Storage and Retrieval/*methods; *Natural Language Processing; Peer Review, Research/*methods; Pennsylvania; *Vocabulary, Controlled",2,,
497,Chapman,2010,Journal of the American Medical Informatics Association : JAMIA,Developing syndrome definitions based on consensus and current use,"OBJECTIVE: Standardized surveillance syndromes do not exist but would facilitate sharing data among surveillance systems and comparing the accuracy of existing systems. The objective of this study was to create reference syndrome definitions from a consensus of investigators who currently have or are building syndromic surveillance systems. DESIGN: Clinical condition-syndrome pairs were catalogued for 10 surveillance systems across the United States and the representatives of these systems were brought together for a workshop to discuss consensus syndrome definitions. RESULTS: Consensus syndrome definitions were generated for the four syndromes monitored by the majority of the 10 participating surveillance systems: Respiratory, gastrointestinal, constitutional, and influenza-like illness (ILI). An important element in coming to consensus quickly was the development of a sensitive and specific definition for respiratory and gastrointestinal syndromes. After the workshop, the definitions were refined and supplemented with keywords and regular expressions, the keywords were mapped to standard vocabularies, and a web ontology language (OWL) ontology was created. LIMITATIONS: The consensus definitions have not yet been validated through implementation. CONCLUSION: The consensus definitions provide an explicit description of the current state-of-the-art syndromes used in automated surveillance, which can subsequently be systematically evaluated against real data to improve the definitions. The method for creating consensus definitions could be applied to other domains that have diverse existing definitions.", *Communicable Diseases; Group Processes; Humans; Population Surveillance/*methods; Syndrome; United States,-2,,
498,Kullo,2010,Journal of the American Medical Informatics Association : JAMIA,Leveraging informatics for genetic studies: use of the electronic medical record to enable a genome-wide association study of peripheral arterial disease,"BACKGROUND: There is significant interest in leveraging the electronic medical record (EMR) to conduct genome-wide association studies (GWAS). METHODS: A biorepository of DNA and plasma was created by recruiting patients referred for non-invasive lower extremity arterial evaluation or stress ECG. Peripheral arterial disease (PAD) was defined as a resting/post-exercise ankle-brachial index (ABI) less than or equal to 0.9, a history of lower extremity revascularization, or having poorly compressible leg arteries. Controls were patients without evidence of PAD. Demographic data and laboratory values were extracted from the EMR. Medication use and smoking status were established by natural language processing of clinical notes. Other risk factors and comorbidities were ascertained based on ICD-9-CM codes, medication use and laboratory data. RESULTS: Of 1802 patients with an abnormal ABI, 115 had non-atherosclerotic vascular disease such as vasculitis, Buerger's disease, trauma and embolism (phenocopies) based on ICD-9-CM diagnosis codes and were excluded. The PAD cases (66+/-11 years, 64% men) were older than controls (61+/-8 years, 60% men) but had similar geographical distribution and ethnic composition. Among PAD cases, 1444 (85.6%) had an abnormal ABI, 233 (13.8%) had poorly compressible arteries and 10 (0.6%) had a history of lower extremity revascularization. In a random sample of 95 cases and 100 controls, risk factors and comorbidities ascertained from EMR-based algorithms had good concordance compared with manual record review; the precision ranged from 67% to 100% and recall from 84% to 100%. CONCLUSION: This study demonstrates use of the EMR to ascertain phenocopies, phenotype heterogeneity and relevant covariates to enable a GWAS of PAD. Biorepositories linked to EMR may provide a relatively efficient means of conducting GWAS."," Aged; Algorithms; Case-Control Studies; Comorbidity; Databases, Factual; *Electronic Health Records; Female; Genome-Wide Association Study/*methods; Humans; Male; Medical Records; Middle Aged; Natural Language Processing; Peripheral Vascular Diseases/epidemiology/*genetics; Risk Factors; United States/epidemiology",-2,,
499,Spasic,2010,Journal of the American Medical Informatics Association : JAMIA,Medication information extraction with linguistic pattern matching and semantic rules,"OBJECTIVE: This study presents a system developed for the 2009 i2b2 Challenge in Natural Language Processing for Clinical Data, whose aim was to automatically extract certain information about medications used by a patient from his/her medical report. The aim was to extract the following information for each medication: name, dosage, mode/route, frequency, duration and reason. DESIGN: The system implements a rule-based methodology, which exploits typical morphological, lexical, syntactic and semantic features of the targeted information. These features were acquired from the training dataset and public resources such as the UMLS and relevant web pages. Information extracted by pattern matching was combined together using context-sensitive heuristic rules. MEASUREMENTS: The system was applied to a set of 547 previously unseen discharge summaries, and the extracted information was evaluated against a manually prepared gold standard consisting of 251 documents. The overall ranking of the participating teams was obtained using the micro-averaged F-measure as the primary evaluation metric. RESULTS: The implemented method achieved the micro-averaged F-measure of 81% (with 86% precision and 77% recall), which ranked this system third in the challenge. The significance tests revealed the system's performance to be not significantly different from that of the second ranked system. Relative to other systems, this system achieved the best F-measure for the extraction of duration (53%) and reason (46%). CONCLUSION: Based on the F-measure, the performance achieved (81%) was in line with the initial agreement between human annotators (82%), indicating that such a system may greatly facilitate the process of extracting relevant information from medical records by providing a solid basis for a manual review process.", Artificial Intelligence; *Electronic Health Records; Humans; Information Storage and Retrieval/*methods; Linguistics; *Natural Language Processing; *Pharmaceutical Preparations; Semantics,-2,,
500,Weekes,2010,BMC bioinformatics,TOPSAN: a collaborative annotation environment for structural genomics,"BACKGROUND: Many protein structures determined in high-throughput structural genomics centers, despite their significant novelty and importance, are available only as PDB depositions and are not accompanied by a peer-reviewed manuscript. Because of this they are not accessible by the standard tools of literature searches, remaining underutilized by the broad biological community. RESULTS: To address this issue we have developed TOPSAN, The Open Protein Structure Annotation Network, a web-based platform that combines the openness of the wiki model with the quality control of scientific communication. TOPSAN enables research collaborations and scientific dialogue among globally distributed participants, the results of which are reviewed by experts and eventually validated by peer review. The immediate goal of TOPSAN is to harness the combined experience, knowledge, and data from such collaborations in order to enhance the impact of the astonishing number and diversity of structures being determined by structural genomics centers and high-throughput structural biology. CONCLUSIONS: TOPSAN combines features of automated annotation databases and formal, peer-reviewed scientific research literature, providing an ideal vehicle to bridge a gap between rapidly accumulating data from high-throughput technologies and a much slower pace for its analysis and integration with other, relevant research."," Computational Biology/methods; Cooperative Behavior; *Databases, Genetic; Genomics/*methods; Internet; Microarray Analysis; Proteins/*chemistry/genetics",-2,,
