{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>AutoSynthesis study group</h1>\n",
    "<h2 align='right'> Session 3 - Modeling </h2>\n",
    "<h3 align='right'> 20th February 2019 </h3>\n",
    "<h3 align='right'> Kazeem </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of last session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul> \n",
    "<li> Loading and analysis of data </li>\n",
    "<li> Basic preprocessing </li>\n",
    "    <ul>\n",
    "        <li> Lowercasing </li>\n",
    "        <li> Stopwords removal </li>\n",
    "        <li> Stemming </li>\n",
    "        <li> Lemmatisation </li>\n",
    "    </ul>\n",
    "<li> Feature representation </li>\n",
    "    <ul>\n",
    "        <li> Bag of words </li>\n",
    "        <li> Tf-idf </li>\n",
    "        <li><font color = 'red'><strong> Binary </strong></font></li>\n",
    "        <li> Word embedding </li>\n",
    "        <li> N-grams </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages import successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB, BernoulliNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print ('Packages import successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  00 000 00001 001 003 01 02 03 038 05  ...   works world worldwide written  \\\n",
      "0  0   0     0   0   0  0  0  0   0  0  ...       0     0         0       0   \n",
      "1  0   0     0   0   0  0  0  0   0  0  ...       0     0         1       0   \n",
      "2  0   0     0   0   0  0  0  0   0  0  ...       0     0         0       0   \n",
      "3  0   0     0   0   0  0  0  0   0  0  ...       0     0         0       0   \n",
      "4  0   0     0   0   0  0  0  0   0  0  ...       0     0         0       0   \n",
      "\n",
      "  xplore year years yield yielded yields  \n",
      "0      0    0     0     0       0      0  \n",
      "1      0    0     0     0       0      0  \n",
      "2      0    0     0     0       1      0  \n",
      "3      0    0     0     0       0      0  \n",
      "4      0    0     0     0       0      0  \n",
      "\n",
      "[5 rows x 2573 columns]\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#load data\n",
    "train = pd.read_csv('../session 2_no password/session 2/AutoSession2.csv')\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True, stop_words = 'english')\n",
    "tdm = vectorizer.fit_transform(train['Abstract'])\n",
    "words = vectorizer.get_feature_names()\n",
    "words = np.asarray(words)\n",
    "\n",
    "BoW =np.vstack((words, tdm.toarray()))\n",
    "tdm_df = pd.DataFrame(data=BoW[1:,:], columns = BoW[0,:])\n",
    "print (tdm_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher\n",
    ">- Why do we need preprocessing?\n",
    ">- Why do we need feature representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML algorithms example 1: Naive Baye's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Based on the equation below:</p>\n",
    "<img src = \"Bayes_rule.webp\">\n",
    "\n",
    "Where,\n",
    "<ul>\n",
    "    <li>P(c|x) is the posterior probability of class (c, target label) given predictor (x, feature).</li>\n",
    "    <li>P(c) is the prior probability of class.</li>\n",
    "    <li>P(x|c) is the likelihood which is the probability of predictor given class.</li>\n",
    "    <li>P(x) is the prior probability of predictor.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML algorithms example 2: support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In support vector machine (SVM) each data item is plotted as a point in an n-dimensional space (where n is the number of features in the TDM)where the value of each feature is the value of each coordinate. The algorithm  classifies the data by finding the hyperplane that best partition the data.</p>\n",
    "<p>The SVM relies on the data points closest to the hyperplane on both sides to make prediction.</p>\n",
    "\n",
    "<img src=\"SVM_1.png\">\n",
    "\n",
    "<h4>Linear separablity and maximum margin</h4>\n",
    "<div>\n",
    "    <img src=\"SVM_21.png\"></div>\n",
    "    <img src=\"SVM_3.png\"></div>\n",
    "    <img src=\"SVM_4.png\"></div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h4>The Kernel trick</h4>\n",
    "<p> What happens in situations where the data is not linearly separable?\n",
    "<div>\n",
    "    <img src=\"SVM_8.png\"></div>\n",
    "    <img src=\"SVM_9.png\"></div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build text predictive models with ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A typical text classification process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"supervised_learning.png\" alt=\"text classification process\" title=\"text classification\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note - using sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - load your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Decision1</th>\n",
       "      <th>Decision2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Lerner</td>\n",
       "      <td>2018</td>\n",
       "      <td>Journal of clinical epidemiology</td>\n",
       "      <td>Automatic screening using word embeddings achi...</td>\n",
       "      <td>OBJECTIVE: We aimed to develop and evaluate an...</td>\n",
       "      <td>automatic screening; live cumulative network ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Zhu</td>\n",
       "      <td>2018</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>Heated humidification did not improve complian...</td>\n",
       "      <td>INTRODUCTION: We performed a meta-analysis on ...</td>\n",
       "      <td></td>\n",
       "      <td>-2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pons</td>\n",
       "      <td>2018</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>Quantifying skeletal muscle volume and shape i...</td>\n",
       "      <td>AIMS: The aim of this study was to report the ...</td>\n",
       "      <td></td>\n",
       "      <td>-2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hinds</td>\n",
       "      <td>2018</td>\n",
       "      <td>PloS one</td>\n",
       "      <td>What demographic attributes do our digital foo...</td>\n",
       "      <td>To what extent does our online activity reveal...</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Murray</td>\n",
       "      <td>2018</td>\n",
       "      <td>Journal of the American Medical Informatics As...</td>\n",
       "      <td>Automated and flexible identification of compl...</td>\n",
       "      <td>Accurate and efficient identification of compl...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article_ID Authors  Year  \\\n",
       "0           1  Lerner  2018   \n",
       "1           2     Zhu  2018   \n",
       "2           3    Pons  2018   \n",
       "3           4   Hinds  2018   \n",
       "4           5  Murray  2018   \n",
       "\n",
       "                                             Journal  \\\n",
       "0                   Journal of clinical epidemiology   \n",
       "1                                           PloS one   \n",
       "2                                           PloS one   \n",
       "3                                           PloS one   \n",
       "4  Journal of the American Medical Informatics As...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Automatic screening using word embeddings achi...   \n",
       "1  Heated humidification did not improve complian...   \n",
       "2  Quantifying skeletal muscle volume and shape i...   \n",
       "3  What demographic attributes do our digital foo...   \n",
       "4  Automated and flexible identification of compl...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  OBJECTIVE: We aimed to develop and evaluate an...   \n",
       "1  INTRODUCTION: We performed a meta-analysis on ...   \n",
       "2  AIMS: The aim of this study was to report the ...   \n",
       "3  To what extent does our online activity reveal...   \n",
       "4  Accurate and efficient identification of compl...   \n",
       "\n",
       "                                            Keywords  Decision1  Decision2  \\\n",
       "0   automatic screening; live cumulative network ...          2        2.0   \n",
       "1                                                            -2       -2.0   \n",
       "2                                                            -2       -2.0   \n",
       "3                                                            -1       -2.0   \n",
       "4                                                             1       -1.0   \n",
       "\n",
       "  label  \n",
       "0     Y  \n",
       "1     N  \n",
       "2     N  \n",
       "3     N  \n",
       "4     N  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('autosynthesis_session3.csv') #set the data path relative to your system and file location\n",
    "print ('Dataset loaded successfully')\n",
    "data.head(5) #view some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steb 1b - explore the dataset to gain insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Lena for her explorative work on the labelled dataset. These can be found in the shared session's folder named '<em><a href = '../data insight/Lena_Results.ipynb'>data instight</a></em>'\n",
    "<p>Note that there are two other excel files in the same folder</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - correct annomalies and fix NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]), array([482], dtype=int64))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test for blank spaces in label\n",
    "np.unique(pd.isna(data.label), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([275, 207], dtype=int64))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test for blank spaces in label\n",
    "np.unique(pd.isna(data.Decision2), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There must be no missing data in data particularly the labels. If it exists, FIX.\n",
    "data.Decision2 = data.Decision2.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - properly encode the target/label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: labels, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['labels'] = le.fit_transform(data['label'])\n",
    "print (data['labels'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall class distribution: \n",
      " (array(['N', 'Y'], dtype=object), array([411,  71], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#view label distribution\n",
    "#import numpy as np\n",
    "label_freq = np.unique(data.label, return_counts=True)\n",
    "print(\"Overall class distribution: \\n\", label_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - extract required subset of data (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TiAbs'] = data[['Title', 'Abstract', 'Keywords']].apply(lambda x: '{} {} {}'.format(x[0], x[1], x[2]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - split data to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['TiAbs'], data['label'], test_size=0.10, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  (433,)\n",
      "Test  data size:  (49,)\n",
      "Train LABEL size:  (433,)\n",
      "Test  LABEL size:  (49,)\n",
      "Overall class distribution: \n",
      " 181    N\n",
      "37     N\n",
      "10     N\n",
      "411    N\n",
      "468    N\n",
      "185    N\n",
      "320    N\n",
      "317    N\n",
      "387    N\n",
      "190    N\n",
      "336    Y\n",
      "179    N\n",
      "12     N\n",
      "238    N\n",
      "197    N\n",
      "108    N\n",
      "200    Y\n",
      "348    N\n",
      "75     N\n",
      "142    Y\n",
      "205    N\n",
      "64     N\n",
      "474    Y\n",
      "237    Y\n",
      "193    N\n",
      "204    N\n",
      "270    N\n",
      "38     N\n",
      "137    N\n",
      "66     Y\n",
      "392    N\n",
      "338    N\n",
      "121    N\n",
      "71     N\n",
      "350    N\n",
      "27     N\n",
      "283    N\n",
      "297    N\n",
      "24     Y\n",
      "150    N\n",
      "383    N\n",
      "187    N\n",
      "456    N\n",
      "26     N\n",
      "96     N\n",
      "145    N\n",
      "465    N\n",
      "242    N\n",
      "339    N\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check to see that data and labels are of the same size\n",
    "print ('Train data size: ', X_train.shape)\n",
    "print ('Test  data size: ', X_test.shape)\n",
    "print ('Train LABEL size: ', X_train.shape)\n",
    "print ('Test  LABEL size: ', X_test.shape)\n",
    "\n",
    "#check distribution\n",
    "label_freq = np.unique(y_test, return_counts=True)\n",
    "print(\"Overall class distribution: \\n\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally write custom preprocessing method.....WHY?\n",
    "def preprocessor(text):\n",
    "    #text = text.apply(lambda x: ' '.join(x.lower().replace('[^\\w\\s]','') for x in str(x).split() if not x in set(stopwords.words('english')) and not x.isdigit()))\n",
    "    \n",
    "    # split into words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    #from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words and len(w) > 3]\n",
    "    \n",
    "    return ' '.join(words) #return the cleaned text string separated by spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6a - data cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: preprocessor(x))\n",
    "X_test = X_test.apply(lambda x: preprocessor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181    data elements myeloid leukemia data standards ...\n",
       "37     occurrence types consequences preventability i...\n",
       "10     computational methods gene regulatory networks...\n",
       "411    vaccine adverse event text mining system extra...\n",
       "468    development quality criteria evaluate nonthera...\n",
       "Name: TiAbs, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 6b - feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "binary_encoder = TfidfVectorizer(stop_words='english', binary = True, max_df=0.8, min_df=3, ngram_range=(1, 1))\n",
    "binary_train_data = binary_encoder.fit_transform(X_train)\n",
    "binary_test_data = binary_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_encoder = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=3, ngram_range=(1, 1))\n",
    "tfidf_train_data = tfidf_encoder.fit_transform(X_train)\n",
    "tfidf_test_data = tfidf_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - fit a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting Gaussian NB model\n",
      "Done fitting Bernoulli NB model\n",
      "Done fitting Multinomial NB model\n",
      "Done fitting Complement NB model\n",
      "Done fitting SVM model\n",
      "------------------------------------------------------------------------ \n",
      "\n",
      "Finished fitting all models\n",
      "Trained models ready for prediction on new data \n",
      "\n",
      "------------------------------------------------------------------------ \n",
      "\n",
      "Done predicting with Gaussian NB model\n",
      "Done predicting with Bernoulli NB model\n",
      "Done predicting with Multinomial NB model\n",
      "Done predicting with Complement NB model\n",
      "Done predicting with SVM model\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB, BernoulliNB\n",
    "#MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "#SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001,\n",
    "# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB(binarize = None)#if dataset is already in binary form\n",
    "mnb = MultinomialNB()\n",
    "cnb = ComplementNB()\n",
    "svm = SVC(C = 10, kernel = 'linear', class_weight=None, gamma = 'scale', random_state=None)\n",
    "\n",
    "\n",
    "#train mmodel using training data\n",
    "gnb_model = gnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Gaussian NB model')\n",
    "bnb_model = bnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Bernoulli NB model')\n",
    "mnb_model = mnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Multinomial NB model')\n",
    "cnb_model = cnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Complement NB model')\n",
    "svm_model = svm.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting SVM model')\n",
    "\n",
    "print ('------------------------------------------------------------------------ \\n')\n",
    "print ('Finished fitting all models')\n",
    "print ('Trained models ready for prediction on new data \\n')\n",
    "print ('------------------------------------------------------------------------ \\n')\n",
    "\n",
    "#use each model to predict on new data\n",
    "gnb_prediction = gnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Gaussian NB model')\n",
    "bnb_prediction = bnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Bernoulli NB model')\n",
    "mnb_prediction = mnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Multinomial NB model')\n",
    "cnb_prediction = cnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Complement NB model')\n",
    "svm_prediction = svm_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with SVM model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n"
     ]
    }
   ],
   "source": [
    "print ('Gaussian NB model: \\n', gnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n"
     ]
    }
   ],
   "source": [
    "print ('Bernoulli NB model: \\n', bnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial  NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n"
     ]
    }
   ],
   "source": [
    "print ('Multinomial  NB model: \\n', mnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complement NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n"
     ]
    }
   ],
   "source": [
    "print ('Complement NB model: \\n', cnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N'\n",
      " 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n"
     ]
    }
   ],
   "source": [
    "print ('SVM model: \\n', svm_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some basic metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'conf_mat.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Yes       0.88      1.00      0.93        42\n",
      "          No       1.00      0.14      0.25         7\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        49\n",
      "   macro avg       0.94      0.57      0.59        49\n",
      "weighted avg       0.89      0.88      0.84        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "accuracy = accuracy_score(y_test, cnb_prediction)\n",
    "precision = precision_score(y_test, cnb_prediction, average= 'micro')\n",
    "recall = recall_score(y_test, cnb_prediction, average= 'micro')\n",
    "confusion_matrix(y_test, cnb_prediction)\n",
    "print(classification_report(y_test, cnb_prediction, target_names=['Yes', 'No']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guassian NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  7   49\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Guassian NB model: \\n')\n",
    "pd.crosstab(gnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  7   49\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Bernoulli NB model: \\n')\n",
    "pd.crosstab(bnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  7   49\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Multinomial NB model: \\n')\n",
    "pd.crosstab(mnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complement NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  6   48\n",
       "Y           0  1    1\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Complement NB model: \\n')\n",
    "pd.crosstab(cnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  4   46\n",
       "Y           0  3    3\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('SVM model: \\n')\n",
    "pd.crosstab(svm_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = X_test\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "test_data['Article_ID'] = pd.DataFrame(data.iloc[list(test_data.index.values), 0])\n",
    "test_data['true label'] = y_test\n",
    "test_data['bnb_binary'] = gnb_prediction\n",
    "test_data['bnb_binary'] = bnb_prediction\n",
    "test_data['mnb_binary'] = mnb_prediction\n",
    "test_data['cnb_binary'] = cnb_prediction\n",
    "test_data['svm_binary'] = svm_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TiAbs</th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>true label</th>\n",
       "      <th>bnb_binary</th>\n",
       "      <th>mnb_binary</th>\n",
       "      <th>cnb_binary</th>\n",
       "      <th>svm_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>data elements myeloid leukemia data standards ...</td>\n",
       "      <td>182</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>occurrence types consequences preventability i...</td>\n",
       "      <td>38</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>computational methods gene regulatory networks...</td>\n",
       "      <td>11</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>vaccine adverse event text mining system extra...</td>\n",
       "      <td>430</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>development quality criteria evaluate nonthera...</td>\n",
       "      <td>487</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>automation bias verification complexity system...</td>\n",
       "      <td>186</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>promise record linkage assessing uptake health...</td>\n",
       "      <td>338</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>miningabs mining associated biomarkers across ...</td>\n",
       "      <td>335</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>formative evaluation accuracy clinical decisio...</td>\n",
       "      <td>407</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>care continuum among people drugs protocol sys...</td>\n",
       "      <td>191</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>caught review webbased suicide prevention back...</td>\n",
       "      <td>354</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>effectiveness cognitive bias modification inte...</td>\n",
       "      <td>180</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>automated extraction diagnostic criteria elect...</td>\n",
       "      <td>13</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>thirty years artificial intelligence medicine ...</td>\n",
       "      <td>239</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>exploration risk factors falls using electroni...</td>\n",
       "      <td>198</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>role bispectral index monitoring burst suppres...</td>\n",
       "      <td>109</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>swiftreview textmining workbench systematic re...</td>\n",
       "      <td>201</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>review approaches identifying patient phenotyp...</td>\n",
       "      <td>367</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>external change agents promote quality improve...</td>\n",
       "      <td>76</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>consolidating emerging evidence surrounding hi...</td>\n",
       "      <td>143</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>impact automated brief messages promoting life...</td>\n",
       "      <td>206</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>toward normalized clinical drug knowledge base...</td>\n",
       "      <td>65</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>combining relevance assignment quality evidenc...</td>\n",
       "      <td>493</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>taxonomy rapid reviews links report types meth...</td>\n",
       "      <td>238</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>finding evidence absence medical notes using c...</td>\n",
       "      <td>194</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>automated identification molecular effects dru...</td>\n",
       "      <td>205</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>sentiment analysis medical settings opportunit...</td>\n",
       "      <td>279</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>health analytics types functions levels review...</td>\n",
       "      <td>39</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>decision support systems cardiology systematic...</td>\n",
       "      <td>138</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>technologyassisted title abstract screening sy...</td>\n",
       "      <td>67</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>automated image interpretation computerassiste...</td>\n",
       "      <td>411</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>cumulative analysis experiences singlesession ...</td>\n",
       "      <td>356</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>data problems healthcare perspective much writ...</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>integration machine learning metaanalysis iden...</td>\n",
       "      <td>72</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>deriving comorbidities medical records using n...</td>\n",
       "      <td>369</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>reflections combining metaanalysis structural ...</td>\n",
       "      <td>28</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>project neurodevelopment package novel method ...</td>\n",
       "      <td>301</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>novel algorithms improved pattern recognition ...</td>\n",
       "      <td>315</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>automatic extraction quantitative data clinica...</td>\n",
       "      <td>25</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>deep learning pharmacovigilance recurrent neur...</td>\n",
       "      <td>151</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>strategies obtaining unpublished drug trial da...</td>\n",
       "      <td>401</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>early diagnosis alzheimer disease multimodal s...</td>\n",
       "      <td>188</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>quantifying displaying accounting heterogeneit...</td>\n",
       "      <td>475</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>implicit motor learning lead greater automatiz...</td>\n",
       "      <td>27</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>enhancing reproducibility scientific computing...</td>\n",
       "      <td>97</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>development automated assessment tool medwatch...</td>\n",
       "      <td>146</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>feasibility mobile anticoagulation telemedicin...</td>\n",
       "      <td>484</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>identifying repetitive institutional review bo...</td>\n",
       "      <td>243</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>metaanalysis repository data impact data regul...</td>\n",
       "      <td>357</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 TiAbs  Article_ID true label  \\\n",
       "181  data elements myeloid leukemia data standards ...         182          N   \n",
       "37   occurrence types consequences preventability i...          38          N   \n",
       "10   computational methods gene regulatory networks...          11          N   \n",
       "411  vaccine adverse event text mining system extra...         430          N   \n",
       "468  development quality criteria evaluate nonthera...         487          N   \n",
       "185  automation bias verification complexity system...         186          N   \n",
       "320  promise record linkage assessing uptake health...         338          N   \n",
       "317  miningabs mining associated biomarkers across ...         335          N   \n",
       "387  formative evaluation accuracy clinical decisio...         407          N   \n",
       "190  care continuum among people drugs protocol sys...         191          N   \n",
       "336  caught review webbased suicide prevention back...         354          Y   \n",
       "179  effectiveness cognitive bias modification inte...         180          N   \n",
       "12   automated extraction diagnostic criteria elect...          13          N   \n",
       "238  thirty years artificial intelligence medicine ...         239          N   \n",
       "197  exploration risk factors falls using electroni...         198          N   \n",
       "108  role bispectral index monitoring burst suppres...         109          N   \n",
       "200  swiftreview textmining workbench systematic re...         201          Y   \n",
       "348  review approaches identifying patient phenotyp...         367          N   \n",
       "75   external change agents promote quality improve...          76          N   \n",
       "142  consolidating emerging evidence surrounding hi...         143          Y   \n",
       "205  impact automated brief messages promoting life...         206          N   \n",
       "64   toward normalized clinical drug knowledge base...          65          N   \n",
       "474  combining relevance assignment quality evidenc...         493          Y   \n",
       "237  taxonomy rapid reviews links report types meth...         238          Y   \n",
       "193  finding evidence absence medical notes using c...         194          N   \n",
       "204  automated identification molecular effects dru...         205          N   \n",
       "270  sentiment analysis medical settings opportunit...         279          N   \n",
       "38   health analytics types functions levels review...          39          N   \n",
       "137  decision support systems cardiology systematic...         138          N   \n",
       "66   technologyassisted title abstract screening sy...          67          Y   \n",
       "392  automated image interpretation computerassiste...         411          N   \n",
       "338  cumulative analysis experiences singlesession ...         356          N   \n",
       "121  data problems healthcare perspective much writ...         122          N   \n",
       "71   integration machine learning metaanalysis iden...          72          N   \n",
       "350  deriving comorbidities medical records using n...         369          N   \n",
       "27   reflections combining metaanalysis structural ...          28          N   \n",
       "283  project neurodevelopment package novel method ...         301          N   \n",
       "297  novel algorithms improved pattern recognition ...         315          N   \n",
       "24   automatic extraction quantitative data clinica...          25          Y   \n",
       "150  deep learning pharmacovigilance recurrent neur...         151          N   \n",
       "383  strategies obtaining unpublished drug trial da...         401          N   \n",
       "187  early diagnosis alzheimer disease multimodal s...         188          N   \n",
       "456  quantifying displaying accounting heterogeneit...         475          N   \n",
       "26   implicit motor learning lead greater automatiz...          27          N   \n",
       "96   enhancing reproducibility scientific computing...          97          N   \n",
       "145  development automated assessment tool medwatch...         146          N   \n",
       "465  feasibility mobile anticoagulation telemedicin...         484          N   \n",
       "242  identifying repetitive institutional review bo...         243          N   \n",
       "339  metaanalysis repository data impact data regul...         357          N   \n",
       "\n",
       "    bnb_binary mnb_binary cnb_binary svm_binary  \n",
       "181          N          N          N          N  \n",
       "37           N          N          N          N  \n",
       "10           N          N          N          N  \n",
       "411          N          N          N          N  \n",
       "468          N          N          N          N  \n",
       "185          N          N          N          N  \n",
       "320          N          N          N          N  \n",
       "317          N          N          N          N  \n",
       "387          N          N          N          N  \n",
       "190          N          N          N          N  \n",
       "336          N          N          N          N  \n",
       "179          N          N          N          N  \n",
       "12           N          N          N          N  \n",
       "238          N          N          N          N  \n",
       "197          N          N          N          N  \n",
       "108          N          N          N          N  \n",
       "200          N          N          N          Y  \n",
       "348          N          N          N          N  \n",
       "75           N          N          N          N  \n",
       "142          N          N          N          N  \n",
       "205          N          N          N          N  \n",
       "64           N          N          N          N  \n",
       "474          N          N          N          Y  \n",
       "237          N          N          N          N  \n",
       "193          N          N          N          N  \n",
       "204          N          N          N          N  \n",
       "270          N          N          N          N  \n",
       "38           N          N          N          N  \n",
       "137          N          N          N          N  \n",
       "66           N          N          Y          Y  \n",
       "392          N          N          N          N  \n",
       "338          N          N          N          N  \n",
       "121          N          N          N          N  \n",
       "71           N          N          N          N  \n",
       "350          N          N          N          N  \n",
       "27           N          N          N          N  \n",
       "283          N          N          N          N  \n",
       "297          N          N          N          N  \n",
       "24           N          N          N          N  \n",
       "150          N          N          N          N  \n",
       "383          N          N          N          N  \n",
       "187          N          N          N          N  \n",
       "456          N          N          N          N  \n",
       "26           N          N          N          N  \n",
       "96           N          N          N          N  \n",
       "145          N          N          N          N  \n",
       "465          N          N          N          N  \n",
       "242          N          N          N          N  \n",
       "339          N          N          N          N  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting Gaussian NB model\n",
      "Done fitting Bernoulli NB model\n",
      "Done fitting Multinomial NB model\n",
      "Done fitting Complement NB model\n",
      "Done fitting SVM model\n",
      "Gaussian NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      "Bernoulli NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      "Multinomial  NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      "Complement NB model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      "SVM model: \n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N'\n",
      " 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N' 'Y' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()#if dataset is already in binary form\n",
    "mnb = MultinomialNB()\n",
    "cnb = ComplementNB()\n",
    "svm = SVC(C = 10, kernel = 'linear', class_weight=None, gamma = 'scale', random_state=None)\n",
    "\n",
    "\n",
    "#train mmodel using training data\n",
    "gnb_model = gnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Gaussian NB model')\n",
    "bnb_model = bnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Bernoulli NB model')\n",
    "mnb_model = mnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Multinomial NB model')\n",
    "cnb_model = cnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Complement NB model')\n",
    "svm_model = svm.fit(tfidf_train_data, y_train)\n",
    "print ('Done fitting SVM model')\n",
    "\n",
    "#use model to predict on new data\n",
    "gnb_prediction = gnb_model.predict(tfidf_test_data.toarray())\n",
    "bnb_prediction = bnb_model.predict(tfidf_test_data.toarray())\n",
    "mnb_prediction = mnb_model.predict(tfidf_test_data.toarray())\n",
    "cnb_prediction = cnb_model.predict(tfidf_test_data.toarray())\n",
    "svm_prediction = svm_model.predict(tfidf_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Gaussian NB model: \\n', gnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Bernoulli NB model: \\n', bnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Multinomial  NB model: \\n', mnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Complement NB model: \\n', cnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('SVM model: \\n', svm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RESULT ........\n",
      "Accuracy:  0.9183673469387755\n",
      "PRECISION:  0.9183673469387755\n",
      "RECALL:  0.9183673469387755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Yes       0.91      1.00      0.95        42\n",
      "          No       1.00      0.43      0.60         7\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        49\n",
      "   macro avg       0.96      0.71      0.78        49\n",
      "weighted avg       0.93      0.92      0.90        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "accuracy = accuracy_score(y_test, svm_prediction)\n",
    "precision = precision_score(y_test, svm_prediction, average= 'micro')\n",
    "recall = recall_score(y_test, svm_prediction, average= 'micro')\n",
    "confusion_matrix(y_test, svm_prediction)\n",
    "\n",
    "print ('SVM RESULT ........')\n",
    "print ('Accuracy: ', accuracy)\n",
    "print('PRECISION: ', precision)\n",
    "print('RECALL: ', recall)\n",
    "print(classification_report(y_test, svm_prediction, target_names=['Yes', 'No']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guassian NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  6   48\n",
       "Y           0  1    1\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Guassian NB model: \\n')\n",
    "pd.crosstab(gnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  5   47\n",
       "Y           0  2    2\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Bernoulli NB model: \\n')\n",
    "pd.crosstab(bnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  7   49\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Multinomial NB model: \\n')\n",
    "pd.crosstab(mnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complement NB model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  6   48\n",
       "Y           0  1    1\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Complement NB model: \\n')\n",
    "pd.crosstab(cnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   N  Y  All\n",
       "Actual               \n",
       "N          42  4   46\n",
       "Y           0  3    3\n",
       "All        42  7   49"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('SVM model: \\n')\n",
    "pd.crosstab(svm_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you need the values for further processing\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, svm_prediction).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 0, 4, 3)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['gnb_tfidf'] = gnb_prediction\n",
    "test_data['bnb_tfidf'] = bnb_prediction\n",
    "test_data['mnb_tfidf'] = mnb_prediction\n",
    "test_data['cnb_tfidf'] = cnb_prediction\n",
    "test_data['svm_tfidf'] = svm_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TiAbs</th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>true label</th>\n",
       "      <th>bnb_binary</th>\n",
       "      <th>mnb_binary</th>\n",
       "      <th>cnb_binary</th>\n",
       "      <th>svm_binary</th>\n",
       "      <th>gnb_tfidf</th>\n",
       "      <th>bnb_tfidf</th>\n",
       "      <th>mnb_tfidf</th>\n",
       "      <th>cnb_tfidf</th>\n",
       "      <th>svm_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>data elements myeloid leukemia data standards ...</td>\n",
       "      <td>182</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>occurrence types consequences preventability i...</td>\n",
       "      <td>38</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>computational methods gene regulatory networks...</td>\n",
       "      <td>11</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>vaccine adverse event text mining system extra...</td>\n",
       "      <td>430</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>development quality criteria evaluate nonthera...</td>\n",
       "      <td>487</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>automation bias verification complexity system...</td>\n",
       "      <td>186</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>promise record linkage assessing uptake health...</td>\n",
       "      <td>338</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>miningabs mining associated biomarkers across ...</td>\n",
       "      <td>335</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>formative evaluation accuracy clinical decisio...</td>\n",
       "      <td>407</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>care continuum among people drugs protocol sys...</td>\n",
       "      <td>191</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>caught review webbased suicide prevention back...</td>\n",
       "      <td>354</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>effectiveness cognitive bias modification inte...</td>\n",
       "      <td>180</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>automated extraction diagnostic criteria elect...</td>\n",
       "      <td>13</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>thirty years artificial intelligence medicine ...</td>\n",
       "      <td>239</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>exploration risk factors falls using electroni...</td>\n",
       "      <td>198</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>role bispectral index monitoring burst suppres...</td>\n",
       "      <td>109</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>swiftreview textmining workbench systematic re...</td>\n",
       "      <td>201</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>review approaches identifying patient phenotyp...</td>\n",
       "      <td>367</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>external change agents promote quality improve...</td>\n",
       "      <td>76</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>consolidating emerging evidence surrounding hi...</td>\n",
       "      <td>143</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>impact automated brief messages promoting life...</td>\n",
       "      <td>206</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>toward normalized clinical drug knowledge base...</td>\n",
       "      <td>65</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>combining relevance assignment quality evidenc...</td>\n",
       "      <td>493</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>taxonomy rapid reviews links report types meth...</td>\n",
       "      <td>238</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>finding evidence absence medical notes using c...</td>\n",
       "      <td>194</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>automated identification molecular effects dru...</td>\n",
       "      <td>205</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>sentiment analysis medical settings opportunit...</td>\n",
       "      <td>279</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>health analytics types functions levels review...</td>\n",
       "      <td>39</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>decision support systems cardiology systematic...</td>\n",
       "      <td>138</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>technologyassisted title abstract screening sy...</td>\n",
       "      <td>67</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>automated image interpretation computerassiste...</td>\n",
       "      <td>411</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>cumulative analysis experiences singlesession ...</td>\n",
       "      <td>356</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>data problems healthcare perspective much writ...</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>integration machine learning metaanalysis iden...</td>\n",
       "      <td>72</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>deriving comorbidities medical records using n...</td>\n",
       "      <td>369</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>reflections combining metaanalysis structural ...</td>\n",
       "      <td>28</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>project neurodevelopment package novel method ...</td>\n",
       "      <td>301</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>novel algorithms improved pattern recognition ...</td>\n",
       "      <td>315</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>automatic extraction quantitative data clinica...</td>\n",
       "      <td>25</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>deep learning pharmacovigilance recurrent neur...</td>\n",
       "      <td>151</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>strategies obtaining unpublished drug trial da...</td>\n",
       "      <td>401</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>early diagnosis alzheimer disease multimodal s...</td>\n",
       "      <td>188</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>quantifying displaying accounting heterogeneit...</td>\n",
       "      <td>475</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>implicit motor learning lead greater automatiz...</td>\n",
       "      <td>27</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>enhancing reproducibility scientific computing...</td>\n",
       "      <td>97</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>development automated assessment tool medwatch...</td>\n",
       "      <td>146</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>feasibility mobile anticoagulation telemedicin...</td>\n",
       "      <td>484</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>identifying repetitive institutional review bo...</td>\n",
       "      <td>243</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>metaanalysis repository data impact data regul...</td>\n",
       "      <td>357</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 TiAbs  Article_ID true label  \\\n",
       "181  data elements myeloid leukemia data standards ...         182          N   \n",
       "37   occurrence types consequences preventability i...          38          N   \n",
       "10   computational methods gene regulatory networks...          11          N   \n",
       "411  vaccine adverse event text mining system extra...         430          N   \n",
       "468  development quality criteria evaluate nonthera...         487          N   \n",
       "185  automation bias verification complexity system...         186          N   \n",
       "320  promise record linkage assessing uptake health...         338          N   \n",
       "317  miningabs mining associated biomarkers across ...         335          N   \n",
       "387  formative evaluation accuracy clinical decisio...         407          N   \n",
       "190  care continuum among people drugs protocol sys...         191          N   \n",
       "336  caught review webbased suicide prevention back...         354          Y   \n",
       "179  effectiveness cognitive bias modification inte...         180          N   \n",
       "12   automated extraction diagnostic criteria elect...          13          N   \n",
       "238  thirty years artificial intelligence medicine ...         239          N   \n",
       "197  exploration risk factors falls using electroni...         198          N   \n",
       "108  role bispectral index monitoring burst suppres...         109          N   \n",
       "200  swiftreview textmining workbench systematic re...         201          Y   \n",
       "348  review approaches identifying patient phenotyp...         367          N   \n",
       "75   external change agents promote quality improve...          76          N   \n",
       "142  consolidating emerging evidence surrounding hi...         143          Y   \n",
       "205  impact automated brief messages promoting life...         206          N   \n",
       "64   toward normalized clinical drug knowledge base...          65          N   \n",
       "474  combining relevance assignment quality evidenc...         493          Y   \n",
       "237  taxonomy rapid reviews links report types meth...         238          Y   \n",
       "193  finding evidence absence medical notes using c...         194          N   \n",
       "204  automated identification molecular effects dru...         205          N   \n",
       "270  sentiment analysis medical settings opportunit...         279          N   \n",
       "38   health analytics types functions levels review...          39          N   \n",
       "137  decision support systems cardiology systematic...         138          N   \n",
       "66   technologyassisted title abstract screening sy...          67          Y   \n",
       "392  automated image interpretation computerassiste...         411          N   \n",
       "338  cumulative analysis experiences singlesession ...         356          N   \n",
       "121  data problems healthcare perspective much writ...         122          N   \n",
       "71   integration machine learning metaanalysis iden...          72          N   \n",
       "350  deriving comorbidities medical records using n...         369          N   \n",
       "27   reflections combining metaanalysis structural ...          28          N   \n",
       "283  project neurodevelopment package novel method ...         301          N   \n",
       "297  novel algorithms improved pattern recognition ...         315          N   \n",
       "24   automatic extraction quantitative data clinica...          25          Y   \n",
       "150  deep learning pharmacovigilance recurrent neur...         151          N   \n",
       "383  strategies obtaining unpublished drug trial da...         401          N   \n",
       "187  early diagnosis alzheimer disease multimodal s...         188          N   \n",
       "456  quantifying displaying accounting heterogeneit...         475          N   \n",
       "26   implicit motor learning lead greater automatiz...          27          N   \n",
       "96   enhancing reproducibility scientific computing...          97          N   \n",
       "145  development automated assessment tool medwatch...         146          N   \n",
       "465  feasibility mobile anticoagulation telemedicin...         484          N   \n",
       "242  identifying repetitive institutional review bo...         243          N   \n",
       "339  metaanalysis repository data impact data regul...         357          N   \n",
       "\n",
       "    bnb_binary mnb_binary cnb_binary svm_binary gnb_tfidf bnb_tfidf mnb_tfidf  \\\n",
       "181          N          N          N          N         N         N         N   \n",
       "37           N          N          N          N         N         N         N   \n",
       "10           N          N          N          N         N         N         N   \n",
       "411          N          N          N          N         N         N         N   \n",
       "468          N          N          N          N         N         N         N   \n",
       "185          N          N          N          N         N         N         N   \n",
       "320          N          N          N          N         N         N         N   \n",
       "317          N          N          N          N         N         N         N   \n",
       "387          N          N          N          N         N         N         N   \n",
       "190          N          N          N          N         N         N         N   \n",
       "336          N          N          N          N         N         N         N   \n",
       "179          N          N          N          N         N         N         N   \n",
       "12           N          N          N          N         N         N         N   \n",
       "238          N          N          N          N         N         N         N   \n",
       "197          N          N          N          N         N         N         N   \n",
       "108          N          N          N          N         N         N         N   \n",
       "200          N          N          N          Y         N         Y         N   \n",
       "348          N          N          N          N         N         N         N   \n",
       "75           N          N          N          N         N         N         N   \n",
       "142          N          N          N          N         N         N         N   \n",
       "205          N          N          N          N         N         N         N   \n",
       "64           N          N          N          N         N         N         N   \n",
       "474          N          N          N          Y         N         N         N   \n",
       "237          N          N          N          N         N         N         N   \n",
       "193          N          N          N          N         N         N         N   \n",
       "204          N          N          N          N         N         N         N   \n",
       "270          N          N          N          N         N         N         N   \n",
       "38           N          N          N          N         N         N         N   \n",
       "137          N          N          N          N         N         N         N   \n",
       "66           N          N          Y          Y         Y         Y         N   \n",
       "392          N          N          N          N         N         N         N   \n",
       "338          N          N          N          N         N         N         N   \n",
       "121          N          N          N          N         N         N         N   \n",
       "71           N          N          N          N         N         N         N   \n",
       "350          N          N          N          N         N         N         N   \n",
       "27           N          N          N          N         N         N         N   \n",
       "283          N          N          N          N         N         N         N   \n",
       "297          N          N          N          N         N         N         N   \n",
       "24           N          N          N          N         N         N         N   \n",
       "150          N          N          N          N         N         N         N   \n",
       "383          N          N          N          N         N         N         N   \n",
       "187          N          N          N          N         N         N         N   \n",
       "456          N          N          N          N         N         N         N   \n",
       "26           N          N          N          N         N         N         N   \n",
       "96           N          N          N          N         N         N         N   \n",
       "145          N          N          N          N         N         N         N   \n",
       "465          N          N          N          N         N         N         N   \n",
       "242          N          N          N          N         N         N         N   \n",
       "339          N          N          N          N         N         N         N   \n",
       "\n",
       "    cnb_tfidf svm_tfidf  \n",
       "181         N         N  \n",
       "37          N         N  \n",
       "10          N         N  \n",
       "411         N         N  \n",
       "468         N         N  \n",
       "185         N         N  \n",
       "320         N         N  \n",
       "317         N         N  \n",
       "387         N         N  \n",
       "190         N         N  \n",
       "336         N         N  \n",
       "179         N         N  \n",
       "12          N         N  \n",
       "238         N         N  \n",
       "197         N         N  \n",
       "108         N         N  \n",
       "200         N         Y  \n",
       "348         N         N  \n",
       "75          N         N  \n",
       "142         N         N  \n",
       "205         N         N  \n",
       "64          N         N  \n",
       "474         N         Y  \n",
       "237         N         N  \n",
       "193         N         N  \n",
       "204         N         N  \n",
       "270         N         N  \n",
       "38          N         N  \n",
       "137         N         N  \n",
       "66          Y         Y  \n",
       "392         N         N  \n",
       "338         N         N  \n",
       "121         N         N  \n",
       "71          N         N  \n",
       "350         N         N  \n",
       "27          N         N  \n",
       "283         N         N  \n",
       "297         N         N  \n",
       "24          N         N  \n",
       "150         N         N  \n",
       "383         N         N  \n",
       "187         N         N  \n",
       "456         N         N  \n",
       "26          N         N  \n",
       "96          N         N  \n",
       "145         N         N  \n",
       "465         N         N  \n",
       "242         N         N  \n",
       "339         N         N  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal tasks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try different parameters for the SVM model. Do you observe any difference in result?\n",
    "Change the Random_State value in the 'train_test_split'. Does this affect observed performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
