{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>AutoSynthesis study group</h1>\n",
    "<h2 align='right'> Session 3 - Modeling </h2>\n",
    "<h3 align='right'> 20th February 2019 </h3>\n",
    "<h3 align='right'> Kazeem </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of last session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul> \n",
    "<li> Loading and analysis of data </li>\n",
    "<li> Basic preprocessing </li>\n",
    "    <ul>\n",
    "        <li> Lowercasing </li>\n",
    "        <li> Stopwords removal </li>\n",
    "        <li> Stemming </li>\n",
    "        <li> Lemmatisation </li>\n",
    "    </ul>\n",
    "<li> Feature representation </li>\n",
    "    <ul>\n",
    "        <li> Bag of words </li>\n",
    "        <li> Tf-idf </li>\n",
    "        <li><font color = 'red'><strong> Binary </strong></font></li>\n",
    "        <li> Word embedding </li>\n",
    "        <li> N-grams </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB, BernoulliNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print ('Packages import successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#load data\n",
    "train = pd.read_csv('../session 2_no password/session 2/AutoSession2.csv')\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True, stop_words = 'english')\n",
    "tdm = vectorizer.fit_transform(train['Abstract'])\n",
    "words = vectorizer.get_feature_names()\n",
    "words = np.asarray(words)\n",
    "\n",
    "BoW =np.vstack((words, tdm.toarray()))\n",
    "tdm_df = pd.DataFrame(data=BoW[1:,:], columns = BoW[0,:])\n",
    "print (tdm_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresher\n",
    ">- Why do we need preprocessing?\n",
    ">- Why do we need feature representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML algorithms example 1: Naive Baye's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Based on the equation below:</p>\n",
    "<img src = \"Bayes_rule.webp\">\n",
    "\n",
    "Where,\n",
    "<ul>\n",
    "    <li>P(c|x) is the posterior probability of class (c, target label) given predictor (x, feature).</li>\n",
    "    <li>P(c) is the prior probability of class.</li>\n",
    "    <li>P(x|c) is the likelihood which is the probability of predictor given class.</li>\n",
    "    <li>P(x) is the prior probability of predictor.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML algorithms example 2: support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In support vector machine (SVM) each data item is plotted as a point in an n-dimensional space (where n is the number of features in the TDM)where the value of each feature is the value of each coordinate. The algorithm  classifies the data by finding the hyperplane that best partition the data.</p>\n",
    "<p>The SVM relies on the data points closest to the hyperplane on both sides to make prediction.</p>\n",
    "\n",
    "<img src=\"SVM_1.png\">\n",
    "\n",
    "<h4>Linear separablity and maximum margin</h4>\n",
    "<div>\n",
    "    <img src=\"SVM_21.png\"></div>\n",
    "    <img src=\"SVM_3.png\"></div>\n",
    "    <img src=\"SVM_4.png\"></div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h4>The Kernel trick</h4>\n",
    "<p> What happens in situations where the data is not linearly separable?\n",
    "<div>\n",
    "    <img src=\"SVM_8.png\"></div>\n",
    "    <img src=\"SVM_9.png\"></div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build text predictive models with ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A typical text classification process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"supervised_learning.png\" alt=\"text classification process\" title=\"text classification\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note - using sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - load your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('autosynthesis_session3.csv') #set the data path relative to your system and file location\n",
    "print ('Dataset loaded successfully')\n",
    "data.head(5) #view some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steb 1b - explore the dataset to gain insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Lena for her explorative work on the labelled dataset. These can be found in the shared session's folder named '<em><a href = '../data insight/Lena_Results.ipynb'>data instight</a></em>'\n",
    "<p>Note that there are two other excel files in the same folder</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - correct annomalies and fix NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for blank spaces in label\n",
    "np.unique(pd.isna(data.label), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for blank spaces in label\n",
    "np.unique(pd.isna(data.Decision2), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There must be no missing data in data particularly the labels. If it exists, FIX.\n",
    "data.Decision2 = data.Decision2.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - properly encode the target/label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['labels'] = le.fit_transform(data['label'])\n",
    "print (data['labels'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view label distribution\n",
    "#import numpy as np\n",
    "label_freq = np.unique(data.label, return_counts=True)\n",
    "print(\"Overall class distribution: \\n\", label_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - extract required subset of data (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TiAbs'] = data[['Title', 'Abstract', 'Keywords']].apply(lambda x: '{} {} {}'.format(x[0], x[1], x[2]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - split data to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['TiAbs'], data['label'], test_size=0.10, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see that data and labels are of the same size\n",
    "print ('Train data size: ', X_train.shape)\n",
    "print ('Test  data size: ', X_test.shape)\n",
    "print ('Train LABEL size: ', X_train.shape)\n",
    "print ('Test  LABEL size: ', X_test.shape)\n",
    "\n",
    "#check distribution\n",
    "label_freq = np.unique(y_test, return_counts=True)\n",
    "print(\"Overall class distribution: \\n\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally write custom preprocessing method.....WHY?\n",
    "def preprocessor(text):\n",
    "    #text = text.apply(lambda x: ' '.join(x.lower().replace('[^\\w\\s]','') for x in str(x).split() if not x in set(stopwords.words('english')) and not x.isdigit()))\n",
    "    \n",
    "    # split into words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    #from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words and len(w) > 3]\n",
    "    \n",
    "    return ' '.join(words) #return the cleaned text string separated by spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6a - data cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: preprocessor(x))\n",
    "X_test = X_test.apply(lambda x: preprocessor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 6b - feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "binary_encoder = TfidfVectorizer(stop_words='english', binary = True, max_df=0.8, min_df=3, ngram_range=(1, 1))\n",
    "binary_train_data = binary_encoder.fit_transform(X_train)\n",
    "binary_test_data = binary_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_encoder = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=3, ngram_range=(1, 1))\n",
    "tfidf_train_data = tfidf_encoder.fit_transform(X_train)\n",
    "tfidf_test_data = tfidf_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - fit a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import GaussianNB, ComplementNB, MultinomialNB, BernoulliNB\n",
    "#MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "#SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001,\n",
    "# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB(binarize = None)#if dataset is already in binary form\n",
    "mnb = MultinomialNB()\n",
    "cnb = ComplementNB()\n",
    "svm = SVC(C = 10, kernel = 'linear', class_weight=None, gamma = 'scale', random_state=None)\n",
    "\n",
    "\n",
    "#train mmodel using training data\n",
    "gnb_model = gnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Gaussian NB model')\n",
    "bnb_model = bnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Bernoulli NB model')\n",
    "mnb_model = mnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Multinomial NB model')\n",
    "cnb_model = cnb.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting Complement NB model')\n",
    "svm_model = svm.fit(binary_train_data.toarray(), y_train)\n",
    "print ('Done fitting SVM model')\n",
    "\n",
    "print ('------------------------------------------------------------------------ \\n')\n",
    "print ('Finished fitting all models')\n",
    "print ('Trained models ready for prediction on new data \\n')\n",
    "print ('------------------------------------------------------------------------ \\n')\n",
    "\n",
    "#use each model to predict on new data\n",
    "gnb_prediction = gnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Gaussian NB model')\n",
    "bnb_prediction = bnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Bernoulli NB model')\n",
    "mnb_prediction = mnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Multinomial NB model')\n",
    "cnb_prediction = cnb_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with Complement NB model')\n",
    "svm_prediction = svm_model.predict(binary_test_data.toarray())\n",
    "print ('Done predicting with SVM model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Gaussian NB model: \\n', gnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Bernoulli NB model: \\n', bnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Multinomial  NB model: \\n', mnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Complement NB model: \\n', cnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('SVM model: \\n', svm_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some basic metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'conf_mat.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "accuracy = accuracy_score(y_test, cnb_prediction)\n",
    "precision = precision_score(y_test, cnb_prediction, average= 'micro')\n",
    "recall = recall_score(y_test, cnb_prediction, average= 'micro')\n",
    "confusion_matrix(y_test, cnb_prediction)\n",
    "print(classification_report(y_test, cnb_prediction, target_names=['Yes', 'No']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Guassian NB model: \\n')\n",
    "pd.crosstab(gnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Bernoulli NB model: \\n')\n",
    "pd.crosstab(bnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Multinomial NB model: \\n')\n",
    "pd.crosstab(mnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Complement NB model: \\n')\n",
    "pd.crosstab(cnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('SVM model: \\n')\n",
    "pd.crosstab(svm_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = X_test\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "test_data['Article_ID'] = pd.DataFrame(data.iloc[list(test_data.index.values), 0])\n",
    "test_data['true label'] = y_test\n",
    "test_data['bnb_binary'] = gnb_prediction\n",
    "test_data['bnb_binary'] = bnb_prediction\n",
    "test_data['mnb_binary'] = mnb_prediction\n",
    "test_data['cnb_binary'] = cnb_prediction\n",
    "test_data['svm_binary'] = svm_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()#if dataset is already in binary form\n",
    "mnb = MultinomialNB()\n",
    "cnb = ComplementNB()\n",
    "svm = SVC(C = 10, kernel = 'linear', class_weight=None, gamma = 'scale', random_state=None)\n",
    "\n",
    "\n",
    "#train mmodel using training data\n",
    "gnb_model = gnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Gaussian NB model')\n",
    "bnb_model = bnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Bernoulli NB model')\n",
    "mnb_model = mnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Multinomial NB model')\n",
    "cnb_model = cnb.fit(tfidf_train_data.toarray(), y_train)\n",
    "print ('Done fitting Complement NB model')\n",
    "svm_model = svm.fit(tfidf_train_data, y_train)\n",
    "print ('Done fitting SVM model')\n",
    "\n",
    "#use model to predict on new data\n",
    "gnb_prediction = gnb_model.predict(tfidf_test_data.toarray())\n",
    "bnb_prediction = bnb_model.predict(tfidf_test_data.toarray())\n",
    "mnb_prediction = mnb_model.predict(tfidf_test_data.toarray())\n",
    "cnb_prediction = cnb_model.predict(tfidf_test_data.toarray())\n",
    "svm_prediction = svm_model.predict(tfidf_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Gaussian NB model: \\n', gnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Bernoulli NB model: \\n', bnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Multinomial  NB model: \\n', mnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Complement NB model: \\n', cnb_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('SVM model: \\n', svm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "accuracy = accuracy_score(y_test, svm_prediction)\n",
    "precision = precision_score(y_test, svm_prediction, average= 'micro')\n",
    "recall = recall_score(y_test, svm_prediction, average= 'micro')\n",
    "confusion_matrix(y_test, svm_prediction)\n",
    "\n",
    "print ('SVM RESULT ........')\n",
    "print ('Accuracy: ', accuracy)\n",
    "print('PRECISION: ', precision)\n",
    "print('RECALL: ', recall)\n",
    "print(classification_report(y_test, svm_prediction, target_names=['Yes', 'No']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Guassian NB model: \\n')\n",
    "pd.crosstab(gnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Bernoulli NB model: \\n')\n",
    "pd.crosstab(bnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Multinomial NB model: \\n')\n",
    "pd.crosstab(mnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Complement NB model: \\n')\n",
    "pd.crosstab(cnb_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('SVM model: \\n')\n",
    "pd.crosstab(svm_prediction, y_test, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case you need the values for further processing\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, svm_prediction).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['gnb_tfidf'] = gnb_prediction\n",
    "test_data['bnb_tfidf'] = bnb_prediction\n",
    "test_data['mnb_tfidf'] = mnb_prediction\n",
    "test_data['cnb_tfidf'] = cnb_prediction\n",
    "test_data['svm_tfidf'] = svm_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal tasks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try different parameters for the SVM model. Do you observe any difference in result?\n",
    "Change the Random_State value in the 'train_test_split'. Does this affect observed performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
