{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>AutoSynthesis study group</h1>\n",
    "<h2 align='center'> Session 6 - Putting it altogether </h2>\n",
    "<h3 align='right'> 22nd may 2019 </h3>\n",
    "<h3 align='right'> Kazeem </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Packages import successful\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "print(__doc__)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "print ('Packages import successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('autosynthesis_session3.csv') #set the data path relative to your system and file location\n",
    "print ('Dataset loaded successfully')\n",
    "data.head(5) #view some samples\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['labels'] = le.fit_transform(data['label'])\n",
    "X = data[['Title', 'Abstract', 'Keywords']].apply(lambda x: '{} {} {}'.format(x[0], x[1], x[2]), axis=1)\n",
    "y = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data to keep part exclusively for testing/validation\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally write custom preprocessing method.....WHY?\n",
    "def preprocessor(text):\n",
    "    #text = text.apply(lambda x: ' '.join(x.lower().replace('[^\\w\\s]','') for x in str(x).split() if not x in set(stopwords.words('english')) and not x.isdigit()))\n",
    "    \n",
    "    # split into words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    #from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words and len(w) > 3]\n",
    "    \n",
    "    return ' '.join(words) #return the cleaned text string separated by spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: preprocessor(x))\n",
    "X_test = X_test.apply(lambda x: preprocessor(x))\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_encoder = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=3, ngram_range=(1, 1))\n",
    "tfidf_train_data = tfidf_encoder.fit_transform(X_train)\n",
    "tfidf_test_data = tfidf_encoder.transform(X_test)\n",
    "\n",
    "#reduce data dimension\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=200)\n",
    "tf_train = bestfeatures.fit_transform(tfidf_train_data,y_train)\n",
    "tf_test = bestfeatures.fit_transform(tfidf_test_data,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best parameters with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ja18581/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.426 (+/-0.003) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.426 (+/-0.003) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.426 (+/-0.003) for {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.426 (+/-0.003) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.426 (+/-0.003) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.890 (+/-0.065) for {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.426 (+/-0.003) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.426 (+/-0.003) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.891 (+/-0.055) for {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.912 (+/-0.109) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.426 (+/-0.003) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.886 (+/-0.066) for {'C': 1000, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.942 (+/-0.018) for {'C': 1, 'kernel': 'linear'}\n",
      "0.856 (+/-0.084) for {'C': 10, 'kernel': 'linear'}\n",
      "0.856 (+/-0.086) for {'C': 100, 'kernel': 'linear'}\n",
      "0.856 (+/-0.086) for {'C': 1000, 'kernel': 'linear'}\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.753 (+/-0.173) for {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.810 (+/-0.134) for {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.686 (+/-0.130) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.809 (+/-0.103) for {'C': 1000, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.619 (+/-0.118) for {'C': 1, 'kernel': 'linear'}\n",
      "0.741 (+/-0.161) for {'C': 10, 'kernel': 'linear'}\n",
      "0.774 (+/-0.096) for {'C': 100, 'kernel': 'linear'}\n",
      "0.774 (+/-0.096) for {'C': 1000, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 'scale'],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score, iid=False)\n",
    "    clf.fit(tf_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Feature selection together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine preprocessing, dimensionality reduction with classification\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('feature_selection', SelectKBest(chi2)),\n",
    "    ('clf', SVC(gamma='scale'))\n",
    "])\n",
    "\n",
    "#set different options for parameters\n",
    "param_grid = {\n",
    "    'vect__max_df': (0.6,0.7,0.8), \n",
    "    'vect__min_df': (2, 3, 4),\n",
    "    'feature_selection__k': (20, 50, 100, 200, 250),\n",
    "    'clf__kernel': ('rbf', 'linear'),\n",
    "    'clf__C': (1, 10, 100, 1000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'vect__max_df': (0.6, 0.7, 0.8), 'vect__min_df': (2, 3, 4), 'feature_selection__k': (20, 50, 100, 200, 250), 'clf__kernel': ('rbf', 'linear'), 'clf__C': (1, 10, 100, 1000)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instanciate the grid search\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, iid=False)\n",
    "\n",
    "#fit the search\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.6, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1000,\n",
       " 'clf__kernel': 'rbf',\n",
       " 'feature_selection__k': 250,\n",
       " 'vect__max_df': 0.6,\n",
       " 'vect__min_df': 3}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple feature selection and classification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: create a pipeline object combining all the steps in order\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('reduce_dim', None),\n",
    "    ('clf', SVC(gamma='scale'))\n",
    "])\n",
    "\n",
    "#Step 2: set optional parameters for the different objects in the pipeline\n",
    "FEATURE_SIZE = [50, 100, 250, 350, 500]\n",
    "param_grid = [{\n",
    "    'vect__max_df': (0.5, 0.7, 0.8),\n",
    "    'vect__min_df': (2,3,5),\n",
    "    'vect__binary': (True, False),\n",
    "    'vect__ngram_range': [(1, 1), (1,2)],\n",
    "    'reduce_dim': [TruncatedSVD(n_iter=5)],\n",
    "    'reduce_dim__n_components': FEATURE_SIZE,\n",
    "    'clf__C': [1, 10, 100] \n",
    "},\n",
    "{\n",
    "    'vect__max_df': (0.5, 0.7, 0.8),\n",
    "    'vect__min_df': (2,3,5),\n",
    "    'vect__binary': (True, False),\n",
    "    'vect__ngram_range': [(1, 1), (1,2)],\n",
    "    'reduce_dim': [SelectKBest(chi2)],\n",
    "    'reduce_dim__k': FEATURE_SIZE,\n",
    "    'clf__C': [1, 10, 100] \n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the pipeline to select best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=False, n_jobs=1,\n",
       "       param_grid=[{'vect__max_df': (0.5, 0.7, 0.8), 'vect__min_df': (2, 3, 5), 'vect__binary': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)], 'reduce_dim': [TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)], 'reduce_dim__n_components': [50, 100, 250...ction chi2 at 0x7fd0678b7e18>)], 'reduce_dim__k': [50, 100, 250, 350, 500], 'clf__C': [1, 10, 100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass the pipeline to an instance of the GridSearch CV\n",
    "grid = GridSearchCV(pipeline, cv=5, n_jobs=1, param_grid=param_grid, iid=False)\n",
    "\n",
    "#fit the GridSearchCV instance\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(3, -1, len(FEATURE_SIZE))\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88779762, 0.89217687, 0.89213435, 0.88796769, 0.89421769],\n",
       "       [0.89613095, 0.89417517, 0.89009354, 0.88796769, 0.89213435],\n",
       "       [0.89196429, 0.89613095, 0.88988095, 0.89005102, 0.89009354],\n",
       "       [0.89005102, 0.89829932, 0.89613095, 0.90446429, 0.91488095],\n",
       "       [0.91071429, 0.91492347, 0.90659014, 0.90659014, 0.90858844],\n",
       "       [0.90654762, 0.92104592, 0.90867347, 0.90663265, 0.90667517],\n",
       "       [0.90871599, 0.90046769, 0.91071429, 0.91483844, 0.90663265],\n",
       "       [0.9107568 , 0.8962585 , 0.89621599, 0.8962585 , 0.89409014],\n",
       "       [0.89834184, 0.90238095, 0.89421769, 0.89621599, 0.89009354],\n",
       "       [0.89421769, 0.90042517, 0.8982568 , 0.88805272, 0.89621599],\n",
       "       [0.89217687, 0.89409014, 0.89213435, 0.8982568 , 0.91071429],\n",
       "       [0.9025085 , 0.90659014, 0.90871599, 0.90459184, 0.91488095],\n",
       "       [0.90238095, 0.89829932, 0.90863095, 0.90454932, 0.89834184],\n",
       "       [0.91084184, 0.90659014, 0.90246599, 0.90459184, 0.90242347],\n",
       "       [0.90459184, 0.90042517, 0.89421769, 0.89409014, 0.89213435],\n",
       "       [0.89821429, 0.90242347, 0.89829932, 0.90038265, 0.89409014],\n",
       "       [0.89213435, 0.89821429, 0.89829932, 0.89829932, 0.89213435],\n",
       "       [0.89413265, 0.89829932, 0.89821429, 0.89829932, 0.89829932],\n",
       "       [0.90867347, 0.90246599, 0.9045068 , 0.9045068 , 0.90046769],\n",
       "       [0.90242347, 0.91071429, 0.90034014, 0.90659014, 0.90454932],\n",
       "       [0.90663265, 0.90454932, 0.89829932, 0.90454932, 0.9045068 ],\n",
       "       [0.90446429, 0.90667517, 0.89834184, 0.90654762, 0.8900085 ],\n",
       "       [0.9065051 , 0.89829932, 0.90858844, 0.8982568 , 0.90442177],\n",
       "       [0.88792517, 0.90029762, 0.8962585 , 0.90858844, 0.89209184],\n",
       "       [0.90029762, 0.8900085 , 0.90233844, 0.90034014, 0.90863095],\n",
       "       [0.89413265, 0.90858844, 0.90446429, 0.90446429, 0.90446429],\n",
       "       [0.90446429, 0.90646259, 0.90863095, 0.90442177, 0.9065051 ],\n",
       "       [0.90863095, 0.9045068 , 0.90446429, 0.90863095, 0.90442177],\n",
       "       [0.90242347, 0.90654762, 0.90034014, 0.90233844, 0.90029762],\n",
       "       [0.8900085 , 0.90446429, 0.8982568 , 0.90034014, 0.89821429],\n",
       "       [0.90238095, 0.8900085 , 0.90654762, 0.8982568 , 0.90242347],\n",
       "       [0.89821429, 0.90238095, 0.8900085 , 0.90858844, 0.89617347],\n",
       "       [0.90242347, 0.89821429, 0.90654762, 0.90238095, 0.90654762],\n",
       "       [0.90858844, 0.91279762, 0.90654762, 0.90654762, 0.90029762],\n",
       "       [0.9065051 , 0.9065051 , 0.90659014, 0.90446429, 0.90446429],\n",
       "       [0.89821429, 0.90446429, 0.90446429, 0.9045068 , 0.90446429],\n",
       "       [0.89005102, 0.88588435, 0.89421769, 0.88384354, 0.8900085 ],\n",
       "       [0.89213435, 0.89413265, 0.88384354, 0.89409014, 0.89005102],\n",
       "       [0.89417517, 0.89005102, 0.88792517, 0.87971939, 0.8900085 ],\n",
       "       [0.88384354, 0.89209184, 0.89621599, 0.90659014, 0.90863095],\n",
       "       [0.90654762, 0.90867347, 0.90242347, 0.90659014, 0.91071429],\n",
       "       [0.8962585 , 0.91071429, 0.90871599, 0.90659014, 0.89834184],\n",
       "       [0.90858844, 0.90246599, 0.9127551 , 0.90659014, 0.90867347],\n",
       "       [0.90663265, 0.89005102, 0.88384354, 0.88792517, 0.88796769],\n",
       "       [0.89838435, 0.89009354, 0.89417517, 0.88384354, 0.8837585 ],\n",
       "       [0.88588435, 0.8900085 , 0.89005102, 0.89209184, 0.87971939],\n",
       "       [0.88588435, 0.88380102, 0.88796769, 0.89009354, 0.90659014],\n",
       "       [0.90459184, 0.90242347, 0.90663265, 0.90446429, 0.90863095],\n",
       "       [0.91071429, 0.90034014, 0.90654762, 0.90454932, 0.90034014],\n",
       "       [0.90863095, 0.90654762, 0.90459184, 0.90446429, 0.9107568 ],\n",
       "       [0.90863095, 0.9107568 , 0.8900085 , 0.8900085 , 0.89213435],\n",
       "       [0.89604592, 0.89213435, 0.89204932, 0.88588435, 0.8900085 ],\n",
       "       [0.8900085 , 0.89604592, 0.89213435, 0.88792517, 0.88796769],\n",
       "       [0.8900085 , 0.88796769, 0.89604592, 0.8962585 , 0.8900085 ],\n",
       "       [0.90446429, 0.90242347, 0.91288265, 0.90242347, 0.9045068 ],\n",
       "       [0.90446429, 0.90654762, 0.90029762, 0.90880102, 0.90654762],\n",
       "       [0.90034014, 0.90246599, 0.90446429, 0.90654762, 0.90463435],\n",
       "       [0.9045068 , 0.90029762, 0.90242347, 0.88796769, 0.8837585 ],\n",
       "       [0.89209184, 0.88584184, 0.90659014, 0.89213435, 0.88796769],\n",
       "       [0.8837585 , 0.89421769, 0.88792517, 0.8962585 , 0.89209184],\n",
       "       [0.88588435, 0.8837585 , 0.89417517, 0.88596939, 0.90038265],\n",
       "       [0.89005102, 0.90654762, 0.90034014, 0.90863095, 0.90663265],\n",
       "       [0.9045068 , 0.90863095, 0.90654762, 0.90242347, 0.90671769],\n",
       "       [0.90863095, 0.90242347, 0.90659014, 0.90238095, 0.9045068 ],\n",
       "       [0.90654762, 0.90654762, 0.9045068 , 0.90659014, 0.88588435],\n",
       "       [0.88584184, 0.8900085 , 0.88796769, 0.90246599, 0.89417517],\n",
       "       [0.88592687, 0.88584184, 0.8900085 , 0.88588435, 0.89834184],\n",
       "       [0.89209184, 0.88592687, 0.88584184, 0.8900085 , 0.88588435],\n",
       "       [0.89834184, 0.89209184, 0.90446429, 0.9045068 , 0.91071429],\n",
       "       [0.91071429, 0.9045068 , 0.90863095, 0.90238095, 0.9045068 ],\n",
       "       [0.90654762, 0.90863095, 0.90446429, 0.90659014, 0.90238095],\n",
       "       [0.9045068 , 0.90454932, 0.90863095, 0.90654762, 0.90659014]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 100,\n",
       " 'reduce_dim': TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "        random_state=None, tol=0.0),\n",
       " 'reduce_dim__n_components': 50,\n",
       " 'vect__binary': False,\n",
       " 'vect__max_df': 0.7,\n",
       " 'vect__min_df': 3,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.921045918367347"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.7, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n"
     ]
    }
   ],
   "source": [
    "best_estimator = grid.best_estimator_\n",
    "print (best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the best model to predict a future article\n",
    "x = ['''The use of bibliography enriched features for automatic citation screening Citation screening (also called study selection) is a phase of systematic review process that has attracted \n",
    "a growing interest on the use of text mining (TM) methods to support it to reduce time and effort. Search results are usually imbalanced between the relevant and the irrelevant classes of returned citations. Class imbalance among other factors has been a persistent problem that impairs the performance of TM models, particularly in the context of automatic citation screening for systematic reviews. This has often caused the performance of classification models using the basic title and abstract data to ordinarily fall short of expectations.\n",
    "In this study, we explore the effects of using full bibliography data in addition to title and abstract on text \n",
    "classification performance for automatic citation screening.\n",
    "We experiment with binary and Word2vec feature representations and SVM models using 4 software engineering (SE) \n",
    "and 15 medical review datasets. We build and compare 3 types of models (binary-non-linear, Word2vec-linear and \n",
    "Word2vec-non-linear kernels) with each dataset using the two feature sets.\n",
    "\n",
    "The bibliography enriched data exhibited consistent improved performance in terms of recall, work saved over \n",
    "sampling (WSS) and Matthews correlation coefficient (MCC) in 3 of the 4 SE datasets that are fairly large in size. For the medical datasets, the results vary, however in the majority of cases the performance is the same or better.\n",
    "Inclusion of the bibliography data provides the potential of improving the performance of the models but to date \n",
    "results are inconclusive. Citation screening automation; Computing methodologies; Feature enrichment; Systematic reviews; Text mining''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example with Bernuoli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: create a pipeline object combining all the steps in order\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('reduce_dim', None),\n",
    "    ('clf', BernoulliNB())\n",
    "])\n",
    "\n",
    "#Step 2: set optional parameters for the different objects in the pipeline\n",
    "FEATURE_SIZE = [50, 100, 250, 350, 500]\n",
    "param_grid = [{\n",
    "    'vect__max_df': (0.5, 0.7, 0.8),\n",
    "    'vect__min_df': (2,3,5),\n",
    "    'vect__binary': (True, False),\n",
    "    'vect__ngram_range': [(1, 1), (1,2)],\n",
    "    'reduce_dim': [TruncatedSVD(n_iter=5)],\n",
    "    'reduce_dim__n_components': FEATURE_SIZE\n",
    "},\n",
    "{\n",
    "    'vect__max_df': (0.5, 0.7, 0.8),\n",
    "    'vect__min_df': (2,3,5),\n",
    "    'vect__binary': (True, False),\n",
    "    'vect__ngram_range': [(1, 1), (1,2)],\n",
    "    'reduce_dim': [SelectKBest(chi2)],\n",
    "    'reduce_dim__k': FEATURE_SIZE\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...reduce_dim', None), ('clf', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=False, n_jobs=1,\n",
       "       param_grid=[{'vect__max_df': (0.5, 0.7, 0.8), 'vect__min_df': (2, 3, 5), 'vect__binary': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)], 'reduce_dim': [TruncatedSVD(algorithm='randomized', n_components=500, n_iter=5,\n",
       "       random_state=None, tol=0.0)], 'reduce_dim__n_components': [50, 100, 25...t(k=100, score_func=<function chi2 at 0x7fd0678b7e18>)], 'reduce_dim__k': [50, 100, 250, 350, 500]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass the pipeline to an instance of the GridSearch CV\n",
    "grid = GridSearchCV(pipeline, cv=5, n_jobs=1, param_grid=param_grid, iid=False)\n",
    "\n",
    "#fit the GridSearchCV instance\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reduce_dim': SelectKBest(k=100, score_func=<function chi2 at 0x7fd0678b7e18>),\n",
       " 'reduce_dim__k': 100,\n",
       " 'vect__binary': True,\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__min_df': 3,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9065051020408165"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ... 0x7fd0678b7e18>)), ('clf', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.predict(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
